{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37788ba0",
   "metadata": {},
   "source": [
    "### SAP Machine Learning Embedding in OpenAI - step 05\n",
    "##### Author: Sergiu Iatco. May, 2023\n",
    "https://people.sap.com/iatco.sergiu <br>\n",
    "https://www.linkedin.com/in/sergiuiatco/ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9498f9c",
   "metadata": {},
   "source": [
    "#### Collected content for embedding.\n",
    "Blogs:<br>\n",
    "https://blogs.sap.com/2022/11/07/sap-community-call-sap-hana-cloud-machine-learning-challenge-i-quit-how-to-prevent-employee-churn/\n",
    "https://blogs.sap.com/2022/11/28/i-quit-how-to-predict-employee-churn-sap-hana-cloud-machine-learning-challenge/\n",
    "https://blogs.sap.com/2022/12/22/sap-hana-cloud-machine-learning-challenge-2022-the-winners-are/\n",
    "\n",
    "https://blogs.sap.com/2023/01/09/sap-hana-cloud-machine-learning-challenge-i-quit-understanding-metrics/\n",
    "\n",
    "Documentation:<br>\n",
    "https://help.sap.com/doc/1d0ebfe5e8dd44d09606814d83308d4b/2.0.04/en-US/hana_ml.dataframe.html\n",
    "https://help.sap.com/doc/1d0ebfe5e8dd44d09606814d83308d4b/2.0.07/en-US/pal/algorithms/hana_ml.algorithms.pal.trees.HybridGradientBoostingClassifier.html\n",
    "\n",
    "GitHub Notebooks:<br>\n",
    "https://github.com/SAP-samples/hana-ml-samples/tree/main/Python-API/usecase-examples/sapcommunity-hanaml-challenge<br>\n",
    "https://github.com/itsergiu/sapcommunity-hanaml-challenge<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddc4bba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac381f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.core.debugger import set_trace\n",
    "os.environ[\"OPENAI_API_KEY\"] = '<OPENAI_API_KEY>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cd5a045",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "import shutil\n",
    "import pathlib\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.CRITICAL)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# There are five standard levels for logging in Python, listed here in increasing order of severity:\n",
    "# DEBUG: Detailed information, typically of interest only when diagnosing problems.\n",
    "# INFO: Confirmation that things are working as expected.\n",
    "# WARNING: An indication that something unexpected happened or indicative of some problem in the near future (e.g., ‘disk space low’). The software is still working as expected.\n",
    "# ERROR: Due to a more serious problem, the software has not been able to perform some function.\n",
    "# CRITICAL: A very serious error, indicating that the program itself may be unable to continue running.\n",
    "\n",
    "class llama_context():\n",
    "    def __init__(self, path=None):\n",
    "        \n",
    "        if path!=None:\n",
    "            self.path = path\n",
    "        else:\n",
    "            self.path = ''\n",
    "        \n",
    "        perisit_sub_dir = \"storage\"\n",
    "        self.perisit_dir = os.path.join(self.path, perisit_sub_dir)\n",
    "        if not os.path.exists(self.perisit_dir):\n",
    "            os.makedirs(self.perisit_dir)\n",
    "        data_sub_dir = \"data\"\n",
    "        self.data_dir = os.path.join(self.path, data_sub_dir)\n",
    "        self.data_dir_counter = 0\n",
    "        \n",
    "        self.cost_model_ada = \"ada\" # https://openai.com/pricing\n",
    "        self.cost_model_davinci = \"davinci\" # https://openai.com/pricing\n",
    "        self.price_ada_1k_tokens = 0.0004\n",
    "        self.price_davinci_1k_tokens = 0.03 \n",
    "\n",
    "        \n",
    "    def load_data(self):\n",
    "        self.documents = SimpleDirectoryReader(self.data_dir).load_data()\n",
    "        print(f\"Documents loaded: {len(self.documents)}.\")\n",
    "    def create_vector_store(self):\n",
    "        self.index = GPTVectorStoreIndex.from_documents(self.documents)\n",
    "        print(\"GPTVectorStoreIndex complete.\")\n",
    "    def save_index(self):\n",
    "        self.index.storage_context.persist(persist_dir=self.perisit_dir)\n",
    "        print(f\"Index saved in path {self.perisit_dir}.\")\n",
    "    def load_index(self):\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=self.perisit_dir)\n",
    "        self.index = load_index_from_storage(storage_context)\n",
    "    def start_query_engine(self):\n",
    "        self.query_engine = self.index.as_query_engine()\n",
    "        print(\"Query_engine started.\")\n",
    "    def post_question(self, question, sleep = None):\n",
    "        if sleep == None:\n",
    "            self.sleep = 0 # trial 20s\n",
    "        self.response_cls = self.query_engine.query(question)\n",
    "        self.response = self.response_cls.response\n",
    "\n",
    "    def del_data_dir(self):\n",
    "        path = self.data_dir\n",
    "        try:\n",
    "            shutil.rmtree(path)\n",
    "            print(f\"{path} deleted successfully!\")\n",
    "        except OSError as error:\n",
    "            print(f\"Error deleting {path}: {error}\")\n",
    "\n",
    "    def copy_file_to_data_dir(self, file_extension ='.txt', verbose = 0):\n",
    "\n",
    "        path_from = self.path\n",
    "        path_to = self.data_dir\n",
    "\n",
    "        if not os.path.exists(path_to):\n",
    "            os.makedirs(path_to)\n",
    "\n",
    "        for filename in os.listdir(path_from):\n",
    "            if filename.endswith(file_extension):\n",
    "                source_path = os.path.join(path_from, filename)\n",
    "                dest_path = os.path.join(path_to, filename)\n",
    "                shutil.copy(source_path, dest_path)\n",
    "                if verbose == 1:\n",
    "                    print(f\"File {filename} copied successfully!\")\n",
    "    \n",
    "        path_to_lib = pathlib.Path(path_to)\n",
    "        path_to_lib_files = path_to_lib.glob(f\"*{file_extension}\")\n",
    "        print(f\"Files {len(list(path_to_lib_files))} copied in {path_to}.\")\n",
    " \n",
    "    def copy_path_from_to_data_dir(self, path_from, file_extension ='.txt', verbose = 0):\n",
    "\n",
    "        path_to = self.data_dir # default data folder for llama\n",
    "        start_counter = self.data_dir_counter\n",
    "        \n",
    "        if not os.path.exists(path_to):\n",
    "            os.makedirs(path_to)\n",
    "\n",
    "        padding_n = 5\n",
    "        path_from_lib = pathlib.Path(path_from)\n",
    "        path_from_lib_files = path_from_lib.glob(f\"**/*{file_extension}\")\n",
    "\n",
    "        files_copied_n = 0\n",
    "        counter = None\n",
    "        for counter, file in enumerate(path_from_lib_files, start_counter):\n",
    "            filename_path = os.path.split(file)[0] # path only\n",
    "            filename = os.path.split(file)[1] # filename only\n",
    "            filename_with_index = f'{str(counter).zfill(padding_n)}_{filename}'\n",
    "            file_to_data_dir = os.path.join(path_to, filename_with_index)\n",
    "            shutil.copy(file, file_to_data_dir)\n",
    "            \n",
    "            if os.path.exists(file_to_data_dir):\n",
    "                files_copied_n += 1\n",
    "                if verbose == 1:\n",
    "                    print(f\"File {filename} -> copied successfully!\")\n",
    "            else:\n",
    "                if verbose == 1:\n",
    "                    print(f\"File {filename} was not copied!\")\n",
    "        \n",
    "#         if 'counter' in locals(): \n",
    "        if counter != None: \n",
    "            self.data_dir_counter = counter + 1 # start from last\n",
    "        \n",
    "        print(f\"Files: {files_copied_n} copied to folder: {path_to}!\")\n",
    "\n",
    "    def estimate_tokens(self, text):\n",
    "        words = text.split()\n",
    "\n",
    "        num_words = int(len(words))\n",
    "        tokens = int(( num_words / 0.75 ))\n",
    "        tokens_1k = tokens / 1000\n",
    "        cost_ada = tokens_1k * self.price_ada_1k_tokens\n",
    "        cost_davinci = tokens_1k * self.price_davinci_1k_tokens\n",
    "        return tokens, cost_ada, cost_davinci\n",
    "    \n",
    "    def estimate_cost(self):\n",
    "        total_tokens = 0\n",
    "        total_cost_ada = 0\n",
    "        total_cost_davinci = 0\n",
    "        costs_rounding = 8\n",
    "        \n",
    "        for doc in self.documents:\n",
    "            text = doc.get_text()\n",
    "            tokens, cost_ada, cost_davinci = self.estimate_tokens(text)\n",
    "            total_tokens += tokens\n",
    "            \n",
    "            total_cost_ada += cost_ada\n",
    "            total_cost_ada = round(total_cost_ada, costs_rounding)\n",
    "            \n",
    "            total_cost_davinci += cost_davinci\n",
    "            total_cost_davinci = round(total_cost_davinci, costs_rounding)\n",
    "            \n",
    "        self.total_tokens = total_tokens\n",
    "        self.total_cost_ada = total_cost_ada\n",
    "        self.total_cost_davinci = total_cost_davinci\n",
    "        print(f\"Total tokens: {self.total_tokens}\")\n",
    "        print(f\"Total estimated costs with model {self.cost_model_ada }: ${self.total_cost_ada}\")\n",
    "        print(f\"Total estimated costs with model {self.cost_model_davinci }: ${self.total_cost_davinci}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2375e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def time_now():\n",
    "    now = datetime.datetime.now()\n",
    "    formatted = now.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(formatted)\n",
    "\n",
    "# time_now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7578ebdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'llama_challenge'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'llama_challenge\\\\data'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'llama_challenge\\\\storage'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path_llama = \"llama_challenge\"\n",
    "lct = llama_context(path=path_llama)\n",
    "\n",
    "display(lct.path)\n",
    "display(lct.data_dir)\n",
    "display(lct.perisit_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "148d2c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# time_now()\n",
    "run_load_create_save = False #True if you want to run previous notebooks to collect data\n",
    "if run_load_create_save:\n",
    "    lct.del_data_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "717c21a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# time_now()\n",
    "run_load_create_save = False #True if you want to run previous notebooks to collect data\n",
    "if run_load_create_save:\n",
    "\n",
    "    path_from1 = \"llama_challenge//html_challenge\"\n",
    "    path_from2 = \"llama_challenge//ipynb_blog\"\n",
    "    path_from3 = \"llama_challenge//ipynb_hana_ml_samples//Python-API//usecase-examples//sapcommunity-hanaml-challenge\"\n",
    "\n",
    "    lct.copy_path_from_to_data_dir(path_from1) # default extension *.txt\n",
    "    lct.copy_path_from_to_data_dir(path_from2) # default extension *.txt\n",
    "    lct.copy_path_from_to_data_dir(path_from3) # default extension *.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e4345e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['path', 'perisit_dir', 'data_dir', 'data_dir_counter', 'cost_model_ada', 'cost_model_davinci', 'price_ada_1k_tokens', 'price_davinci_1k_tokens'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(lct).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a451b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "Documents loaded: 12.\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# time_now()\n",
    "run_load_create_save = True #load documents\n",
    "if run_load_create_save:\n",
    "    lct.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bee30ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "Total tokens: 43819\n",
      "Total estimated costs with model ada: $0.0175276\n",
      "Total estimated costs with model davinci: $1.31457\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# time_now()\n",
    "run_load_create_save = True #Estimate costs\n",
    "if run_load_create_save:\n",
    "    lct.estimate_cost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f6ad1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "# https://platform.openai.com/account/api-keys\n",
    "%time\n",
    "# time_now()\n",
    "run_load_create_save = False # True if you want to create vector_store\n",
    "if run_load_create_save:\n",
    "    lct.create_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb6d9c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# time_now()\n",
    "run_load_create_save = False # True if you created vector_store and want to load it\n",
    "if run_load_create_save:\n",
    "    lct.save_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5472eff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# time_now()\n",
    "lct.load_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f120dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lct.index.vector_store.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "247f52cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(lct.index.vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "952ee2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on SimpleVectorStore in module llama_index.vector_stores.simple object:\n",
      "\n",
      "class SimpleVectorStore(llama_index.vector_stores.types.VectorStore)\n",
      " |  SimpleVectorStore(*args, **kwds)\n",
      " |  \n",
      " |  Simple Vector Store.\n",
      " |  \n",
      " |  In this vector store, embeddings are stored within a simple, in-memory dictionary.\n",
      " |  \n",
      " |  Args:\n",
      " |      simple_vector_store_data_dict (Optional[dict]): data dict\n",
      " |          containing the embeddings and doc_ids. See SimpleVectorStoreData\n",
      " |          for more details.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SimpleVectorStore\n",
      " |      llama_index.vector_stores.types.VectorStore\n",
      " |      typing.Protocol\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, data: Union[llama_index.vector_stores.simple.SimpleVectorStoreData, NoneType] = None, fs: Union[fsspec.spec.AbstractFileSystem, NoneType] = None, **kwargs: Any) -> None\n",
      " |      Initialize params.\n",
      " |  \n",
      " |  __subclasshook__ = _proto_hook(other)\n",
      " |      # Set (or override) the protocol subclass hook.\n",
      " |  \n",
      " |  add(self, embedding_results: List[llama_index.vector_stores.types.NodeWithEmbedding]) -> List[str]\n",
      " |      Add embedding_results to index.\n",
      " |  \n",
      " |  delete(self, ref_doc_id: str, **delete_kwargs: Any) -> None\n",
      " |      Delete nodes using with ref_doc_id.\n",
      " |      \n",
      " |      Args:\n",
      " |          ref_doc_id (str): The doc_id of the document to delete.\n",
      " |  \n",
      " |  get(self, text_id: str) -> List[float]\n",
      " |      Get embedding.\n",
      " |  \n",
      " |  persist(self, persist_path: str = './storage\\\\vector_store.json', fs: Union[fsspec.spec.AbstractFileSystem, NoneType] = None) -> None\n",
      " |      Persist the SimpleVectorStore to a directory.\n",
      " |  \n",
      " |  query(self, query: llama_index.vector_stores.types.VectorStoreQuery, **kwargs: Any) -> llama_index.vector_stores.types.VectorStoreQueryResult\n",
      " |      Get nodes for response.\n",
      " |  \n",
      " |  to_dict(self) -> dict\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_dict(save_dict: dict) -> 'SimpleVectorStore' from typing._ProtocolMeta\n",
      " |  \n",
      " |  from_persist_dir(persist_dir: str = './storage', fs: Union[fsspec.spec.AbstractFileSystem, NoneType] = None) -> 'SimpleVectorStore' from typing._ProtocolMeta\n",
      " |      Load from persist dir.\n",
      " |  \n",
      " |  from_persist_path(persist_path: str, fs: Union[fsspec.spec.AbstractFileSystem, NoneType] = None) -> 'SimpleVectorStore' from typing._ProtocolMeta\n",
      " |      Create a SimpleKVStore from a persist directory.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  client\n",
      " |      Get client.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'stores_text': <class 'bool'>}\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  stores_text = False\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from llama_index.vector_stores.types.VectorStore:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from llama_index.vector_stores.types.VectorStore:\n",
      " |  \n",
      " |  is_embedding_query = True\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Protocol:\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from typing._ProtocolMeta\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from typing._ProtocolMeta\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwds)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(lct.index.vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f078570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'copy_file_to_data_dir',\n",
       " 'copy_path_from_to_data_dir',\n",
       " 'cost_model_ada',\n",
       " 'cost_model_davinci',\n",
       " 'create_vector_store',\n",
       " 'data_dir',\n",
       " 'data_dir_counter',\n",
       " 'del_data_dir',\n",
       " 'documents',\n",
       " 'estimate_cost',\n",
       " 'estimate_tokens',\n",
       " 'index',\n",
       " 'load_data',\n",
       " 'load_index',\n",
       " 'path',\n",
       " 'perisit_dir',\n",
       " 'post_question',\n",
       " 'price_ada_1k_tokens',\n",
       " 'price_davinci_1k_tokens',\n",
       " 'save_index',\n",
       " 'start_query_engine',\n",
       " 'total_cost_ada',\n",
       " 'total_cost_davinci',\n",
       " 'total_tokens']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(lct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cb5b568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on llama_context in module __main__ object:\n",
      "\n",
      "class llama_context(builtins.object)\n",
      " |  llama_context(path=None)\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, path=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  copy_file_to_data_dir(self, file_extension='.txt', verbose=0)\n",
      " |  \n",
      " |  copy_path_from_to_data_dir(self, path_from, file_extension='.txt', verbose=0)\n",
      " |  \n",
      " |  create_vector_store(self)\n",
      " |  \n",
      " |  del_data_dir(self)\n",
      " |  \n",
      " |  estimate_cost(self)\n",
      " |  \n",
      " |  estimate_tokens(self, text)\n",
      " |  \n",
      " |  load_data(self)\n",
      " |  \n",
      " |  load_index(self)\n",
      " |  \n",
      " |  post_question(self, question, sleep=None)\n",
      " |  \n",
      " |  save_index(self)\n",
      " |  \n",
      " |  start_query_engine(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(lct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a1cb851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "Query_engine started.\n"
     ]
    }
   ],
   "source": [
    "# lct.vector_store.to_dict()\n",
    "%time\n",
    "# time_now()\n",
    "lct.start_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f5b399a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lct.documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17871a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspecting the object lct\n",
    "# lct.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "605a8ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_data',\n",
       " '_fs',\n",
       " '_is_protocol',\n",
       " '_is_runtime_protocol',\n",
       " 'add',\n",
       " 'client',\n",
       " 'delete',\n",
       " 'from_dict',\n",
       " 'from_persist_dir',\n",
       " 'from_persist_path',\n",
       " 'get',\n",
       " 'is_embedding_query',\n",
       " 'persist',\n",
       " 'query',\n",
       " 'stores_text',\n",
       " 'to_dict']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What embedding stores - vector_store.json\n",
    "dir(lct.index.vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "951aa344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_add_nodes_to_index',\n",
       " '_aget_node_embedding_results',\n",
       " '_async_add_nodes_to_index',\n",
       " '_build_index_from_nodes',\n",
       " '_delete_node',\n",
       " '_docstore',\n",
       " '_get_node_embedding_results',\n",
       " '_index_struct',\n",
       " '_insert',\n",
       " '_is_protocol',\n",
       " '_service_context',\n",
       " '_storage_context',\n",
       " '_store_nodes_override',\n",
       " '_use_async',\n",
       " '_vector_store',\n",
       " 'as_chat_engine',\n",
       " 'as_query_engine',\n",
       " 'as_retriever',\n",
       " 'build_index_from_nodes',\n",
       " 'delete',\n",
       " 'delete_nodes',\n",
       " 'delete_ref_doc',\n",
       " 'docstore',\n",
       " 'from_documents',\n",
       " 'from_vector_store',\n",
       " 'index_id',\n",
       " 'index_struct',\n",
       " 'index_struct_cls',\n",
       " 'insert',\n",
       " 'insert_nodes',\n",
       " 'ref_doc_info',\n",
       " 'refresh',\n",
       " 'refresh_ref_docs',\n",
       " 'service_context',\n",
       " 'set_index_id',\n",
       " 'storage_context',\n",
       " 'summary',\n",
       " 'update',\n",
       " 'update_ref_doc',\n",
       " 'vector_store']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(lct.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee02e6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "\n",
      "The content is about SAP HANA and its related technologies, such as SAP HANA Cloud's Auto ML capabilities, SAP HANA Python Client API for Machine Learning Algorithms, and SAP HANA Predictive Analysis Library (PAL). It also includes information about a book related to SAP HANA and a blog post about SAP HANA Machine Learning with ABAP Managed Database Procedures in SAP BW/4HANA.\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# time_now()\n",
    "question = \"What is content about?\"\n",
    "lct.post_question(question)\n",
    "print(lct.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39348fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1237 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The SAP HANA Cloud Machine Learning Challenge team organized the Community Call.\n"
     ]
    }
   ],
   "source": [
    "question = \"Who organized the Community Call?\"\n",
    "lct.post_question(question)\n",
    "print(lct.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ccef2f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Participants must solve the problem of predicting employee churn.\n"
     ]
    }
   ],
   "source": [
    "question = \"What problem participants must solve?\"\n",
    "lct.post_question(question)\n",
    "print(lct.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2914750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data for predicting employee churn can include information about the employee such as their job title, years of experience, salary, performance reviews, and other factors that may influence their decision to stay or leave the company. Additionally, data can be collected from the company itself, such as the onboarding process, internal employee turnover, and learning recommendations. By analyzing this data, patterns can be identified that can help to better understand employee churn and make more effective business decisions.\n"
     ]
    }
   ],
   "source": [
    "question = \"Explain data for predicting employee churn\"\n",
    "lct.post_question(question)\n",
    "print(lct.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "443189c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The participants utilized the HybridGradientBoostingTree model for their machine learning.\n"
     ]
    }
   ],
   "source": [
    "question = \"Can you tell me which machine learning models were utilized by the participants?\"\n",
    "lct.post_question(question)\n",
    "print(lct.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29f335b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The top 5 important features discovered by the model are: SICKDAYS, HRTRAINING, PREVIOUS_CAREER_PATH, LINKEDIN, and FUNCTIONALAREACHANGETYPE.\n"
     ]
    }
   ],
   "source": [
    "question = \"Which are the top 5 important features discoverd by the model?\"\n",
    "lct.post_question(question)\n",
    "print(lct.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05302378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The following code is an example of using the SAP HANA Python Client API for Machine Learning Algorithms to implement a HGBT (Hierarchical Gradient Boosting Tree) model. \n",
      "\n",
      "# Import the necessary libraries\n",
      "import hana_ml\n",
      "from hana_ml.algorithms.apl.hgbt import HGBT\n",
      "\n",
      "# Create a connection to the SAP HANA system\n",
      "connection_context = hana_ml.dataframe.ConnectionContext(address='<hostname>:<port>',\n",
      "                                                        user='<username>',\n",
      "                                                        password='<password>')\n",
      "\n",
      "# Load the data into a dataframe\n",
      "df = connection_context.table('<schema>.<table>')\n",
      "\n",
      "# Create the HGBT model\n",
      "hgbt = HGBT(conn_context=connection_context)\n",
      "\n",
      "# Fit the model\n",
      "hgbt.fit(data=df, key='<key_column>', label='<label_column>')\n",
      "\n",
      "# Make predictions\n",
      "predictions = hgbt.predict(data=df)\n",
      "\n",
      "# Evaluate the model\n",
      "hgbt.evaluate(data=df, label='<label_column>')\n"
     ]
    }
   ],
   "source": [
    "question = \"Python full code SAP HANA Machine learning HGBT example\"\n",
    "lct.post_question(question)\n",
    "print(lct.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f4dee16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In [1]:\n",
      "# Import the necessary libraries\n",
      "import hana_ml\n",
      "import pandas as pd\n",
      "\n",
      "# Load the CSV file into a Python object (Pandas DataFrame)\n",
      "df_data = pd.read_csv(r'Emp_Churn_Train.csv', sep = ',')\n",
      "\n",
      "# Create a connection to the HANA system\n",
      "connection_context = hana_ml.dataframe.ConnectionContext(address='<HANA_SYSTEM_ADDRESS>', port=<HANA_SYSTEM_PORT>, user='<HANA_SYSTEM_USER>', password='<HANA_SYSTEM_PASSWORD>')\n",
      "\n",
      "# Create a dataframe object from the Pandas DataFrame\n",
      "df_remote = connection_context.table('EMP_CHURN_TRAIN', schema='<HANA_SYSTEM_SCHEMA>', data=df_data)\n",
      "\n",
      "# Create training and testing set\n",
      "from hana_ml.algorithms.pal import partition\n",
      "hdf_train, hdf_test, hdf_val = partition.train_test_val_split( random_seed = 1017,\n",
      "                                                               data = df_remote, \n",
      "                                                               training_percentage = 0.8, \n",
      "                                                                testing_percentage = 0.1,\n",
      "                                                                validation_percentage = 0.1)\n",
      "\n",
      "# Union train and validation data into one set\n",
      "df_trainval=hdf_train.select('*', ('1', 'TRAIN_VAL_INDICATOR' )).union(hdf_val.select('*', ('2', 'TRAIN_VAL_INDICATOR'))\n",
      "\n",
      "# Print the size of the training and testing set\n",
      "print('Size of training subset: ' + str(hdf_train.count()))\n",
      "print('Size of test subset: ' + str(hdf_test.count()))\n"
     ]
    }
   ],
   "source": [
    "question = \"Python full code hana_ml dataframe example\"\n",
    "lct.post_question(question)\n",
    "print(lct.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092a3e52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
