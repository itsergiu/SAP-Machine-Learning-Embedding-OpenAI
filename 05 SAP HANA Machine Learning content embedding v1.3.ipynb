{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37788ba0",
   "metadata": {},
   "source": [
    "### SAP Machine Learning Embedding in OpenAI - step 05\n",
    "##### Author: Sergiu Iatco. May, 2023\n",
    "https://people.sap.com/iatco.sergiu <br>\n",
    "https://www.linkedin.com/in/sergiuiatco/ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9498f9c",
   "metadata": {},
   "source": [
    "#### Collected content for embedding.\n",
    "Blogs:<br>\n",
    "https://blogs.sap.com/2022/11/07/sap-community-call-sap-hana-cloud-machine-learning-challenge-i-quit-how-to-prevent-employee-churn/\n",
    "https://blogs.sap.com/2022/11/28/i-quit-how-to-predict-employee-churn-sap-hana-cloud-machine-learning-challenge/\n",
    "https://blogs.sap.com/2022/12/22/sap-hana-cloud-machine-learning-challenge-2022-the-winners-are/\n",
    "\n",
    "https://blogs.sap.com/2023/01/09/sap-hana-cloud-machine-learning-challenge-i-quit-understanding-metrics/\n",
    "\n",
    "Documentation:<br>\n",
    "https://help.sap.com/doc/1d0ebfe5e8dd44d09606814d83308d4b/2.0.04/en-US/hana_ml.dataframe.html\n",
    "https://help.sap.com/doc/1d0ebfe5e8dd44d09606814d83308d4b/2.0.07/en-US/pal/algorithms/hana_ml.algorithms.pal.trees.HybridGradientBoostingClassifier.html\n",
    "\n",
    "GitHub Notebooks:<br>\n",
    "https://github.com/SAP-samples/hana-ml-samples/tree/main/Python-API/usecase-examples/sapcommunity-hanaml-challenge<br>\n",
    "https://github.com/itsergiu/sapcommunity-hanaml-challenge<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddc4bba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac381f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.core.debugger import set_trace\n",
    "# os.environ[\"OPENAI_API_KEY\"] = '<OPENAI_API_KEY>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cd5a045",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "import shutil\n",
    "import pathlib\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.CRITICAL)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# There are five standard levels for logging in Python, listed here in increasing order of severity:\n",
    "# DEBUG: Detailed information, typically of interest only when diagnosing problems.\n",
    "# INFO: Confirmation that things are working as expected.\n",
    "# WARNING: An indication that something unexpected happened or indicative of some problem in the near future (e.g., ‚Äòdisk space low‚Äô). The software is still working as expected.\n",
    "# ERROR: Due to a more serious problem, the software has not been able to perform some function.\n",
    "# CRITICAL: A very serious error, indicating that the program itself may be unable to continue running.\n",
    "\n",
    "class llama_context():\n",
    "    def __init__(self, path=None):\n",
    "        \n",
    "        if path!=None:\n",
    "            self.path = path\n",
    "        else:\n",
    "            self.path = ''\n",
    "        \n",
    "        perisit_sub_dir = \"storage\"\n",
    "        self.perisit_dir = os.path.join(self.path, perisit_sub_dir)\n",
    "        if not os.path.exists(self.perisit_dir):\n",
    "            os.makedirs(self.perisit_dir)\n",
    "        data_sub_dir = \"data\"\n",
    "        self.data_dir = os.path.join(self.path, data_sub_dir)\n",
    "        self.data_dir_counter = 0\n",
    "        \n",
    "        self.cost_model_ada = \"ada\" # https://openai.com/pricing\n",
    "        self.cost_model_davinci = \"davinci\" # https://openai.com/pricing\n",
    "        self.price_ada_1k_tokens = 0.0004\n",
    "        self.price_davinci_1k_tokens = 0.03 \n",
    "\n",
    "        \n",
    "    def load_data(self):\n",
    "        self.documents = SimpleDirectoryReader(self.data_dir).load_data()\n",
    "        print(f\"Documents loaded: {len(self.documents)}.\")\n",
    "    def create_vector_store(self):\n",
    "        self.index = GPTVectorStoreIndex.from_documents(self.documents)\n",
    "        print(\"GPTVectorStoreIndex complete.\")\n",
    "    def save_index(self):\n",
    "        self.index.storage_context.persist(persist_dir=self.perisit_dir)\n",
    "        print(f\"Index saved in path {self.perisit_dir}.\")\n",
    "    def load_index(self):\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=self.perisit_dir)\n",
    "        self.index = load_index_from_storage(storage_context)\n",
    "    def start_query_engine(self):\n",
    "        self.query_engine = self.index.as_query_engine()\n",
    "        print(\"Query_engine started.\")\n",
    "    def post_question(self, question, sleep = None):\n",
    "        if sleep == None:\n",
    "            self.sleep = 0 # trial 20s\n",
    "        self.response_cls = self.query_engine.query(question)\n",
    "        self.response = self.response_cls.response\n",
    "\n",
    "    def del_data_dir(self):\n",
    "        path = self.data_dir\n",
    "        try:\n",
    "            shutil.rmtree(path)\n",
    "            print(f\"{path} deleted successfully!\")\n",
    "        except OSError as error:\n",
    "            print(f\"Error deleting {path}: {error}\")\n",
    "\n",
    "    def copy_file_to_data_dir(self, file_extension ='.txt', verbose = 0):\n",
    "\n",
    "        path_from = self.path\n",
    "        path_to = self.data_dir\n",
    "\n",
    "        if not os.path.exists(path_to):\n",
    "            os.makedirs(path_to)\n",
    "\n",
    "        for filename in os.listdir(path_from):\n",
    "            if filename.endswith(file_extension):\n",
    "                source_path = os.path.join(path_from, filename)\n",
    "                dest_path = os.path.join(path_to, filename)\n",
    "                shutil.copy(source_path, dest_path)\n",
    "                if verbose == 1:\n",
    "                    print(f\"File {filename} copied successfully!\")\n",
    "    \n",
    "        path_to_lib = pathlib.Path(path_to)\n",
    "        path_to_lib_files = path_to_lib.glob(f\"*{file_extension}\")\n",
    "        print(f\"Files {len(list(path_to_lib_files))} copied in {path_to}.\")\n",
    " \n",
    "    def copy_path_from_to_data_dir(self, path_from, file_extension ='.txt', verbose = 0):\n",
    "\n",
    "        path_to = self.data_dir # default data folder for llama\n",
    "        start_counter = self.data_dir_counter\n",
    "        \n",
    "        if not os.path.exists(path_to):\n",
    "            os.makedirs(path_to)\n",
    "\n",
    "        padding_n = 5\n",
    "        path_from_lib = pathlib.Path(path_from)\n",
    "        path_from_lib_files = path_from_lib.glob(f\"**/*{file_extension}\")\n",
    "\n",
    "        files_copied_n = 0\n",
    "        counter = None\n",
    "        for counter, file in enumerate(path_from_lib_files, start_counter):\n",
    "            filename_path = os.path.split(file)[0] # path only\n",
    "            filename = os.path.split(file)[1] # filename only\n",
    "            filename_with_index = f'{str(counter).zfill(padding_n)}_{filename}'\n",
    "            file_to_data_dir = os.path.join(path_to, filename_with_index)\n",
    "            shutil.copy(file, file_to_data_dir)\n",
    "            \n",
    "            if os.path.exists(file_to_data_dir):\n",
    "                files_copied_n += 1\n",
    "                if verbose == 1:\n",
    "                    print(f\"File {filename} -> copied successfully!\")\n",
    "            else:\n",
    "                if verbose == 1:\n",
    "                    print(f\"File {filename} was not copied!\")\n",
    "        \n",
    "#         if 'counter' in locals(): \n",
    "        if counter != None: \n",
    "            self.data_dir_counter = counter + 1 # start from last\n",
    "        \n",
    "        print(f\"Files: {files_copied_n} copied to folder: {path_to}!\")\n",
    "\n",
    "    def estimate_tokens(self, text):\n",
    "        words = text.split()\n",
    "\n",
    "        num_words = int(len(words))\n",
    "        tokens = int(( num_words / 0.75 ))\n",
    "        tokens_1k = tokens / 1000\n",
    "        cost_ada = tokens_1k * self.price_ada_1k_tokens\n",
    "        cost_davinci = tokens_1k * self.price_davinci_1k_tokens\n",
    "        return tokens, cost_ada, cost_davinci\n",
    "    \n",
    "    def estimate_cost(self):\n",
    "        total_tokens = 0\n",
    "        total_cost_ada = 0\n",
    "        total_cost_davinci = 0\n",
    "        costs_rounding = 8\n",
    "        \n",
    "        for doc in self.documents:\n",
    "            text = doc.get_text()\n",
    "            tokens, cost_ada, cost_davinci = self.estimate_tokens(text)\n",
    "            total_tokens += tokens\n",
    "            \n",
    "            total_cost_ada += cost_ada\n",
    "            total_cost_ada = round(total_cost_ada, costs_rounding)\n",
    "            \n",
    "            total_cost_davinci += cost_davinci\n",
    "            total_cost_davinci = round(total_cost_davinci, costs_rounding)\n",
    "            \n",
    "        self.total_tokens = total_tokens\n",
    "        self.total_cost_ada = total_cost_ada\n",
    "        self.total_cost_davinci = total_cost_davinci\n",
    "        print(f\"Total tokens: {self.total_tokens}\")\n",
    "        print(f\"Total estimated costs with model {self.cost_model_ada }: ${self.total_cost_ada}\")\n",
    "        print(f\"Total estimated costs with model {self.cost_model_davinci }: ${self.total_cost_davinci}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2375e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def time_now():\n",
    "    now = datetime.datetime.now()\n",
    "    formatted = now.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(formatted)\n",
    "\n",
    "# time_now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7578ebdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'llama_challenge'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'llama_challenge\\\\data'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'llama_challenge\\\\storage'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path_llama = \"llama_challenge\"\n",
    "lct = llama_context(path=path_llama)\n",
    "\n",
    "display(lct.path)\n",
    "display(lct.data_dir)\n",
    "display(lct.perisit_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "148d2c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "Error deleting llama_challenge\\data: [WinError 3] The system cannot find the path specified: 'llama_challenge\\\\data'\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# time_now()\n",
    "run_load_create_save = True\n",
    "if run_load_create_save:\n",
    "    lct.del_data_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "717c21a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "Files: 6 copied to folder: llama_challenge\\data!\n",
      "Files: 1 copied to folder: llama_challenge\\data!\n",
      "Files: 5 copied to folder: llama_challenge\\data!\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# time_now()\n",
    "# run_load_create_save = False\n",
    "if run_load_create_save:\n",
    "\n",
    "    path_from1 = \"llama_challenge//html_challenge\"\n",
    "    path_from2 = \"llama_challenge//ipynb_blog\"\n",
    "    path_from3 = \"llama_challenge//ipynb_hana_ml_samples//Python-API//usecase-examples//sapcommunity-hanaml-challenge\"\n",
    "\n",
    "    lct.copy_path_from_to_data_dir(path_from1) # default extension *.txt\n",
    "    lct.copy_path_from_to_data_dir(path_from2) # default extension *.txt\n",
    "    lct.copy_path_from_to_data_dir(path_from3) # default extension *.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e4345e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['path', 'perisit_dir', 'data_dir', 'data_dir_counter', 'cost_model_ada', 'cost_model_davinci', 'price_ada_1k_tokens', 'price_davinci_1k_tokens'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(lct).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a451b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "Documents loaded: 12.\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# time_now()\n",
    "run_load_create_save = True\n",
    "if run_load_create_save:\n",
    "    lct.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bee30ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "Total tokens: 43819\n",
      "Total estimated costs with model ada: $0.0175276\n",
      "Total estimated costs with model davinci: $1.31457\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# time_now()\n",
    "run_load_create_save = True\n",
    "if run_load_create_save:\n",
    "    lct.estimate_cost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f6ad1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1025 > 1024). Running this sequence through the model will result in indexing errors\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 147741 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [build_index_from_nodes] Total embedding token usage: 147741 tokens\n",
      "GPTVectorStoreIndex complete.\n"
     ]
    }
   ],
   "source": [
    "# https://platform.openai.com/account/api-keys\n",
    "%time\n",
    "# time_now()\n",
    "# run_load_create_save = False\n",
    "if run_load_create_save:\n",
    "    lct.create_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb6d9c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "Index saved in path llama_challenge\\storage.\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# time_now()\n",
    "# run_load_create_save = False\n",
    "if run_load_create_save:\n",
    "    lct.save_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5472eff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.loading:Loading all indices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# time_now()\n",
    "lct.load_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f120dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lct.index.vector_store.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "247f52cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(lct.index.vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "952ee2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on SimpleVectorStore in module llama_index.vector_stores.simple object:\n",
      "\n",
      "class SimpleVectorStore(llama_index.vector_stores.types.VectorStore)\n",
      " |  SimpleVectorStore(*args, **kwds)\n",
      " |  \n",
      " |  Simple Vector Store.\n",
      " |  \n",
      " |  In this vector store, embeddings are stored within a simple, in-memory dictionary.\n",
      " |  \n",
      " |  Args:\n",
      " |      simple_vector_store_data_dict (Optional[dict]): data dict\n",
      " |          containing the embeddings and doc_ids. See SimpleVectorStoreData\n",
      " |          for more details.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SimpleVectorStore\n",
      " |      llama_index.vector_stores.types.VectorStore\n",
      " |      typing.Protocol\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, data: Union[llama_index.vector_stores.simple.SimpleVectorStoreData, NoneType] = None, **kwargs: Any) -> None\n",
      " |      Initialize params.\n",
      " |  \n",
      " |  __subclasshook__ = _proto_hook(other)\n",
      " |      # Set (or override) the protocol subclass hook.\n",
      " |  \n",
      " |  add(self, embedding_results: List[llama_index.vector_stores.types.NodeEmbeddingResult]) -> List[str]\n",
      " |      Add embedding_results to index.\n",
      " |  \n",
      " |  delete(self, doc_id: str, **delete_kwargs: Any) -> None\n",
      " |      Delete a document.\n",
      " |  \n",
      " |  get(self, text_id: str) -> List[float]\n",
      " |      Get embedding.\n",
      " |  \n",
      " |  persist(self, persist_path: str) -> None\n",
      " |      Persist the SimpleVectorStore to a directory.\n",
      " |  \n",
      " |  query(self, query: llama_index.vector_stores.types.VectorStoreQuery) -> llama_index.vector_stores.types.VectorStoreQueryResult\n",
      " |      Get nodes for response.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_persist_dir(persist_dir: str = './storage') -> 'SimpleVectorStore' from typing._ProtocolMeta\n",
      " |  \n",
      " |  from_persist_path(persist_path: str) -> 'SimpleVectorStore' from typing._ProtocolMeta\n",
      " |      Create a SimpleKVStore from a persist directory.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  client\n",
      " |      Get client.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'stores_text': <class 'bool'>}\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  stores_text = False\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from llama_index.vector_stores.types.VectorStore:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from llama_index.vector_stores.types.VectorStore:\n",
      " |  \n",
      " |  is_embedding_query = True\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Protocol:\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from typing._ProtocolMeta\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from typing._ProtocolMeta\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwds)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(lct.index.vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f078570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'copy_file_to_data_dir',\n",
       " 'copy_path_from_to_data_dir',\n",
       " 'cost_model_ada',\n",
       " 'cost_model_davinci',\n",
       " 'create_vector_store',\n",
       " 'data_dir',\n",
       " 'data_dir_counter',\n",
       " 'del_data_dir',\n",
       " 'documents',\n",
       " 'estimate_cost',\n",
       " 'estimate_tokens',\n",
       " 'index',\n",
       " 'load_data',\n",
       " 'load_index',\n",
       " 'path',\n",
       " 'perisit_dir',\n",
       " 'post_question',\n",
       " 'price_ada_1k_tokens',\n",
       " 'price_davinci_1k_tokens',\n",
       " 'save_index',\n",
       " 'start_query_engine',\n",
       " 'total_cost_ada',\n",
       " 'total_cost_davinci',\n",
       " 'total_tokens']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(lct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cb5b568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on llama_context in module __main__ object:\n",
      "\n",
      "class llama_context(builtins.object)\n",
      " |  llama_context(path=None)\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, path=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  copy_file_to_data_dir(self, file_extension='.txt', verbose=0)\n",
      " |  \n",
      " |  copy_path_from_to_data_dir(self, path_from, file_extension='.txt', verbose=0)\n",
      " |  \n",
      " |  create_vector_store(self)\n",
      " |  \n",
      " |  del_data_dir(self)\n",
      " |  \n",
      " |  estimate_cost(self)\n",
      " |  \n",
      " |  estimate_tokens(self, text)\n",
      " |  \n",
      " |  load_data(self)\n",
      " |  \n",
      " |  load_index(self)\n",
      " |  \n",
      " |  post_question(self, question, sleep=None)\n",
      " |  \n",
      " |  save_index(self)\n",
      " |  \n",
      " |  start_query_engine(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(lct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a1cb851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "Query_engine started.\n"
     ]
    }
   ],
   "source": [
    "# lct.vector_store.to_dict()\n",
    "%time\n",
    "# time_now()\n",
    "lct.start_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f5b399a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lct.documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17871a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': 'llama_challenge',\n",
       " 'perisit_dir': 'llama_challenge\\\\storage',\n",
       " 'data_dir': 'llama_challenge\\\\data',\n",
       " 'data_dir_counter': 12,\n",
       " 'cost_model_ada': 'ada',\n",
       " 'cost_model_davinci': 'davinci',\n",
       " 'price_ada_1k_tokens': 0.0004,\n",
       " 'price_davinci_1k_tokens': 0.03,\n",
       " 'documents': [Document(text='\\nSAP HANA Cloud Machine Learning Challenge ‚ÄúI quit!‚Äù ‚Äì understanding metrics | SAP Blogs\\n \\nSkip to Content\\nSAP Community Log-in UpdateIn a few months, SAP Community will switch to SAP Universal ID as the only option to login. Don‚Äôt wait, create your SAP Universal ID now! If you have multiple accounts, use the Consolidation Tool to merge your content.Get started with SAP Universal ID\\nHome\\nCommunity\\nAsk a Question\\nWrite a Blog Post\\nLogin / Sign-up\\n \\nTechnical Articles\\n \\nSergiu Iatco\\nJanuary 9, 2023\\n15 minute read\\nSAP HANA Cloud Machine Learning Challenge ‚ÄúI quit!‚Äù ‚Äì understanding metrics\\n4        \\n13        \\n1,402        \\nAuthor: Sergiu Iatco\\xa0\\nI participated in the SAP HANA ML Challenge ‚Äì Employee Churn ü§ñ and I came in second place üèÜ.\\nI would like to thank the entire SAP expert team for the initiative and organization! üëè\\nPresentation of the winners‚Äô solutions on YouTube:\\nHow to prevent Employee Churn using SAP HANA Cloud\\u200b | SAP Community Call\\nMy achievement post on LinkedIn.\\nTable of content\\n1. Getting started\\n2. Understanding the challenge\\n3. Exploratory data analysis and pre-processing\\n4. Baseline model with initial data\\n5. Understanding statistics\\n6. Training model with additional data\\n7. Reducing the number of features\\n8. Balancing training data with SMOTE\\n9. Balancing test data and increasing training data\\n10. Finding top SMOTE\\n11. Building top models\\n12. Analyzing top models\\n13. Visualizing statistics\\n14. Further steps\\n1. Getting started\\nThe challenge was an opportunity for me to get hands-on experience with SAP Python Machine Learning Client for SAP HANA. The participants had continuous support from SAP team experts during open office hours, discussions on SAP Community Groups,\\xa0and tutorials on GitHub. I gained my previous experience from Kaggle and SAP open courses. Participants without SAP HANA Cloud instance, myself included, received credentials to execute Jupyter Notebooks in the cloud. The first step was to set up the environment. I installed Anaconda, created the environment, and installed SciPy and hana-ml.\\nconda create --name ml_hana python=3.8\\nconda install -c conda-forge scikit-learn\\nconda install -c anaconda pandas\\nconda install -c anaconda jupyter\\npip install hana-ml\\nAfter successfully testing the connection, I was ready to dig deeper into the problem we needed to solve.\\n2. Understanding the challenge\\nThe task is to build a model that predicts if an employee will stay or quit. The collected data were provided in a CSV file. To look into the data we can use python libraries locally for light tasks and hana_ml for resource-consuming tasks. For data processing locally I used Pandas DataFrame and for the cloud the HANA ML DataFrame, libraries are aligned in terms of methods, but the syntax is specific for each. I received all the required support from the expert team and I moved on with my exploratory data analysis. I focused on building a model with the best results and alternatives to it and providing a possibility to apply A/B testing on deployed models. The results of the final model I presented with visualization libraries. The machine learning problem of the challenge is a classification problem with two classes. One class is Stay labeled with No and another one is Quit labeled with Yes. I call them class No and class Yes. The class we have to predict is Yes. Counting the rows we can find that class No : Class Yes = 17015:2100 = 8:1. Looking at proportions I can see that data are highly imbalanced and I have a majority class No and a minority class Yes. Minority class Yes is the one I have to predict.\\nThe final code in Jupyter Notebook contains 270 cells. In the blog, I will only go through the main parts. The entire code is on GitHub, published with my last run. As with any code, it can be improved further if I had to maintain it in the long run.\\n3. Exploratory data analysis and pre-processing\\nAnalyzing feature distributions.\\n# In [26]\\n# EDA features distributions\\ncol_exc = [\\'ID\\']\\nfor col in df_remote_add.columns:\\n    if col not in col_exc:\\n        print(df_remote_add.agg([(\\'count\\', col, \\'Count\\')], group_by=col).collect())\\nData contains Null values and I filled them with ‚Äò-999‚Äô.\\n# In [31]:\\ndf_remote = df_remote.fillna(-999)\\ndf_remote = df_remote.fillna(\"-999\")\\nFeature engineering.\\n# In [34]\\n# TIMEINPREVPOSITIONMONTH - feature engineering group in intervals\\n# I will check by importance if this mattters\\n    \\ndf_remote = df_remote.select \\\\\\n    (\\'*\\',(\"\"\"CASE WHEN TIMEINPREVPOSITIONMONTH <=12 THEN \\'PREV_POS (,12]\\'\\n                  WHEN TIMEINPREVPOSITIONMONTH > 12 AND TIMEINPREVPOSITIONMONTH <=24 THEN \\'PREV_POS (12,24]\\'\\n                  WHEN TIMEINPREVPOSITIONMONTH > 24 AND TIMEINPREVPOSITIONMONTH <=36 THEN \\'PREV_POS (24,36]\\'\\n                  WHEN TIMEINPREVPOSITIONMONTH > 36 AND TIMEINPREVPOSITIONMONTH <=48 THEN \\'PREV_POS (36,48]\\'\\n                  WHEN TIMEINPREVPOSITIONMONTH > 48 AND TIMEINPREVPOSITIONMONTH <=60 THEN \\'PREV_POS (48,60]\\'\\n                  WHEN TIMEINPREVPOSITIONMONTH > 60 THEN \\'TIMEPREV_POS (60,]\\'\\n              END\"\"\",\\'TIMEINPREVPOS_INT\\'))\\ndf_remote = df_remote.to_tail(\\'FLIGHT_RISK\\')\\ndf_remote.head(3).collect()\\n4. Baseline model with initial data\\nI split data into train data and test data and fit train data to build the first baseline model.\\n# In [42]:\\nfrom hana_ml.algorithms.pal.partition import train_test_val_split as split\\nd_train, d_test, d_val = split( data=df_remote, partition_method=\\'stratified\\', stratified_column=target, validation_percentage = 0)\\nTo build the model I used HybridGradientBoostingTree wrapped in a function.\\nI didn‚Äôt have to encode features the model did it for me. The model did the magic, but I had to feed it with data in the most efficient way step by step.\\n# In [47]:\\n# Function to use repeatedly\\ndef model_fit(d_train):\\n    HGBT_MODEL = UnifiedClassification(\\'HybridGradientBoostingTree\\')\\n\\xa0\\n# In [56]:\\nHGBT_MODEL.statistics_.collect() # MODEL statistics\\nOut [56]:\\n1\\nRECALL\\n0.9959588537839824\\nNo\\n2\\nPRECISION\\n0.9249402934152167\\nNo\\n3\\nF1_SCORE\\n0.9591367415531578\\nNo\\n4\\nSUPPORT\\n2722\\nNo\\n5\\nRECALL\\n0.34523809523809523\\nYes\\n6\\nPRECISION\\n0.9133858267716536\\nYes\\n7\\nF1_SCORE\\n0.5010799136069115\\nYes\\n8\\nSUPPORT\\n336\\nYes\\n9\\nACCURACY\\n0.9244604316546763\\nNone\\nLooking into model statistics at first glance I can consider myself very lucky with ACCURACY = 0.92, but looking at SUPPORT No = 2722 and SUPPORT Yes = 336 I recognize that validation data used for models metrics calculations is imbalanced and this high ACCURACY is because of the majority class No with RECALL No = 0.99, while RECALL Yes = 0.34. Since I am interested in the class Yes I can see that the results are very poor. What is a RECALL? I spent some time breaking down the confusion matrix into basic elements. Because if I don‚Äôt feel confident with metrics I would not be able to improve the prediction model. Computers are fast and accurate at calculations, but the reasoning is a human task.\\n5. Understanding statistics\\nTo calculate the metrics of the confusion matrix all we need is algebra, first, we count TP (true positive), FP (false positive), FN (false negative), and TN (true negative), then with these four elements we can calculate all the metrics. If classes are imbalanced I can rely only on RECALL because calculations are performed with the prediction from each class RECALL Positive = TP / TP + FP and RECALL Negative = TN / TN + FN. To calculate PRECISION I need true prediction (TP) from one class and false predictions (FP) from another class. To calculate ACCURACY I need all four elements TP, FP, FN, and TN, and with imbalanced classes, results are misleading because the extremely good results of the majority class and poor results of the minority class will make overall results look good. You can also read this blog Failure of Classification Accuracy for Imbalanced Class Distributions.\\nLet‚Äôs deep into confusion matrix principles, best explained with an example.\\nPredicted No\\nPredicted Yes\\nActual No\\nTrue No\\nFalse Yes\\nActual Yes\\nFalse No\\nTrue Yes\\nAssume one built a model with extremely good metrics for majority class No.\\nMajority class No, Minority class Yes = 900, 100\\nAssume confusion matrix results.\\nPredicted No\\nPredicted Yes\\nActual No\\n900\\n9\\nActual Yes\\n82\\n9\\nModel statistics.\\nRECALL No\\n0.99\\nextremely good\\nPRECISION No\\n0.92\\nvery good\\nF1_SCORE No\\n0.95\\nvery good\\nSUPPORT No\\n909\\nRECALL Yes\\n0.10\\nvery poor\\nPRECISION Yes\\n0.50\\npoor\\nF1_SCORE Yes\\n0.17\\nvery poor\\nSUPPORT Yes\\n91\\nACCURACY\\n0.91\\nvery good\\nIf I wanted to rely on all metrics I would force balancing classes in the confusion matrix dividing Actual No by 9.\\nPredicted No\\nPredicted Yes\\nActual No\\n90\\n1\\nActual Yes\\n82\\n9\\nModel statistics adjusted.\\nRECALL No\\n0.99\\nvery good (same)\\nPRECISION No\\n0.52\\npoor\\nF1_SCORE No\\n0.68\\ngood\\nSUPPORT No\\n91\\nRECALL Yes\\n0.10\\nvery poor (same)\\nPRECISION Yes\\n0.90\\nvery good\\nF1_SCORE Yes\\n0.18\\nvery poor\\nSUPPORT Yes\\n91\\nACCURACY\\n0.54\\npoor\\nNow I can see that with balanced metrics RECALL No and RECALL Yes remain the same, PRECISON No drops significantly, PRECISION Yes increases significantly, but F1_SCORE Yes drops to very poor, and ACCURACY drops from 0.91 (very good) to 0.54 (poor).\\nYou can read more about balanced accuracy score here.\\nModel statistics calculates metrics with validations data split from train data. Validation data is unseen data however used in fitting for iterative optimization.\\nTo see how the model performs we have to calculate metrics for absolutely unseen data the test data.\\nPrediction statistics wrapped in a local function.\\n# In [61]:\\nf_score_res(HGBT_MODEL,d_test)[1].collect()\\nOut [61]:\\nSTAT_NAME\\nSTAT_VALUE\\nCLASS_NAME\\n1\\nRECALL\\n0.9964747356051704\\nNo\\n2\\nPRECISION\\n0.9247546346782988\\nNo\\n3\\nF1_SCORE\\n0.9592760180995475\\nNo\\n4\\nSUPPORT\\n1702\\nNo\\n5\\nRECALL\\n0.34285714285714286\\nYes\\n6\\nPRECISION\\n0.9230769230769231\\nYes\\n7\\nF1_SCORE\\n0.5000000000000001\\nYes\\n8\\nSUPPORT\\n210\\nYes\\n9\\nACCURACY\\n0.9246861924686193\\nNone\\nRECALL Yes = 0.34 for predicted data is very poor and ACCURACY = 0.92 doesn‚Äôt tell the true story.\\nIn order to analyze the performance of subsequent approaches I will collect results in a local pandas DataFrame wrapped in a local function to add subsequent models results.\\n# In [64]:\\ndef f_stat_all(p_model, p_d_test, p_col_name):\\ndf_model_stat_last # model metrics\\ndf_pred_stat_last  # prediction metrics\\n\\xa0\\nI will store the initial baseline results in column INIT.\\n# In [65]:\\ndf_model_stat_last, df_pred_stat_last = f_stat_all(HGBT_MODEL, d_test, \\'INIT\\')\\nI will take a look at the initial feature importance.\\n# In [68]:\\n# Feature importance\\nHGBT_MODEL.importance_.sort(\\'IMPORTANCE\\', desc=True).collect()\\nOut [68]:\\nThe top feature by importance is\\xa0 FUNCTIONALAREACHANGETYPE.\\n6. Training model with additional data\\nMeanwhile, additional data has been provided; the shape of the data is consistent.\\n# In [19]:\\ndf_data_add.shape, df_remote.shape # check rows and columns\\nOut [19]:\\n((19115, 4), [19115, 43])\\nJoining of initial data with additional data.\\n# In [70]:\\ndf_remote_new = df_remote.set_index(tab_id).join(df_remote_add.set_index(tab_id)) \\nAfter fitting the model statistics are the following:\\nOut [75]:\\n1\\nRECALL\\n0.979059515062454\\nNo\\n4\\nSUPPORT\\n2722\\nNo\\n5\\nRECALL\\n0.6101190476190477\\nYes\\n8\\nSUPPORT\\n336\\nYes\\nI observe that the RECALL Yes of model statistics increased to a good one 0.61.\\nAnalyzing feature importance I observe that the top feature HRTRAINING comes from additional data.\\n# Out [76]:\\ndf_imp = HGBT_MODEL.importance_.sort(\\'IMPORTANCE\\', desc=True).collect()\\nPrediction statistics.\\nOut [78]:\\n1\\nRECALL\\n0.9800235017626322\\nNo\\n4\\nSUPPORT\\n1702\\nNo\\n5\\nRECALL\\n0.5714285714285714\\nYes\\n8\\nSUPPORT\\n210\\nYes\\nRECALL Yes of prediction statistics is 0.57 is acceptable and less than model statistics 0.61.\\nI conclude that predictions improve.\\n7. Reducing the number of features\\nLooking further into the data I notice dependent features by description and content.\\n# In [87]:\\ncol_age_dep = [\\'AGE\\', \\'AGE_GROUP5\\', \\'AGE_GROUP10\\', \\'GENERATION\\'] # age group\\n# In [90]:\\ncol_imp_prev_loc = [\\'PREVIOUS_COUNTRY\\', \\'PREVCOUNTRYLAT\\', \\'PREVCOUNTRYLON\\', \\'PREVIOUS_REGION\\'] # previous country group\\n# In [92]:\\ncol_imp_curr_loc = [\\'CURRENT_REGION\\', \\'CURRENT_COUNTRY\\', \\'CURCOUNTRYLAT\\', \\'CURCOUNTRYLON\\'] # current country group\\n# In [94:]\\ncol_imp_tenure = [\\'TENURE_MONTHS\\',\\'TENURE_INTERVAL_YEARS\\',\\'TENURE_INTERVALL_DESC\\'] # tenure group\\n\\xa0\\nFrom each group, I select the first top feature and fit again the model.\\nRECALL Yes from prediction statistics improves to 0.61 from 0.57 which means that model performs better with only top features from groups.\\n# In [104]:\\nf_score_res(HGBT_MODEL, d_test[cols_imp])[1].collect()\\nOut [104]:\\n1\\nRECALL\\n0.9782608695652174\\nNo\\n4\\nSUPPORT\\n1702\\nNo\\n5\\nRECALL\\n0.6190476190476191\\nYes\\n8\\nSUPPORT\\n210\\nYes\\nFor countries, I prefer adjusted selection with top PREVIOUS_COUNTRY, and CURRENT_COUNTRY.\\n# In [110]:\\n# Reducing features with meaning preference\\nCOL_IMP_PREVIOUS_COUNTRY_MEANING = [\\'PREVIOUS_COUNTRY\\',\\'PREVCOUNTRYLAT\\', \\'PREVCOUNTRYLON\\', \\'PREVIOUS_REGION\\']\\nCOL_IMP_CURRENT_REGION_MEANING = [\\'CURRENT_COUNTRY\\',\\'CURRENT_REGION\\', \\'CURCOUNTRYLAT\\', \\'CURCOUNTRYLON\\']\\nFitting with meaning preference.\\n# In [114]:\\nf_score_res(HGBT_MODEL, d_test[cols_imp_mn])[1].collect() # meaning preference\\nPrediction statistics.\\nOut [114]:\\n1\\nRECALL\\n0.981198589894242\\nNo\\n4\\nSUPPORT\\n1702\\nNo\\n5\\nRECALL\\n0.6238095238095238\\nYes\\n8\\nSUPPORT\\n210\\nYes\\nI observe that RECALL Yes is almost the same 0.62.\\n8. Balancing training data with SMOTE\\nA quick introduction to imbalanced data you could find in the blog Data Yoga-It is all about finding the right balance. For oversampling of minority class Yes\\xa0I will use SMOTE (Synthetic minority over-sampling technique).\\nChecking class proportions from train data.\\n# In [124]:\\nf_class_count(d_train).collect()\\nOut [124]:\\n\\xa0\\nFLIGHT_RISK\\nCount\\n0\\nNo\\n13612\\n1\\nYes\\n1680\\nI know that classes are highly imbalanced and I want to find a way to fix this with SMOTE.\\nI wrapped the calculation of max SMOTE into a local function.\\n# In [126]:\\n# return max smote amount\\ndef f_smote_nmax(dfh):\\nThe max SMOTE is 710.\\nGenerating train data with max SMOTE to get balanced classes.\\n# In [129]:\\nd_train_sm = f_smote(d_train,f_smote_nmax(d_train)) # generate smote with max amount\\nChecking the proportions of the classes with max SMOTE.\\n# In [130]:\\nf_class_count(d_train_sm).collect() #check classes count for Majority No:Minority Yes = 1:1\\nOut [130]:\\n\\xa0\\nFLIGHT_RISK\\nCount\\n0\\nNo\\n13612\\n1\\nYes\\n13608\\n# In [134]:\\n# Fit with smote balanced No:Yes with all features\\nHGBT_MODEL=model_fit(d_train_sm)\\nPrediction statistics.\\nOut [137]:\\n1\\nRECALL\\n0.945358401880141\\nNo\\n3\\nF1_SCORE\\n0.9600238663484487\\nNo\\n4\\nSUPPORT\\n1702\\nNo\\n5\\nRECALL\\n0.8047619047619048\\nYes\\n8\\nSUPPORT\\n210\\nYes\\nRECALL Yes jumps from 0.62 to 0.80, which means SMOTE improves the model.\\nThe fitting of the model with max SMOTE and important features.\\n# In [144]:\\n# Fit with smote balanced No:Yes with important features\\nHGBT_MODEL=model_fit(d_train_sm[cols_imp])\\nPrediction statistics\\nOut [146]:\\n1\\nRECALL\\n0.9195064629847238\\nNo\\n2\\nPRECISION\\n0.9726538222498446\\nNo\\n4\\nSUPPORT\\n1702\\nNo\\n5\\nRECALL\\n0.7904761904761904\\nYes\\n8\\nSUPPORT\\n210\\nYes\\nRECALL Yes with important features and max SMOTE almost same 0.79\\n9. Balancing test data and increasing training data\\nI notice that SUPPORT Yes \\xa0< \\xa0SUPPORT No. To fix it I will keep for test rows SUPPORT No = SUPPORT Yes. The rest of the SUPPORT No rows I will add to train data because I want to use precious data.\\nBalanced test data.\\n# In [164]:\\nf_class_count(d_test_bl).collect() # check test classes are balanced\\nOut [164]:\\nFLIGHT_RISK\\nCount\\n0\\nYes\\n210\\n1\\nNo\\n210\\nIncreased rows of train data.\\n# In [166]:\\nd_train_target_count = f_class_count(d_train_bl) # added No from d_test that are not used (unseen)\\nd_train_target_count.collect()\\nOut [166]:\\nFLIGHT_RISK\\nCount\\n0\\nNo\\n15104\\n1\\nYes\\n1680\\nApplying SMOTE to train data.\\n# In [168]:\\nf_class_count(d_train_bl_sm).collect() # #check classes count for Majority No:Minority Yes = 1:1\\nOut [168]:\\nFLIGHT_RISK\\nCount\\n0\\nNo\\n15104\\n1\\nYes\\n15103\\n# In [171]:\\n# Fit with balanced d_train_bl_sm important features\\nHGBT_MODEL=model_fit(d_train_bl_sm[cols_imp])\\nPrediction statistics.\\n# In [173]:\\nf_score_res(HGBT_MODEL, d_test_bl[cols_imp])[1].collect()\\nOut [173]:\\nSTAT_NAME\\nSTAT_VALUE\\nCLASS_NAME\\n1\\nRECALL\\n0.919047619047619\\nNo\\n2\\nPRECISION\\n0.8427947598253275\\nNo\\n3\\nF1_SCORE\\n0.8792710706150342\\nNo\\n4\\nSUPPORT\\n210\\nNo\\n5\\nRECALL\\n0.8285714285714286\\nYes\\n6\\nPRECISION\\n0.9109947643979057\\nYes\\n7\\nF1_SCORE\\n0.8678304239401496\\nYes\\n8\\nSUPPORT\\n210\\nYes\\n9\\nACCURACY\\n0.8738095238095238\\nNone\\nRECALL Yes has a small improvement to 0.82.\\nSUPPORT Yes = SUPPORT No that means I can rely on all statistics. I notice that ACCURACY is 0.87 and that is a very good one.\\n10. Finding top SMOTE\\nI want to execute SMOTE with the step of 100 and analyze ACCURACY from prediction statistics.\\n# In [188]:\\n# SMOTE with steps\\n# fire on all cylinders\\ndict_models_smote, df_model_statistics_sm, df_predicted_statistics_sm = \\\\\\nf_smote_models(d_train_bl, d_test_bl, cols_imp, 100)\\nls_smote: [0, 100, 200, 300, 400, 500, 600, 700, 799]\\nThis is an intensive processing in the HANA ML cloud with Wall time: 8min 34s.\\nModel statistics are collected in local pandas DataFrame.\\n# In [190]:\\ndf_model_statistics_sm\\nPrediction statistics are collected in the local pandas DataFrame.\\n# In [191]:\\ndf_predicted_statistics_sm\\nSelecting SMOTE for best prediction ACCURACY with local function.\\n# In [194]:\\n# SUPPORT Yes = SUPPORT No - ACCURACY reliable\\nf_accuracy_descending(df_predicted_statistics_sm)\\nOut [194]:\\nSTAT_NAME\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ACCURACY\\nSTAT_VALUE_700\\xa0\\xa0\\xa0 0.8738095238095238\\nSTAT_VALUE_200\\xa0\\xa0\\xa0 0.8666666666666667\\nSTAT_VALUE_500\\xa0\\xa0\\xa0 0.8666666666666667\\nSTAT_VALUE_400\\xa0\\xa0\\xa0 0.8642857142857143\\nSTAT_VALUE_600\\xa0\\xa0\\xa0 0.8642857142857143\\nSTAT_VALUE_799\\xa0\\xa0\\xa0 0.8642857142857143\\nSTAT_VALUE_300\\xa0\\xa0\\xa0 0.8547619047619047\\nSTAT_VALUE_100\\xa0\\xa0\\xa0 0.8523809523809524\\nSTAT_VALUE_0\\xa0 \\xa0 \\xa0 \\xa0 0.7928571428571428\\n11. Building top models\\nI fit models with smote_best_amount adding the features one by one in order of importance.\\n# In [207]:\\n# FEATURES IMPORTANCE TOP n\\n# fire on all cylinders\\ndef f_smote_model_top_f(p_d_train, p_cols_imp, p_d_test, p_smote_best_amount, p_cols_imp_start, p_cols_imp_step, p_verbosity):\\n# ...\\n   return lc_dict_models_top_f, lc_df_model_statistics, lc_df_predicted_statistics\\n\\xa0\\n# In [212]:\\n%%time\\n# FEATURES IMPORTANCE TOP n\\n# fire on all cylinders\\ndict_models_top_f, df_model_statistics_top_f, df_predicted_statistics_top_f = \\\\\\nf_smote_model_top_f(d_train_bl, sm_best_cols_imp, d_test_bl, smote_best_amount, 1, 1, True)\\nThis is very intensive processing in the HANA ML cloud with Wall time: 21min 6s.\\nRelax and enjoy a cup of coffee or tea! üôÇ\\nSelecting top features with top prediction ACCURACY with a local function.\\n# In [217]:\\n# SUPPORT Yes = SUPPORT No ACCURACY is reliable\\nf_accuracy_descending(df_predicted_statistics_top_f)[:10]\\nOut[217]\\nSTAT_NAME\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0ACCURACY\\nSTAT_VALUE_top15\\xa0 0.8928571428571429\\nSTAT_VALUE_top6\\xa0 \\xa0 0.888095238095238\\nSTAT_VALUE_top29\\xa0 0.8857142857142857\\nSTAT_VALUE_top7\\xa0 \\xa0 0.8833333333333333\\nSTAT_VALUE_top18\\xa0 0.8833333333333333\\nSTAT_VALUE_top8\\xa0 \\xa0 0.8833333333333333\\nSTAT_VALUE_top19\\xa0 0.8833333333333333\\nSTAT_VALUE_top25\\xa0 0.8809523809523809\\nSTAT_VALUE_top23\\xa0 0.8761904761904762\\nGreat surprise, with the best SMOTE and best top15 features prediction ACCURACY increases to 0.89!\\nI stored all models and other dependencies in the dictionary dict_models_top_f to access them easily.\\n# In [218]:\\ndef f_sel_top_pred_acc(p_df,p_n):\\n    s =  f_accuracy_descending(p_df) # sorted series\\n    ls_s = s[:p_n+1].index.to_list() # sorted list with last n+1\\n    key = ls_s[-1] # last element\\n    model = dict_models_top_f[key][0]\\n    cols = dict_models_top_f[key][1]\\n    cols_len = dict_models_top_f[key][2]\\n    cols_name = dict_models_top_f[key][3]\\n    return [key, model, cols, cols_len, cols_name]\\n\\xa0\\n# In [219]:\\n# key, model, cols, cols_len, cols_name Top 1\\ntop_1_imp_ls = f_sel_top_pred_acc(df_predicted_statistics_top_f,1)\\ntop_1_imp_ls[0], top_1_imp_ls[1], top_1_imp_ls[2], top_1_imp_ls[3], top_1_imp_ls[4]\\nOut [219]:\\n(‚ÄòSTAT_VALUE_top15‚Äô,\\n<hana_ml.algorithms.pal.unified_classification.UnifiedClassification at 0x25c02d19430>,\\n[‚ÄòEMPLOYEE_ID‚Äô,\\n‚ÄòHRTRAINING‚Äô,\\n‚ÄòFUNCTIONALAREACHANGETYPE‚Äô,\\n‚ÄòSICKDAYS‚Äô,\\n‚ÄòPROMOTION_WITHIN_LAST_3_YEARS‚Äô,\\n‚ÄòTIMEINPREVPOSITIONMONTH‚Äô,\\n‚ÄòEMPLOYMENT_TYPE_2‚Äô,\\n‚ÄòPREVIOUS_CAREER_PATH‚Äô,\\n‚ÄòRISK_OF_LOSS‚Äô,\\n‚ÄòCURRENT_COUNTRY‚Äô,\\n‚ÄòSALARY‚Äô,\\n‚ÄòJOBLEVELCHANGETYPE‚Äô,\\n‚ÄòAGE‚Äô,\\n‚ÄòCHANGE_IN_PERFORMANCE_RATING‚Äô,\\n‚ÄòPREVIOUS_COUNTRY‚Äô,\\n‚ÄòLINKEDIN‚Äô,\\n‚ÄòFLIGHT_RISK‚Äô],\\n17,\\n‚Äòtop15‚Äô)\\nPredictions statistics in Pandas DataFrame with results from subsequent steps.\\n# In Out[230]:\\ndf_pred_stat_last\\nOut[230]:\\nSTAT_NAME\\nCLASS_NAME\\nINIT\\nADD\\ncols_imp\\ncols_imp_mn\\nsm_max_cols_all\\nsm_max_cols_imp\\nsm_max_bl_cols_imp\\nsm_best_bl_cols_imp\\ntop15\\n0\\nAUC\\nNone\\n0.9733\\n0.9834\\n0.9857\\n0.9870\\n0.9784\\n0.9709\\n0.9422\\n0.9474\\n0.9498\\n1\\nRECALL\\nNo\\n0.9964\\n0.9800\\n0.9782\\n0.9811\\n0.9453\\n0.9195\\n0.9190\\n0.9476\\n0.9380\\n2\\nPRECISION\\nNo\\n0.9247\\n0.9488\\n0.9541\\n0.9548\\n0.9751\\n0.9726\\n0.8427\\n0.8257\\n0.8602\\n3\\nF1_SCORE\\nNo\\n0.9592\\n0.9641\\n0.9660\\n0.9678\\n0.9600\\n0.9453\\n0.8792\\n0.8824\\n0.8974\\n4\\nSUPPORT\\nNo\\n1702\\n1702\\n1702\\n1702\\n1702\\n1702\\n210\\n210\\n210\\n5\\nRECALL\\nYes\\n0.3428\\n0.5714\\n0.6190\\n0.6238\\n0.8047\\n0.7904\\n0.8285\\n0.8\\n0.8476\\n6\\nPRECISION\\nYes\\n0.9230\\n0.7792\\n0.7784\\n0.8036\\n0.6450\\n0.5478\\n0.9109\\n0.9385\\n0.9319\\n7\\nF1_SCORE\\nYes\\n0.5000\\n0.6593\\n0.6896\\n0.7024\\n0.7161\\n0.6471\\n0.8678\\n0.8637\\n0.8877\\n8\\nSUPPORT\\nYes\\n210\\n210\\n210\\n210\\n210\\n210\\n210\\n210\\n210\\n9\\nACCURACY\\nNone\\n0.9246\\n0.9351\\n0.9388\\n0.9419\\n0.9299\\n0.9053\\n0.8738\\n0.8738\\n0.8928\\n10\\nKAPPA\\nNone\\n0.4683\\n0.6244\\n0.6562\\n0.6708\\n0.6766\\n0.5945\\n0.7476\\n0.7476\\n0.7857\\n11\\nMCC\\nNone\\n0.5363\\n0.6336\\n0.6615\\n0.6774\\n0.6820\\n0.6079\\n0.7506\\n0.7559\\n0.7889\\n12. Analyzing top models\\nFirst I analyze the top 1 reason for the top 1 model prediction.\\n\\xa0\\n# In [233]:\\n# Top 1 contribution feauture to prediction\\nf_score_res(top_1_imp_ls[1], d_test_bl)[0].\\\\\\n    select(\\'EMPLOYEE_ID\\', \\'SCORE\\', \\'CONFIDENCE\\', \\'REASON_CODE\\', \\n                      (\\'json_query(\"REASON_CODE\", \\\\\\'$[0].attr\\\\\\')\\', \\'Top1\\'), \\n                      (\\'json_query(\"REASON_CODE\", \\\\\\'$[0].pct\\\\\\')\\', \\'PCT_1\\') ).head(5).collect()\\nOut[233]:\\nEMPLOYEE_ID\\nSCORE\\nCONFIDENCE\\nREASON_CODE\\nTop1\\nPCT_1\\n0\\n10033\\nNo\\n0.998697\\n[{‚Äúattr‚Äù:‚ÄùSICKDAYS‚Äù,‚Äùval‚Äù:-2.3005012586457966,‚Ä¶\\n‚ÄúSICKDAYS‚Äù\\n28.49847521037374\\n1\\n10034\\nNo\\n0.997572\\n[{‚Äúattr‚Äù:‚ÄùSICKDAYS‚Äù,‚Äùval‚Äù:-2.1461261874013997,‚Ä¶\\n‚ÄúSICKDAYS‚Äù\\n28.604720941656255\\n2\\n10036\\nYes\\n0.884731\\n[{‚Äúattr‚Äù:‚ÄùHRTRAINING‚Äù,‚Äùval‚Äù:1.2600622007181038‚Ä¶\\n‚ÄúHRTRAINING‚Äù\\n17.883981590687534\\n3\\n10056\\nYes\\n0.992384\\n[{‚Äúattr‚Äù:‚ÄùSICKDAYS‚Äù,‚Äùval‚Äù:3.2852997983870959,‚Äù‚Ä¶\\n‚ÄúSICKDAYS‚Äù\\n30.97615277828276\\n4\\n10064\\nNo\\n0.985765\\n[{‚Äúattr‚Äù:‚ÄùHRTRAINING‚Äù,‚Äùval‚Äù:-1.738132753015509‚Ä¶\\n‚ÄúHRTRAINING‚Äù\\n27.416934166112573\\nI can compare test data FLIGHT_RISK (actual) with prediction SCORE.\\n# In [236]:\\nprint(f\"Top 1 voting with features {top_1_imp_ls[4]}\") # Top 1 voting\\nf_actual_vs_score(top_1_imp_ls[1], d_test_bl).head(5).collect()\\nTop 1 model (top15 features ) prediction and top 1 reason code.\\nOut [236]:\\nEMPLOYEE_ID\\nFLIGHT_RISK\\nSCORE\\nCONFIDENCE\\nREASON_CODE\\n0\\n10033\\nNo\\nNo\\n0.998697\\n[{‚Äúattr‚Äù:‚ÄùSICKDAYS‚Äù,‚Äùval‚Äù:-2.3005012586457966,‚Ä¶\\n1\\n10034\\nNo\\nNo\\n0.997572\\n[{‚Äúattr‚Äù:‚ÄùSICKDAYS‚Äù,‚Äùval‚Äù:-2.1461261874013997,‚Ä¶\\n2\\n10036\\nYes\\nYes\\n0.884731\\n[{‚Äúattr‚Äù:‚ÄùHRTRAINING‚Äù,‚Äùval‚Äù:1.2600622007181038‚Ä¶\\n3\\n10056\\nYes\\nYes\\n0.992384\\n[{‚Äúattr‚Äù:‚ÄùSICKDAYS‚Äù,‚Äùval‚Äù:3.2852997983870959,‚Äù‚Ä¶\\n4\\n10064\\nNo\\nNo\\n0.985765\\n[{‚Äúattr‚Äù:‚ÄùHRTRAINING‚Äù,‚Äùval‚Äù:-1.738132753015509‚Ä¶\\nI can analyze further top 5 REASON_CODE with code wrapped in a local function.\\n# In [240]:\\n# Top 5 contribution feauture to prediction for rows\\ndef f_contrib_top5f_head(p_model, p_d_test, p_rows):\\n# In [241]:\\nf_contrib_top5f_head(top_1_imp_ls[1], d_test_bl, 3)\\nOut [241]:\\nEMPLOYEE_ID\\nSCORE\\nCONFIDENCE\\nREASON_CODE\\nTop\\nPCT\\n0\\n10033\\nNo\\n0.998697\\n[{‚Äúattr‚Äù:‚ÄùSICKDAYS‚Äù,‚Äùval‚Äù:-2.3005012586457966,‚Ä¶\\n‚ÄúSICKDAYS‚Äù\\n28.49847521037374\\n3\\n10033\\nNo\\n0.998697\\n[{‚Äúattr‚Äù:‚ÄùSICKDAYS‚Äù,‚Äùval‚Äù:-2.3005012586457966,‚Ä¶\\n‚ÄúHRTRAINING‚Äù\\n20.004226264298525\\n6\\n10033\\nNo\\n0.998697\\n[{‚Äúattr‚Äù:‚ÄùSICKDAYS‚Äù,‚Äùval‚Äù:-2.3005012586457966,‚Ä¶\\n‚ÄúPREVIOUS_CAREER_PATH‚Äù\\n13.468698519836725\\n9\\n10033\\nNo\\n0.998697\\n[{‚Äúattr‚Äù:‚ÄùSICKDAYS‚Äù,‚Äùval‚Äù:-2.3005012586457966,‚Ä¶\\n‚ÄúLINKEDIN‚Äù\\n6.355941761599763\\n12\\n10033\\nNo\\n0.998697\\n[{‚Äúattr‚Äù:‚ÄùSICKDAYS‚Äù,‚Äùval‚Äù:-2.3005012586457966,‚Ä¶\\n‚ÄúFUNCTIONALAREACHANGETYPE‚Äù\\n6.23681257187179\\n13. Visualizing statistics\\nPresenting model results with Visualizing Model Report as explained in the blog ‚ÄúML in SAP Data Warehouse Cloud‚Äù.\\n# In [243]:\\nfrom hana_ml.visualizers.unified_report import UnifiedReport\\nUnifiedReport(top_1_imp_ls[1]).build().display()\\n\\xa0\\nVariable Importance\\n\\xa0\\nI can analyze the contributions of features with shapely_explainer as presented in the library example.\\n# In [257]:\\nshapley_explainer = TreeModelDebriefing.shapley_explainer(feature_data=d_test_bl.head(500).select(features_shapely),\\nreason_code_data=pred_res_shapely.head(500).select(\\'REASON_CODE\\'))\\nshapley_explainer.summary_plot()\\n\\xa0\\nShapley explainer\\nConfusion matrix for validation data from model training as explained in the tutorial on GitHub.\\nIn [244]:\\nimport matplotlib.pyplot as plt\\nfrom hana_ml.visualizers.metrics import MetricsVisualizer\\nf, ax1 = plt.subplots(1,1)\\nmv1 = MetricsVisualizer(ax1, title = \\'Modedl confusion matrix\\')\\nax1 = mv1.plot_confusion_matrix(top_1_imp_ls[1].confusion_matrix_, normalize=False)\\n\\xa0\\nModel confusion matrix\\nModel confusion matrix\\n# In [245]:\\nf, ax1 = plt.subplots(1,1)\\nmv1 = MetricsVisualizer(ax1, title = \\'Model confusion matrix %\\')\\nax1 = mv1.plot_confusion_matrix(top_1_imp_ls[1].confusion_matrix_, normalize=True)\\nModel confusion matrix\\nConfusion matrix for predictions wrapped in local functions.\\n# In [259]:\\ndef f_cf_mx_values(p_model, p_d_test):\\n# In [267]:\\nsns.heatmap(cf_matrix, annot=labels, fmt=\\'\\', cmap=\\'Blues\\')\\nPrediction confusion matrix\\n\\xa0\\n14. Further steps\\nRegarding to further models exploring I could use as starting point Automated Predictive Libraries GradientBoostingBinaryClassifier, tutorial Fraud Detection Auto ML Script, blog Hands-On Tutorial: Automated Predictive (APL) in SAP HANA Cloud and blog SAP HANA Cloud Machine Learning Challenge. I would increase the test data percentage from split and use ensemble techniques for voting.\\nThinking in terms of transfer learning as in computer vision, what if we can collect data across different companies and industries, build specialized or generalized models, shake hands with developers, and integrate prediction in applications to use internally or externally as on demand service. Data required for monthly evaluation could be collected from internal sources and external sources with coded extractors or scraping tools.\\nSince I am an end-to-end developer of Business Intelligence reports, I am looking forward to getting hands-on experience regarding how to integrate extraction of data from an SAP environment or other external sources to build models, consume machine learning models, and integrate prediction into the business apps.\\n\\xa0\\n\\xa0\\n\\xa0\\nFollow Like \\nRSS Feed\\n            \\n \\nAlert Moderator\\n    \\nAlerting is not available for unauthorized users\\nAssigned TagsSimilar Blog PostsRelated Questions \\n/\\n      4 Comments      \\n                You must be Logged on to comment or reply to a post.\\n        \\n \\nFarooq Azam  \\nJanuary 9, 2023 at 5:16 pm\\nHi Sergiu,\\n\\xa0\\nGreat work and write up. Very detailed and informative!\\nThanks for sharing.\\n\\xa0\\nRegards,\\nFarooq\\nLike 2ShareRight click and copy the link to share this comment\\n \\nSergiu Iatco Blog Post Author \\nJanuary 26, 2023 at 5:18 pm\\nThanks a lot, Farooq!\\nLike 0ShareRight click and copy the link to share this comment\\n \\nMynyna Chau  \\nJanuary 11, 2023 at 8:59 am\\nCongrats on the 2nd place, Sergiu Iatco\\xa0!\\nLike 1ShareRight click and copy the link to share this comment\\n \\nSergiu Iatco Blog Post Author \\nJanuary 26, 2023 at 5:20 pm\\nThanks a lot, Mynyna!\\nLike 0ShareRight click and copy the link to share this comment\\n \\n \\nFind us on\\nPrivacy\\nTerms of Use\\nLegal Disclosure\\nCopyright\\nTrademark\\nNewsletter\\nSupport\\n \\n \\n Insert/edit link\\nClose\\nEnter the destination URL\\nURL\\nLink Text\\n Open link in a new tab\\nOr link to existing content\\nSearch\\nNo search term specified. Showing recent items.\\n', doc_id='3ff90ae2-9454-42dd-bd96-c8a7f3dd0143', embedding=None, doc_hash='52b898d1906f1b5211f0843e8ab6fa3d1f17c5c12d94a5fb0f233a36ea15e0fe', extra_info=None),\n",
       "  Document(text=\"\\nSAP Community Call | SAP HANA Cloud Machine Learning Challenge: I quit!‚Äù ‚Äì How to prevent employee churn! | SAP Blogs\\n \\nSkip to Content\\nSAP Community Log-in UpdateIn a few months, SAP Community will switch to SAP Universal ID as the only option to login. Don‚Äôt wait, create your SAP Universal ID now! If you have multiple accounts, use the Consolidation Tool to merge your content.Get started with SAP Universal ID\\nHome\\nCommunity\\nAsk a Question\\nWrite a Blog Post\\nLogin / Sign-up\\n \\nEvent Information\\n \\nSusen Poppe\\n \\nNovember 7, 2022\\n2 minute read\\nSAP Community Call | SAP HANA Cloud Machine Learning Challenge: I quit!‚Äù ‚Äì How to prevent employee churn!\\n8        \\n24        \\n800        \\nDear SAP Community,\\xa0\\nWe are excited to invite you to the kick-off call for our next SAP Community challenge: ‚ÄúI quit!‚Äù ‚Äì How to prevent employee churn!\\xa0After having discovered the graph world last time, we are more than excited to dive deeper into the world of machine learning in SAP HANA Cloud this time.\\xa0\\nWhat: ‚ÄúI quit!‚Äù ‚Äì How to prevent employee churn | SAP HANA Cloud Machine Learning Challenge\\xa0\\nWhen: November 28, 11am EST / 5pm CET\\xa0\\nDuration: 1 hour (presentation + Community challenge kick-off + Q&A)\\xa0\\nSAP HANA Cloud offers a comprehensive, natively embedded set of machine learning capabilities and interfaces, allowing you to develop smart applications and to analyze sensitive data while ensuring privacy. Within SAP HANA Cloud, SAP HANA database you get two machine learning libraries: The Predictive Analysis Library (PAL) and the Automated Predictive Library (APL). Targeting data scientists, that are leveraging SAP HANA Cloud‚Äôs native machine learning capabilities, there are Python and R machine learning client packages available. This truly simplifies moving from an open-source scenario to embedded machine learning in SAP HANA Cloud.\\xa0\\nIn our SAP community call, we will provide you with an overview of the embedded machine learning capabilities in SAP HANA Cloud. This includes the Predictive Analysis Library (PAL) for the machine learning expert and a more automated approach through the Automated Predictive Library (APL). You will for example learn how you can stay in your favorite Python environment and create machine learning models without extracting data into your local environment. Finally, we are excited to kick-off our SAP HANA Cloud Machine Learning challenge, that will be hosted in cooperation with our SAP Machine Learning Experts.\\xa0\\xa0\\nDon‚Äôt worry, in case you cannot attend the call, we will also publish a dedicated blogpost describing the challenge. Stay tuned for a blogpost on the prerequisites that will be published in the next couple of days.\\xa0\\xa0\\nSo, if you are interested in SAP HANA Cloud‚Äôs natively embedded Machine Learning capabilities and want to get hands-on experience with them, this call will be a great fit for you. Join our experts Christoph Morgen, Yannick Schaper, Sarah Detzler, and Andreas Forster to dive deep into the topic of machine learning in SAP HANA Cloud.\\xa0\\nRegister now!\\nIn case you cannot attend the call, don‚Äôt worry, we are live streaming on YouTube and the recording will be available right afterwards for you to re-watch and share with others that might also be interested in the contents presented. We will also include the link to the recording on the SAP HANA on-premises and cloud databases Community Page afterwards, as well as a dedicated blog post explaining the SAP HANA Cloud Machine Learning Challenge in detail.\\xa0\\xa0\\nDo you already have questions? Feel free to post them in the comment section below this blogpost. Also, feel free to tag your colleagues that should join this call together with you.\\xa0\\nDo you want to learn more about machine learning in SAP HANA Cloud in advance of the call? Then check out these resources:\\xa0\\nWebinar: Accelerate your Machine Learning efforts ‚Äì benefit from SAP HANA Cloud AutoML\\xa0\\nBlogpost: Hands-On Tutorial: Leverage SAP HANA Machine Learning in the Cloud through the Predictive Analysis Library\\xa0\\nBlogpost: Hands-On Tutorial: Automated Predictive (APL) in SAP HANA Cloud\\xa0\\xa0\\nBlogpost: Data Yoga-It is all about finding the right balance\\xa0\\nVideo: Build your Machine Learning Scenario for your SAP HANA Cloud application from Python\\xa0\\nVideo: Intelligence out of the box | Native Machine Learning in SAP HANA Cloud\\nFind a list of all scheduled SAP Community Calls here\\xa0\\nDo you have questions regarding another topic? Ask the SAP Community.\\xa0\\xa0\\nWe are looking forward to seeing you in the call,\\xa0\\nSusen on behalf the SAP HANA Cloud Machine Learning Challenge team\\xa0\\nFollow Like \\nRSS Feed\\n            \\n \\nAlert Moderator\\n    \\nAlerting is not available for unauthorized users\\nAssigned TagsSimilar Blog PostsRelated Questions \\n/\\n      8 Comments      \\n                You must be Logged on to comment or reply to a post.\\n        \\n \\nSergiu Iatco \\nNovember 14, 2022 at 8:15 am\\nHi Susen. I am a full stack SAP BW developer and I learned and understood what machine learning is by experimenting on Kaggle. Reading is not sufficient you have to work with data and code. Kaggle provides the playground with knowledge projects, competitions and shared working codes. How can I get such an environment on SAP HANA cloud? Regards. Sergey.\\nLike 0ShareRight click and copy the link to share this comment\\n \\nSusen Poppe Blog Post Author  \\nNovember 14, 2022 at 9:33 am\\nHi Sergiu,\\nThanks for reaching out! You should definitely stay tuned for our challenge where you will have the opportunity to get hands-on-experience with machine learning in SAP HANA Cloud. Register for the kick-off call. More information will follow soon.\\nAll the best,\\nSusen\\nLike 1ShareRight click and copy the link to share this comment\\n \\nSergiu Iatco \\nNovember 16, 2022 at 7:57 am\\nHi Susen. Please recommend some books for machine learning in SAP HANA Cloud.\\xa0Regards. Sergiu.\\nLike 0ShareRight click and copy the link to share this comment\\n \\nChristoph Morgen  \\nNovember 16, 2022 at 8:09 am\\nHi Sergiu,\\nthe is a HANA Machine Learning blog collection SAP HANA Machine Learning Resources | SAP Blogs, which crosses a great variety of different topics and use cases applying ML in context of SAP HANA!\\nBest regards,\\nChristoph\\nLike 3ShareRight click and copy the link to share this comment\\n \\nSusen Poppe Blog Post Author  \\nNovember 16, 2022 at 8:14 am\\nIn case you are german speaking, I can recommend this recently published book: https://www.rheinwerk-verlag.de/data-science-mit-sap-hana/\\nLike 0ShareRight click and copy the link to share this comment\\n \\nSergiu Iatco \\nNovember 16, 2022 at 10:33 am\\nI speak English. I found in blogs official documentation for the components.\\nSAP HANA Python Client API for Machine Learning Algorithms\\nSAP HANA Predictive Analysis Library (PAL)\\nDocumentation comes along with statistics used in processing. Statistics for developers is to understand how it works and insert some enhancements, but the focus should be on end-to-end project implementations. If a problem can be solved with an AutoML probably there is a chance for fine tuning by exploring other libraries. To improve something in the first place, you need something that works.\\nLike 0ShareRight click and copy the link to share this comment\\n \\nSusen Poppe Blog Post Author  \\nNovember 17, 2022 at 8:01 am\\nHi Sergiu, If you are looking for more insights into AutoML, you should join our SAP Community Call on SAP HANA Cloud's Auto ML capabilities on November 23: https://groups.community.sap.com/t5/sap-community-calls/accelerate-your-machine-learning-efforts-benefit-from-sap-hana/ec-p/124250#M84\\nLike 0ShareRight click and copy the link to share this comment\\n \\nChristoph Morgen  \\nNovember 21, 2022 at 4:03 pm\\nA BW specific end-2-end implementation blog would be SAP HANA Machine Learning with ABAP Managed Database Procedures in SAP BW/4HANA | SAP Blogs\\nLike 0ShareRight click and copy the link to share this comment\\n \\n \\nFind us on\\nPrivacy\\nTerms of Use\\nLegal Disclosure\\nCopyright\\nTrademark\\nNewsletter\\nSupport\\n \\n \\n Insert/edit link\\nClose\\nEnter the destination URL\\nURL\\nLink Text\\n Open link in a new tab\\nOr link to existing content\\nSearch\\nNo search term specified. Showing recent items.\\n\", doc_id='cf500805-4377-45cf-9eae-c24adcfa737d', embedding=None, doc_hash='0129c0ec851982d7ca53bdcd55d2b8004219169a532783da0b71f85959049ed6', extra_info=None),\n",
       "  Document(text='\\n‚ÄúI quit!‚Äù ‚Äì How to predict employee churn | SAP HANA Cloud Machine Learning Challenge [Submission closed] | SAP Blogs\\n \\nSkip to Content\\nSAP Community Log-in UpdateIn a few months, SAP Community will switch to SAP Universal ID as the only option to login. Don‚Äôt wait, create your SAP Universal ID now! If you have multiple accounts, use the Consolidation Tool to merge your content.Get started with SAP Universal ID\\nHome\\nCommunity\\nAsk a Question\\nWrite a Blog Post\\nLogin / Sign-up\\n \\nEvent Information\\n \\nSusen Poppe\\n \\nNovember 28, 2022\\n6 minute read\\n‚ÄúI quit!‚Äù ‚Äì How to predict employee churn | SAP HANA Cloud Machine Learning Challenge [Submission closed]\\n4        \\n9        \\n1,146        \\nCompanies want to prevent their highly skilled employees from leaving the company. Today, many of them rely on their managers‚Äô intuition to foresee employee churn. However, this method is neither precise nor unbiased.\\nThink about how some managers say that their best guess for a high churn probability is an employee‚Äôs new haircut. A new haircut might indicate that a person is ready for more important changes. However, could we really deduce this from one factor?\\nBy looking at concrete data and analyzing complex patterns, we can better make effective business decisions. There are people who would argue that human behavior is too complex to be reflected in datasets. However, we don‚Äôt favor one approach over the other. Instead, we are suggesting using both subjective information (such as the opinion of a manager) and the analytical and mathematical conclusions from the data to discover new insights.\\nConsider this when analyzing employee churn:\\nA high churn rate among new hires could indicate that the onboarding process needs to be improved and thus leads to a reflection of the company culture and a change in the treatment of new hires.\\nWe could also take a closer look at internal employee turnover and their success rate or make better learning recommendations based on data.\\nFurther, data could be used to help reflect on whether we are leveraging the full potential of our most experienced colleagues.\\nUnderstand our challenge\\nWatch the recording of our kick-off call\\nThe purpose of the challenge is to give participants some hands-on experience on how the Machine Learning embedded in SAP HANA Cloud and SAP Data Warehouse Cloud can be utilized. IT experts can experiment with Machine Learning with the support of a global team of experts, ready to support in different time zones and seasoned Data Scientists can participate to become familiar with SAP‚Äôs components for Machine Learning on structured data.\\nTo win the challenge, you will need to come up with a clever implementation of the use case with SAP HANA Machine Learning\\u200b and an appealing presentation delivering a clear message that matches your persona and use case. We are NOT just looking for the highest accuracy, precision or recall but also a catchy presentation, or a solid story.\\u200b\\nJoin us on\\nDecember 16, 2022, 8AM CET (4PM KST) | Join here\\nDecember 16, 2022, 4PM CET (10AM EST) | Join here\\nto present your solution to our SAP expert team.\\nIf you cannot attend the calls, please publish an SAP Community blogpost with your solution until December 16, 2022 11:59pm CET.\\nSupport\\nDuring the challenge, we are happy to offer ‚Äúopen office hours‚Äù where you can join our experts to get your open questions answered and address any issues you are dealing with regarding the challenge. Please find the slots offered below:\\nNovember 29, 2022, 9AM CET (5PM KST) | Join here\\nDecember 1, 2022, 5PM CET (11AM EST) | Join here\\nDecember 6, 2022, 9AM CET (5PM KST) | Join here\\nDecember 8, 2022, 5PM CET (11AM EST) | Join\\xa0 here\\nDecember 13, 2022, 9AM CET (5PM KST) | Join here\\nDecember 15, 2022, 5PM CET (11AM EST) | Join here\\nWe are also happy to offer you the option to discuss open questions in this discussion thread.*\\nDo you accept the challenge? Let us know by commenting ‚ÄúI am no quitter‚Äù and tag your fellow machine learning enthusiasts that should also participate. Follow this blogpost to not miss any updates.\\nPrerequisites\\nTo be able to participate in the challenge there are some prerequisites you need to fulfil.\\nYou will need an SAP HANA Cloud instance. Free tier or trial are not compatible with the challenge contents. If you don‚Äôt have an adequate set-up, please\\xa0reach out to the team\\xa0to get access to a suitable instance.\\nYou need to have a local Python environment (You need to use version 3.4 ‚Äì 3.8)\\nInstall\\xa0Python Machine Learning client for HANA\\nPlease take a look at the technical prerequisites at\\xa0https://pypi.org/project/hana-ml/\\xa0or in the\\xa0documentation\\nPlease test the connection from your python environment to SAP HANA Cloud\\nRead this\\xa0blogpost\\xa0for an example APL hands-on blogpost (how to set up your local python environment)\\nAccess the challenge data set via this\\xa0Jupyter Notebook\\nAn extensive list can be found in our Prerequisites blogpost.\\nWhat‚Äôs in for you\\nApart from learning more about and getting hands on experience with the machine learning capabilities embedded in SAP HANA Cloud, we will give chosen winners the opportunity to present their solution in an SAP Community call together with our SAP HANA machine learning experts!\\nMoreover, the chosen winner(s) will be awarded a gift card.\\nWinners will be contacted after the challenge to clarify the details of the SAP Community call.\\n1st price: Gift card + Invitation to present in an SAP Community Call together with our SAP machine learning experts\\n2nd & 3rd: Invitation to present in an SAP Community Call together with our SAP machine learning experts\\nOverall Timeline\\nNovember 28, 2022\\nOfficial Challenge Kick-off | Watch the recording here\\nNovember 29, 2022 (9AM CET)\\nOpen Office Hour Challenge (Asia Pacific & European time zone)\\nDecember 1, 2022 (5PM CET)\\nOpen Office Hour Challenge (European & American time zone)\\nDecember 6, 2022 (9AM CET)\\nOpen Office Hour Challenge (Asia Pacific & European time zone)\\nDecember 8, 2022 (5PM CET)\\nOpen Office Hour Challenge (European & American time zone)\\nDecember 13, 2022 (9AM CET)\\nOpen Office Hour Challenge (Asia Pacific & European time zone)\\nDecember 15, 2022 (5PM CET)\\nOpen Office Hour Challenge (European & American time zone)\\nDecember 16, 2022\\nEnd of the challenge + Presentation slots for your solution\\n¬∑\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 Slot 1 (8AM CET):\\xa0Asia Pacific & European time zone\\xa0\\n¬∑\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 Slot 2 (4PM CET):\\xa0European and American time zone\\nCW51, 2022\\nAnnouncement of the winners\\nCW4, 2023\\nCommunity Call with the winners\\nKeep exploring\\nBlogpost: Hands-On Tutorial: Leverage SAP HANA Machine Learning in the Cloud through the Predictive Analysis Library\\nBlogpost: Hands-On Tutorial: Automated Predictive (APL) in SAP HANA Cloud \\nBlogpost: Data Yoga-It is all about finding the right balance\\nVideo: Build your Machine Learning Scenario for your SAP HANA Cloud application from Python\\nVideo: Intelligence out of the Box ‚Äì Native Machine Learning in SAP HANA Cloud | SAP Community Call\\n\\xa0\\nAre you ready? Set? GOOOO!\\nThe SAP HANA Cloud Machine Learning Challenge team is wishing you good luck!\\n\\xa0\\nTerms & Conditions\\nChallenge definition\\nThe SAP HANA Machine learning challenge is an online challenge, taking place in SAP Community\\xa0(community.sap.com) where the announcement of the challenge as well as the winner selection is being published as a blogpost in SAP Community. Valid submissions need to be presented during the presentation slots at the end of the challenge or written down in a blogpost in the SAP Community as described in this announcement where every interested individual SAP Community member can participate in handing in their solution.\\xa0Participation in the challenge is subject to these Terms and Conditions.\\nThe Promoter of this challenge is the SAP HANA product management team.\\nPrize draw entry period details\\xa0\\nEligible individuals may participate in the challenge from November 28 to December 16, 2022 at\\xa011:59 PM\\xa0Central European\\xa0Time.\\nEligibility Criteria\\xa0\\nEntrants must (i) be eighteen (18) years or older; (ii) be a registered\\xa0and active\\xa0member of the SAP Community\\xa0(community.sap.com)\\xa0(iii) you are prohibited from participating in this Challenge if you are located in a country embargoed by the United States or if you are on the U.S. treasury department‚Äôs list of specially designated nationals or if you are an government official.\\xa0\\xa0(iv)\\xa0SAP reserves the right to verify eligibility and to adjudicate on any dispute at any time. The Challenge\\xa0is subject to all federal, state, and local laws and is void where prohibited.\\xa0Participants in the Challenge\\xa0agree to be bound by these Official Rules and the decisions of the Sponsor.\\nContest Rules & submission requirements\\xa0\\nChallenge solutions must:\\nbe submitted by an individual authentic account\\nbe presented during one of the presentation slots on December 16 or must be described in a blogpost before December 16, 2022 11:59pm CET.\\n\\xa0\\nSelection and Notification of Winner \\nOne can win this challenge by presenting a correct solution in one of the calls on December 16 or writing a blogpost about a correct solution until December 16, 2022 11:59pm. \\xa0Winners will be featured and linked in future blogposts.\\nWinners will be chosen by the SAP HANA Cloud Machine Learning Challenge team.\\nConsent and Release \\xa0\\nPromoter is not responsible for any typographical or other error in the printing of the offer, administration of the Prize Draw or in the announcement of prize or for any suspension of the Prize Draw or inability to implement the Prize Draw or award the prize as contemplated herein due to any event beyond its control, including delays or interruptions caused by acts of God, acts of war, natural disasters, weather, utility outages, acts of terrorism or any national, federal, territorial or local government law, order, or regulation, order of any court or regulator. This Prize Draw shall be governed by and interpreted under the laws of\\xa0Germany\\xa0without regard to its conflicts of\\xa0laws\\xa0provisions.\\xa0Any and all\\xa0disputes, claims, and causes of action arising out of or in connection with this Prize Draw, shall be resolved individually, without resort to any form of class action.\\xa0Any claims, judgments and/or awards shall be limited to actual out-of-pocket costs associated with entering this prize draw. Participant hereby waives any rights or claims to legal fees, indirect, special, punitive, incidental or consequential damages of participant, whether foreseeable or not and whether based on negligence or otherwise.\\nPersonal information and privacy\\xa0\\nBy participating in the\\xa0Challenge, Participant consents to the collection, use and disclosure of Participant‚Äôs personal information as described at\\xa0https://www.sap.com/corporate/en/legal/terms-of-use.html?view=PP.\\n\\xa0\\n\\xa0\\n* Please note that you will need to separately register for SAP Groups.\\n\\xa0\\n\\xa0\\n\\xa0\\nFollow Like \\nRSS Feed\\n            \\n \\nAlert Moderator\\n    \\nAlerting is not available for unauthorized users\\nAssigned TagsSimilar Blog PostsRelated Questions \\n/\\n      4 Comments      \\n                You must be Logged on to comment or reply to a post.\\n        \\n \\nChristoph Morgen  \\nNovember 29, 2022 at 1:46 pm\\nHave you been able to connect to SAP HANA CLoud and upload the employee-churn data set yet?\\nIf you got stuck, here is an example Jupyter Notebook in our ML community challenge github example repo\\xa0\\nJoin our open office calls (see above for times & days) for questions or post your questions in this Q&A thread.\\nHave fun working on the challenge!\\nChristoph\\nLike 3ShareRight click and copy the link to share this comment\\n \\nChristoph Morgen  \\nDecember 12, 2022 at 2:14 pm\\nLast week of our SAP HANA Cloud Machine Learning challenge hast started!\\nTune and finalize your models, looking forward for all your results presentations this Friday 16th Dec.\\nPresentation slots for your solution\\n¬∑\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 Slot 1 (8AM CET):\\xa0Asia Pacific & European time zone\\xa0\\n¬∑\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 Slot 2 (4PM CET):\\xa0European and American time zone\\nSee you all this Friday!\\nChristoph\\nLike 0ShareRight click and copy the link to share this comment\\n \\nDirk Kemper \\nDecember 14, 2022 at 9:19 am\\nHi Christoph,\\nunfortunately I am not able to join the call next Friday on the provided time slots. I have written the following blog post about my analysis: https://blogs.sap.com/2022/12/14/sap-hana-cloud-machine-learning-challenge-preventing-employee-churn/\\nLooking forward to your comments.\\nRegards, Dirk\\nLike 0ShareRight click and copy the link to share this comment\\n \\nSusen Poppe Blog Post Author  \\nDecember 16, 2022 at 6:34 am\\nHi Dirk,\\nWe will definitely consider your blogpost. Thanks for already publishing it.\\nAll the best,\\nSusen on behalf of the team\\nLike 0ShareRight click and copy the link to share this comment\\n \\n \\nFind us on\\nPrivacy\\nTerms of Use\\nLegal Disclosure\\nCopyright\\nTrademark\\nNewsletter\\nSupport\\n \\n \\n Insert/edit link\\nClose\\nEnter the destination URL\\nURL\\nLink Text\\n Open link in a new tab\\nOr link to existing content\\nSearch\\nNo search term specified. Showing recent items.\\n', doc_id='66cb0730-ba21-462f-a31d-91ea06e0b648', embedding=None, doc_hash='46f162f49fd48042c10410192b6ed08671bbad65f66fd1687b6314795751ca95', extra_info=None),\n",
       "  Document(text='\\nSAP HANA Cloud Machine Learning Challenge 2022 | The winners are‚Ä¶ | SAP Blogs\\n \\nSkip to Content\\nSAP Community Log-in UpdateIn a few months, SAP Community will switch to SAP Universal ID as the only option to login. Don‚Äôt wait, create your SAP Universal ID now! If you have multiple accounts, use the Consolidation Tool to merge your content.Get started with SAP Universal ID\\nHome\\nCommunity\\nAsk a Question\\nWrite a Blog Post\\nLogin / Sign-up\\n \\nEvent Information\\n \\nSusen Poppe\\n \\nDecember 22, 2022\\n2 minute read\\nSAP HANA Cloud Machine Learning Challenge 2022 | The winners are‚Ä¶\\n1        \\n7        \\n571        \\nDear SAP Community,\\nAs you may already know, we‚Äôve launched\\xa0the very first SAP HANA Cloud Machine Learning Challenge\\xa0in November 2022.\\nOn behalf of the entire SAP HANA Cloud Machine Learning Challenge Team and the SAP HANA Product Management Team, congratulations to all the participants who accepted the challenge and took the time to submit their solution! It is great to see the different approaches and how all our participants tackled the challenge in diverse ways.\\nNow, we are excited to announce the winners of the SAP HANA Cloud Machine Learning Challenge in 2022:\\nClemens W√∂hrer\\nSergiu Iatco\\xa0 & Vishwas Madhuvarshi\\nOnce again, a big thank you for all your contributions! Keep up your good work, excitement, and enthusiasm for SAP HANA Cloud‚Äôs Machine Learning capabilities!\\nYou are not one of the lucky winners? How about sharing your experience with the SAP Community via a blogpost? Start blogging right away. Looking for some inspiration? Dirk Kemper‚Äôs blog is a good starting point. Read it now.\\nPrizes\\nApart from learning more about the Machine Learning capabilities in SAP HANA Cloud, the chosen first place winner will be contacted via email to receive their price.\\nWe will also reach out to all winners to clarify the details of the opportunity to present their solutions in an SAP Community call together with our SAP HANA Cloud Machine Learning experts!\\nWhat‚Äôs next?\\nStay tuned for upcoming events, including the SAP Community Call with the winners of this challenge, by\\xa0following our product tag in the SAP Community.\\nDid you stumble about this blogpost without having participated in the challenge? Don‚Äôt worry, you can still accept the challenge and find your way to prevent employee churn. Read this blogpost or watch this video for more details. The challenge data set can be found in this GitHub repository.\\nPlease note that neither the free tier nor the free trial for SAP HANA Cloud are suitable to use SAP HANA Cloud‚Äôs Machine Learning capabilities!\\nFor more information about SAP HANA Cloud:\\nBlogpost:\\xa0Hands-On Tutorial: Leverage SAP HANA Machine Learning in the Cloud through the Predictive Analysis Library\\nBlogpost:\\xa0Hands-On Tutorial: Automated Predictive (APL) in SAP HANA Cloud\\nBlogpost:\\xa0Data Yoga-It is all about finding the right balance\\nVideo:\\xa0Build your Machine Learning Scenario for your SAP HANA Cloud application from Python\\nVideo:\\xa0Intelligence out of the Box ‚Äì Native Machine Learning in SAP HANA Cloud | SAP Community Call\\nFind a list of all scheduled SAP Community Calls\\xa0here\\nFollow us on\\xa0 YouTube\\n\\xa0\\nDo you have questions regarding another topic?\\xa0Ask the SAP Community.\\n\\xa0\\nAll the best, happy holidays, and a great start into the new year,\\nYour SAP HANA Cloud Machine Learning Challenge team\\nFollow Like \\nRSS Feed\\n            \\n \\nAlert Moderator\\n    \\nAlerting is not available for unauthorized users\\nAssigned TagsSimilar Blog PostsRelated Questions \\n/\\n      1 Comment      \\n                You must be Logged on to comment or reply to a post.\\n        \\n \\nWitalij Rudnicki  \\nJanuary 1, 2023 at 4:33 pm\\nClemens W√∂hrer, Sergiu Iatco, Vishwas Madhuvarshi, congratulations!\\nLike 1ShareRight click and copy the link to share this comment\\n \\n \\nFind us on\\nPrivacy\\nTerms of Use\\nLegal Disclosure\\nCopyright\\nTrademark\\nNewsletter\\nSupport\\n \\n \\n Insert/edit link\\nClose\\nEnter the destination URL\\nURL\\nLink Text\\n Open link in a new tab\\nOr link to existing content\\nSearch\\nNo search term specified. Showing recent items.\\n', doc_id='a147b81a-c4d3-4d1b-9273-145f4bc09b99', embedding=None, doc_hash='b31544c6f576d9866d3374823c0c0724e6f11c225bba4944b99dc9fc9ce2ca21', extra_info=None),\n",
       "  Document(text='\\nhana_ml.dataframe ‚Äî hana_ml 1.0.8.post5 documentation\\nNavigation\\nindex\\nmodules |\\nnext |\\nprevious |\\nhana_ml 1.0.8.post5 documentation ¬ª\\nhana_ml.dataframe¬∂\\nThis module represents a database query as a dataframe.\\nMost operations are designed to not bring data back from the database\\nunless explicitly requested.\\nThe following classes and functions are available:\\nConnectionContext\\nDataFrame\\nquotename()\\ncreate_dataframe_from_pandas()\\nhana_ml.dataframe.quotename(name)¬∂\\nEscapes a schema, table, or column name for use in SQL. hana_ml functions and methods that take schema, table, or column names\\nalready escape their input by default, but those that take SQL don‚Äôt (and can‚Äôt) perform escaping automatically.\\nParameters\\nnamestrThe schema, table, or column name.\\nReturns\\nstrThe escaped name. The string is surrounded in quotation marks,\\nand existing quotation marks are escaped by doubling them.\\nclass hana_ml.dataframe.ConnectionContext(address=\\'\\', port=0, user=\\'\\', password=\\'\\', autocommit=True, packetsize=None, userkey=None, **properties)¬∂\\nBases: object\\nRepresents a connection to an SAP HANA system.\\nConnectionContext includes methods for creating DataFrames from data\\non SAP HANA. DataFrames are tied to a ConnectionContext, and are unusable\\nonce their ConnectionContext is closed.\\nParameters\\nSame as hdbcli.dbapi.connect. Please see the following online docs for hdbcli.dbapi.connect:\\nhttps://help.sap.com/viewer/0eec0d68141541d1b07893a39944924e/latest/en-US/ee592e89dcce4480a99571a4ae7a702f.html\\nExamples\\nQuerying data from SAP HANA into a Pandas DataFrame:\\n>>> with ConnectionContext(\\'address\\', port, \\'user\\', \\'password\\') as cc:\\n...     df = (cc.table(\\'MY_TABLE\\', schema=\\'MY_SCHEMA\\')\\n...             .filter(\\'COL3 > 5\\')\\n...             .select(\\'COL1\\', \\'COL2\\'))\\n...     pandas_df = df.collect()\\nThe underlying hdbcli.dbapi.connect can be accessed if necessary:\\n>>> with ConnectionContext(\\'127.0.0.1\\', 30215, \\'MLGUY\\', \\'manager\\') as cc:\\n...     cc.connection.setclientinfo(\\'SOMEKEY\\', \\'somevalue\\')\\n...     df = cc.sql(\\'some sql that needs that session variable\\')\\n...     ...\\nAttributes\\nconnectionhdbcli.dbapi.connectThe underlying dbapi connection. Use this connection to run SQL directly,\\nor to access connection methods like getclientinfo/setclientinfo.\\nclose(self)¬∂\\nCloses the existing connection.\\nParameters\\nNone\\nReturns\\nNone\\nhana_version(self)¬∂\\nReturns the version of SAP HANA.\\nParameters\\nNone\\nReturns\\nstrThe SAP HANA version.\\nget_current_schema(self)¬∂\\nReturns the current schema name.\\nReturns\\nstrThe current schema name.\\nhana_major_version(self)¬∂\\nReturns the major number of HANA version.\\nParameters\\nNone\\nReturns\\nstrThe major number of HANA version.\\nsql(self, sql)¬∂\\nReturns a DataFrame representing a query.\\nParameters\\nsqlstrSQL query.\\nReturns\\nDataFrameThe DataFrame for the query.\\nExamples\\n>>> df = cc.sql(\\'SELECT T.A, T2.B FROM T, T2 WHERE T.C=T2.C\\')\\ntable(table, *, schema=None)¬∂\\nReturns a DataFrame that represents the specified table.\\nParameters\\ntablestrThe table name.\\nschemastr, optional, keyword-onlyThe schema name. If this value is not provided or set to None, then the value defaults to the\\nConnectionContext‚Äôs current schema.\\nReturns\\nDataFrameThe DataFrame that is selecting data from the specified table.\\nExamples\\n>>> df1 = cc.table(\\'MY_TABLE\\')\\n>>> df2 = cc.table(\\'MY_OTHER_TABLE\\', schema=\\'MY_SCHEMA\\')\\nclass hana_ml.dataframe.DataFrame(connection_context, select_statement, _name=None)¬∂\\nBases: object\\nRepresents a frame that is backed by a database SQL statement.\\nParameters\\nconnection_contextConnectionContextThe connection to the SAP HANA system.\\nselect_statementstrThe SQL query backing the dataframe.\\nNote\\nParameters beyond connection_context and select_statement are intended for internal use. Do not rely on them; they may change without notice.\\ncolumns¬∂\\nLists the current DataFrame‚Äôs column names. Computed lazily and cached.\\nEach access to this property creates a new copy; mutating the list does not alter or corrupt the DataFrame.\\nname¬∂\\nReturns the name of the DataFrame. This value does not correspond to an SAP HANA table name.\\nThis value is useful for joining predicates when the joining DataFrames have columns with the same name.\\nquoted_name¬∂\\nSpecifies the escaped name of the original DataFrame.\\nDefault-generated DataFrame names are safe to use in SQL without escaping, but names set with DataFrame.alias may require escaping.\\ndeclare_lttab_usage(self, usage)¬∂\\nDeclares whether this DataFrame makes use of local temporary tables.\\nSome PAL execution routines can execute more efficiently if they know up front whether a DataFrame‚Äôs SELECT statement requires\\naccess to local temporary tables.\\nParameters\\nusagebool, optionalSpecifies whether this DataFrame uses local temporary tables.\\nadd_id(self, id_col)¬∂\\nReturn a new DataFrame with <id_col>.\\nParameters\\nidstrThe name of ID column.\\nReturns\\nDataFrameThe DataFrame with <id_col>.\\nalias(self, alias)¬∂\\nReturns a new DataFrame with an alias set.\\nParameters\\naliasstrThe name of the DataFrame.\\nReturns\\nDataFrameThe DataFrame with an alias set.\\nSee also\\nDataFrame.rename_columnsFor renaming individual columns.\\ncount(self)¬∂\\nComputes the number of rows in the DataFrame.\\nParameters\\nNone\\nReturns\\nintThe number of rows in the DataFrame.\\ndiff(self, index, periods=1)¬∂\\nReturns a new DataFrame with differenced values.\\nParameters\\nindexint or strIndex of the HANA DataFrame.\\nperiodsint, default 1Periods to shift for calculating difference, accepts negative values.\\nReturns\\nDataFrameDataFrame with differenced values. No calculation happens if it contains string.\\ndrop(self, cols)¬∂\\nReturns a new DataFrame without the specified columns.\\nParameters\\ncolslist of strThe list of column names to drop.\\nReturns\\nDataFrameThe new DataFrame that retains only the columns not listed in cols.\\nExamples\\n>>> df.collect()\\n   A  B\\n0  1  3\\n1  2  4\\n>>> df.drop([\\'B\\']).collect()\\n   A\\n0  1\\n1  2\\ndistinct(self, cols=None)¬∂\\nReturns a new DataFrame with distinct values for the specified columns.\\nIf no columns are specified, then the distinct row values from all columns are returned.\\nParameters\\ncolsstr or list of str, optionalA column or list of columns to consider when getting distinct\\nvalues. Defaults to use all columns.\\nReturns\\nDataFrameThe DataFrame with distinct values for cols.\\nExamples\\nInput:\\n>>> df.collect()\\n   A  B    C\\n0  1  A  100\\n1  1  A  101\\n2  1  A  102\\n3  1  B  100\\n4  1  B  101\\n5  1  B  102\\n6  1  B  103\\n7  2  A  100\\n8  2  A  100\\nDistinct values in a column:\\n>>> df.distinct(\"B\").collect()\\n   B\\n0  A\\n1  B\\nDistinct values of a subset of columns:\\n>>> df.distinct([\"A\", \"B\"]).collect()\\n   A  B\\n0  1  B\\n1  2  A\\n2  1  A\\nDistinct values of the entire data set:\\n>>> df.distinct().collect()\\n   A  B    C\\n0  1  A  102\\n1  1  B  103\\n2  1  A  101\\n3  2  A  100\\n4  1  B  101\\n5  1  A  100\\n6  1  B  100\\n7  1  B  102\\ndrop_duplicates(self, subset=None)¬∂\\nReturns a new DataFrame with duplicate rows removed. All columns in the\\nDataFrame are returned. There is no way to keep specific duplicate rows.\\nWarning\\nSpecifying a non-None value of subset may produce an unstable            DataFrame, the contents of which may be different every time you            look at it. Specifically, if two rows are duplicates in their            subset columns and have different values in other columns,            Then a different row could be picked every time you look at the result.\\nParameters\\nsubsetlist of str, optionalA list of columns to consider when deciding whether rows are             duplicates of each other. Defaults to use all columns.\\nReturns\\nDataFrameA DataFrame with only one copy of duplicate rows.\\nExamples\\nInput:\\n>>> df.collect()\\n   A  B    C\\n0  1  A  100\\n1  1  A  101\\n2  1  A  102\\n3  1  B  100\\n4  1  B  101\\n5  1  B  102\\n6  1  B  103\\n7  2  A  100\\n8  2  A  100\\nDrop duplicates based on the values of a subset of columns:\\n>>> df.drop_duplicates([\"A\", \"B\"]).collect()\\n   A  B    C\\n0  1  A  100\\n1  1  B  100\\n2  2  A  100\\nDistinct values on the entire data set:\\n>>> df.drop_duplicates().collect()\\n   A  B    C\\n0  1  A  102\\n1  1  B  103\\n2  1  A  101\\n3  2  A  100\\n4  1  B  101\\n5  1  A  100\\n6  1  B  100\\n7  1  B  102\\ndropna(self, how=None, thresh=None, subset=None)¬∂\\nReturns a new DataFrame with NULLs removed.\\nParameters\\nhow{‚Äòany‚Äô, ‚Äòall‚Äô}, optionalIf provided, ‚Äòany‚Äô eliminates rows with any NULLs,             and ‚Äòall‚Äô eliminates rows that are entirely NULLs.             If neither how nor thresh are provided, how             defaults to ‚Äòany‚Äô.\\nthreshint, optionalIf provided, rows with fewer than thresh non-NULL values             are dropped.             You cannot specify both how and thresh.\\nsubsetlist of str, optionalThe columns to consider when looking for NULLs. Values in\\nother columns are ignored, whether they are NULL or not.\\nDefaults to all columns.\\nReturns\\nDataFrameA new DataFrame with a SELECT statement that removes NULLs.\\nExamples\\nDropping rows with any nulls:\\n>>> df.collect()\\n     A    B    C\\n0  1.0  3.0  5.0\\n1  2.0  4.0  NaN\\n2  3.0  NaN  NaN\\n3  NaN  NaN  NaN\\n>>> df.dropna().collect()\\n     A    B    C\\n0  1.0  3.0  5.0\\nDropping rows that are entirely nulls:\\n>>> df.dropna(how=\\'all\\').collect()\\n     A    B    C\\n0  1.0  3.0  5.0\\n1  2.0  4.0  NaN\\n2  3.0  NaN  NaN\\nDropping rows with less than 2 non-null values:\\n>>> df.dropna(thresh=2).collect()\\n     A    B    C\\n0  1.0  3.0  5.0\\n1  2.0  4.0  NaN\\ndtypes(self, subset=None)¬∂\\nReturns a sequence of tuples describing the DataFrame‚Äôs SQL types.\\nThe tuples list the name, SQL type name, and display size\\ncorresponding to the DataFrame‚Äôs columns.\\nParameters\\nsubsetlist of str, optionalThe columns that the information is generated from.\\nDefaults to all columns.\\nReturns\\ndtypeslist of tuplesEach tuple consists of the name, SQL type name, and display size\\nfor one of the DataFrame‚Äôs columns. The list is in the order\\nspecified by the subset, or in the DataFrame‚Äôs original column\\norder if a subset is not provided.\\nempty(self)¬∂\\nReturns True if this DataFrame has 0 rows.\\nParameters\\nnone\\nReturns\\nemptyboolTrue if the DataFrame is empty.\\nNotes\\nIf a DataFrame contains only NULLs, it is not considered empty.\\nExamples\\n>>> df1.collect()\\nEmpty DataFrame\\nColumns: [ACTUAL, PREDICT]\\nIndex: []\\n>>> df1.empty()\\nTrue\\n>>> df2.collect()\\n  ACTUAL PREDICT\\n0   None    None\\n>>> df2.empty()\\nFalse\\nfilter(self, condition)¬∂\\nSelects rows that match the given condition.\\nVery little checking is done on the condition string.\\nUse only with trusted inputs.\\nParameters\\nconditionstrA filter condition. Format as SQL <condition>.\\nReturns\\nDataFrameA DataFrame with rows that match the given condition.\\nRaises\\nhana_ml.ml_exceptions.BadSQLErrorIf comments or malformed tokens are detected in condition.\\nMay have false positives and false negatives.\\nExamples\\n>>> df.collect()\\n   A  B\\n0  1  3\\n1  2  4\\n>>> df.filter(\\'B < 4\\').collect()\\n   A  B\\n0  1  3\\nhas(self, col)¬∂\\nReturns True if a column is in the DataFrame.\\nParameters\\ncolstrThe name of column to search in the projection list of this DataFrame.\\nReturns\\nboolReturns True if the column exists in the DataFrame‚Äôs projection list.\\nExamples\\n>>> df.columns\\n[\\'A\\', \\'B\\']\\n>>> df.has(\\'A\\')\\nTrue\\n>>> df.has(\\'C\\')\\nFalse\\nhead(self, n=1)¬∂\\nReturns a DataFrame of the first n rows in the current DataFrame.\\nParameters\\nnint, optionalThe number of rows returned. Default value: 1.\\nReturns\\nDataFrameA new DataFrame of the first n rows of the current DataFrame.\\nhasna(self, cols=None)¬∂\\nReturns True if a DataFrame contains NULLs.\\nParameters\\ncolsstr or list of str, optionalA column or list of columns to be checked for NULL values.\\nDefaults to all columns.\\nReturns\\nboolTrue if this DataFrame contains NULLs.\\nExamples\\n>>> df1.collect()\\n  ACTUAL PREDICT\\n0   1.0    None\\n>>> df1.hasna()\\nTrue\\nfillna(self, value, subset=None)¬∂\\nReturns a DataFrame with NULLs replaced with a specified value.\\nYou can only fill a DataFrame with numeric columns.\\nParameters\\nvalueint or floatThe value that replaces NULL. value should have a type that is\\nappropriate for the selected columns.\\nsubsetlist of str, optionalA list of columns whose NULL values will be replaced.\\nDefaults to all columns.\\nReturns\\nDataFrameA new DataFrame with the NULL values replaced.\\njoin(self, other, condition, how=\\'inner\\', select=None)¬∂\\nReturns a new DataFrame that is a join of the current DataFrame with\\nanother specified DataFrame.\\nParameters\\notherDataFrameThe DataFrame to join with.\\nconditionstrThe join predicate.\\nhow{‚Äòinner‚Äô, ‚Äòleft‚Äô, ‚Äòright‚Äô, ‚Äòouter‚Äô}, optionalThe type of join. Defaults to ‚Äòinner‚Äô.\\nselectlist, optionalIf provided, each element specifies a column in the result.\\nA string in the select list should be the name of a column in\\none of the input DataFrames. A (expression, name) tuple creates\\na new column with the given name, computed from the given\\nexpression.\\nIf this value is not provided, defaults to selecting all columns from both\\nDataFrames, with the left DataFrame‚Äôs columns first.\\nReturns\\nDataFrameA new DataFrame object made from the join of the current DataFrame\\nwith another DataFrame.\\nRaises\\nhana_ml.ml_exceptions.BadSQLErrorIf comments or malformed tokens are detected in condition\\nor in a column expression.\\nMay have false positives and false negatives.\\nExamples\\nUse the expression selection functionality to disambiguate duplicate\\ncolumn names in a join:\\n>>> df1.collect()\\n   A  B    C\\n0  1  2  3.5\\n1  2  4  5.6\\n2  3  3  1.1\\n>>> df2.collect()\\n   A  B     D\\n0  2  1  14.0\\n1  3  4   5.6\\n2  4  3   0.0\\n>>> df1.alias(\\'L\\').join(df2.alias(\\'R\\'), \\'L.A = R.A\\', select=[\\n...     (\\'L.A\\', \\'A\\'),\\n...     (\\'L.B\\', \\'B_LEFT\\'),\\n...     (\\'R.B\\', \\'B_RIGHT\\'),\\n...     \\'C\\',\\n...     \\'D\\']).collect()\\n   A  B_LEFT  B_RIGHT    C     D\\n0  2       4        1  5.6  14.0\\n1  3       3        4  1.1   5.6\\nsave(self, where, table_type=None, force=False)¬∂\\nCreates a table or view holding the current DataFrame‚Äôs data.\\nParameters\\nwherestr or (str, str) tupleThe table name or (schema name, table name) tuple. If no schema\\nis provided, then the table or view is created in the current\\nschema.\\ntable_typestr, optionalThe type of table to create. The value is case insensitive.\\nPermanent table options:\\n‚ÄúROW‚Äù\\n‚ÄúCOLUMN‚Äù\\n‚ÄúHISTORY COLUMN‚Äù\\nTemporary table options:\\n‚ÄúGLOBAL TEMPORARY‚Äù\\n‚ÄúGLOBAL TEMPORARY COLUMN‚Äù\\n‚ÄúLOCAL TEMPORARY‚Äù\\n‚ÄúLOCAL TEMPORARY COLUMN‚Äù\\nNot a table:\\n‚ÄúVIEW‚Äù\\nDefaults to ‚ÄòLOCAL TEMPORARY COLUMN‚Äô if where starts\\nwith ‚Äò#‚Äô. Otherwise, the default is ‚ÄòCOLUMN‚Äô.\\nforcebool, optionalIf force is True, it will replace the existing table.\\nReturns\\nDataFrameA DataFrame that represents the new table or view.\\nNotes\\nFor this operation to succeed, the table name must not be in\\nuse, the schema must exist, and the user must have permission\\nto create tables (or views) in the target schema.\\nsort(self, cols, desc=False)¬∂\\nReturns a new DataFrame sorted by the specified columns.\\nParameters\\ncolsstr or list of strA column or list of columns to sort by.\\nIf a list is specified, then the sort order in parameter desc is used\\nfor all columns.\\ndescbool, optionalSet to True to sort in descending order. Defaults to False,\\nfor ascending order. Default value is False.\\nReturns\\nDataFrameNew DataFrame object with rows sorted as specified.\\nselect(self, *cols)¬∂\\nReturns a new DataFrame with columns derived from the current DataFrame.\\nWarning\\nThere is no check that inputs interpreted as SQL expressions are\\nactually valid expressions; an ‚Äúexpression‚Äù like\\n‚ÄúA FROM TAB; DROP TABLE IMPORTANT_THINGS; SELECT A‚Äù can cause\\na lot of damage.\\nParameters\\ncolsstr or (str, str) tuple.The columns in the new DataFrame. A string is treated as the name\\nof a column to select; a (str, str) tuple is treated as\\n(SQL expression, alias). As a special case, ‚Äò*‚Äô is expanded\\nto all columns of the original DataFrame.\\nReturns\\nDataFrameA new DataFrame object with the specified columns.\\nRaises\\nhana_ml.ml_exceptions.BadSQLErrorIf comments or malformed tokens are detected in a column\\nexpression. May have false positives and false negatives.\\nExamples\\nInput:\\n>>> df.collect()\\n   A  B  C\\n0  1  2  3\\nSelecting a subset of existing columns:\\n>>> df.select(\\'A\\', \\'B\\').collect()\\n   A  B\\n0  1  2\\nComputing a new column based on existing columns:\\n>>> df.select(\\'*\\', (\\'B * 4\\', \\'D\\')).collect()\\n   A  B  C  D\\n0  1  2  3  8\\nunion(self, other, all=True)¬∂\\nCombines this DataFrame‚Äôs rows and another DataFrame‚Äôs rows into\\none DataFrame. This operation is equivalent to a SQL UNION ALL.\\nParameters\\notherDataFrameThe right side of the union.\\nallbool, optionalIf True, keep duplicate rows; equivalent to UNION ALL in SQL.\\nIf False, keep only one copy of duplicate rows (even if they\\ncome from the same side of the union); equivalent to a UNION\\nor a UNION ALL followed by DISTINCT in SQL.\\nDefaults to True.\\nReturns\\nDataFrameThe combined data from self and other.\\nExamples\\nWe have two DataFrames we want to union, with some duplicate rows:\\n>>> df1.collect()\\n   A  B\\n0  1  2\\n1  1  2\\n2  2  3\\n>>> df2.collect()\\n   A  B\\n0  2  3\\n1  3  4\\nunion() produces a DataFrame that contains all rows of both df1\\nand df2, like a UNION ALL:\\n>>> df1.union(df2).collect()\\n   A  B\\n0  1  2\\n1  1  2\\n2  2  3\\n3  2  3\\n4  3  4\\nTo get the deduplication effect of a UNION DISTINCT, pass\\nall=False or call distinct() after union():\\n>>> df1.union(df2, all=False).collect()\\n   A  B\\n0  1  2\\n1  2  3\\n2  3  4\\n>>> df1.union(df2).distinct().collect()\\n   A  B\\n0  1  2\\n1  2  3\\n2  3  4\\ncollect(self)¬∂\\nCopies the current DataFrame to a new Pandas DataFrame.\\nParameters\\nNone\\nReturns\\npandas.DataFrameA Pandas DataFrame that contains the current DataFrame‚Äôs data.\\nExamples\\nViewing a hana_ml DataFrame doesn‚Äôt execute the underlying SQL or fetch the data:\\n>>> df = cc.table(\\'T\\')\\n>>> df\\n<hana_ml.dataframe.DataFrame object at 0x7f2b7f24ddd8>\\nUsing collect() executes the SQL and fetchs the results into a Pandas DataFrame:\\n>>> df.collect()\\n   A  B\\n0  1  3\\n1  2  4\\n>>> type(df.collect())\\n<class \\'pandas.core.frame.DataFrame\\'>\\nrename_columns(self, names)¬∂\\nReturns a DataFrame with renamed columns.\\nParameters\\nnameslist or dictIf a list, specifies new names for every column in this DataFrame.\\nIf a dict, each dict entry maps an existing name to a new name,\\nand not all columns need to be renamed.\\nReturns\\nDataFrameThe same data as the original DataFrame with new column names.\\nSee also\\nDataFrame.aliasFor renaming the DataFrame itself.\\nExamples\\n>>> df.collect()\\n   A  B\\n0  1  3\\n1  2  4\\n>>> df.rename_columns([\\'C\\', \\'D\\']).collect()\\n   C  D\\n0  1  3\\n1  2  4\\n>>> df.rename_columns({\\'B\\': \\'D\\'}).collect()\\n   A  D\\n0  1  3\\n1  2  4\\ncast(self, cols, new_type)¬∂\\nReturns a DataFrame with columns cast to a new type.\\nThe name of the column in the returned DataFrame is the same as the original column.\\nWarning\\nType compatibility between the existing column type and the new type is not checked.\\nAn incompatibility results in an error.\\nParameters\\ncolsstr or listThe column(s) to be cast to a different type.\\nnew_typestrThe database datatype to cast the column(s) to.\\nNo checks are performed to see if the new type is valid.\\nAn invalid type can lead to SQL errors or even SQL injection vulnerabilities.\\nReturns\\nDataFrameThe same data as this DataFrame, but with columns cast to the specified type.\\nExamples\\nInput:\\n>>> df1 = cc.sql(\\'SELECT \"AGE\", \"PDAYS\", \"HOUSING\" FROM DBM_TRAINING_TBL\\')\\n>>> df1.dtypes()\\n[(\\'AGE\\', \\'INT\\', 10), (\\'PDAYS\\', \\'INT\\', 10), (\\'HOUSING\\', \\'VARCHAR\\', 100)]\\nCasting a column to NVARCHAR(20):\\n>>> df2 = df1.cast(\\'AGE\\', \\'NVARCHAR(20)\\')\\n>>> df2.dtypes()\\n[(\\'AGE\\', \\'NVARCHAR\\', 20), (\\'PDAYS\\', \\'INT\\', 10), (\\'HOUSING\\', \\'VARCHAR\\', 100)]\\nCasting a list of columns to NVARCHAR(50):\\n>>> df3 = df1.cast([\\'AGE\\', \\'PDAYS\\'], \\'NVARCHAR(50)\\')\\n>>> df3.dtypes()\\n[(\\'AGE\\', \\'NVARCHAR\\', 50), (\\'PDAYS\\', \\'NVARCHAR\\', 50), (\\'HOUSING\\', \\'VARCHAR\\', 100)]\\nto_head(self, col)¬∂\\nReturns a DataFrame with specified column as the first item in the projection.\\nParameters\\ncolstrThe column to move to the first position.\\nReturns\\nDataFrameThe same data as this DataFrame but with the specified column in the first position.\\nExamples\\nInput:\\n>>> df1 = cc.table(\"DBM_TRAINING\")\\n>>> import pprint\\n>>> pprint.pprint(df1.columns)\\n[\\'ID\\',\\n \\'AGE\\',\\n \\'JOB\\',\\n \\'MARITAL\\',\\n \\'EDUCATION\\',\\n \\'DBM_DEFAULT\\',\\n \\'HOUSING\\',\\n \\'LOAN\\',\\n \\'CONTACT\\',\\n \\'DBM_MONTH\\',\\n \\'DAY_OF_WEEK\\',\\n \\'DURATION\\',\\n \\'CAMPAIGN\\',\\n \\'PDAYS\\',\\n \\'PREVIOUS\\',\\n \\'POUTCOME\\',\\n \\'EMP_VAR_RATE\\',\\n \\'CONS_PRICE_IDX\\',\\n \\'CONS_CONF_IDX\\',\\n \\'EURIBOR3M\\',\\n \\'NREMPLOYED\\',\\n \\'LABEL\\']\\nMoving the column ‚ÄòLABEL‚Äô to head:\\n>>> df2 = df1.to_head(\\'LABEL\\')\\n>>> pprint.pprint(df2.columns)\\n[\\'LABEL\\',\\n \\'ID\\',\\n \\'AGE\\',\\n \\'JOB\\',\\n \\'MARITAL\\',\\n \\'EDUCATION\\',\\n \\'DBM_DEFAULT\\',\\n \\'HOUSING\\',\\n \\'LOAN\\',\\n \\'CONTACT\\',\\n \\'DBM_MONTH\\',\\n \\'DAY_OF_WEEK\\',\\n \\'DURATION\\',\\n \\'CAMPAIGN\\',\\n \\'PDAYS\\',\\n \\'PREVIOUS\\',\\n \\'POUTCOME\\',\\n \\'EMP_VAR_RATE\\',\\n \\'CONS_PRICE_IDX\\',\\n \\'CONS_CONF_IDX\\',\\n \\'EURIBOR3M\\',\\n \\'NREMPLOYED\\']\\ndescribe(self, cols=None)¬∂\\nReturns a DataFrame that contains various statistics for the requested column(s).\\nParameters\\ncolsstr or list, optionalThe column(s) to be described. Defaults to all columns.\\nReturns\\nDataFrameA DataFrame that contains statistics for the specified column(s)\\nin the current DataFrame.\\nThe statistics included are the count of rows (‚Äúcount‚Äù),\\nnumber of distinct values (‚Äúunique‚Äù),\\nnumber of nulls (‚Äúnulls‚Äù),\\naverage (‚Äúmean‚Äù),\\nstandard deviation(‚Äústd‚Äù)\\nmedian (‚Äúmedian‚Äù),\\nminimum value (‚Äúmin‚Äù),\\nmaximum value (‚Äúmax‚Äù),\\n25% percentile when treated as continuous variable (‚Äú25_percent_cont‚Äù),\\n25% percentile when treated as discrete variable (‚Äú25_percent_disc‚Äù),\\n50% percentile when treated as continuous variable (‚Äú50_percent_cont‚Äù),\\n50% percentile when treated as discrete variable (‚Äú50_percent_disc‚Äù),\\n75% percentile when treated as continuous variable (‚Äú75_percent_cont‚Äù),\\n75% percentile when treated as discrete variable (‚Äú75_percent_disc‚Äù).\\nFor columns that are strings, statistics such as average (‚Äúmean‚Äù),\\nstandard deviation (‚Äústd‚Äù), median (‚Äúmedian‚Äù), and the various percentiles\\nare NULLs.\\nIf the list of columns contain both string and numeric data types,\\nminimum and maximum values become NULLs.\\nExamples\\nInput:\\n>>> df1 = cc.table(\"DBM_TRAINING\")\\n>>> import pprint\\n>>> pprint.pprint(df2.columns)\\n[\\'LABEL\\',\\n \\'ID\\',\\n \\'AGE\\',\\n \\'JOB\\',\\n \\'MARITAL\\',\\n \\'EDUCATION\\',\\n \\'DBM_DEFAULT\\',\\n \\'HOUSING\\',\\n \\'LOAN\\',\\n \\'CONTACT\\',\\n \\'DBM_MONTH\\',\\n \\'DAY_OF_WEEK\\',\\n \\'DURATION\\',\\n \\'CAMPAIGN\\',\\n \\'PDAYS\\',\\n \\'PREVIOUS\\',\\n \\'POUTCOME\\',\\n \\'EMP_VAR_RATE\\',\\n \\'CONS_PRICE_IDX\\',\\n \\'CONS_CONF_IDX\\',\\n \\'EURIBOR3M\\',\\n \\'NREMPLOYED\\']\\nDescribe a few numeric columns and collect them to return a Pandas DataFrame:\\n>>> df1.describe([\\'AGE\\', \\'PDAYS\\']).collect()\\n  column  count  unique  nulls        mean         std  min  max  median\\n0    AGE  16895      78      0   40.051376   10.716907   17   98      38\\n1  PDAYS  16895      24      0  944.406688  226.331944    0  999     999\\n   25_percent_cont  25_percent_disc  50_percent_cont  50_percent_disc\\n0             32.0               32             38.0               38\\n1            999.0              999            999.0              999\\n   75_percent_cont  75_percent_disc\\n0             47.0               47\\n1            999.0              999\\nDescribe some non-numeric columns and collect them to return a Pandas DataFrame:\\n>>> df1.describe([\\'JOB\\', \\'MARITAL\\']).collect()\\n    column  count  unique  nulls  mean   std       min      max median\\n0      JOB  16895      12      0  None  None    admin.  unknown   None\\n1  MARITAL  16895       4      0  None  None  divorced  unknown   None\\n  25_percent_cont 25_percent_disc 50_percent_cont 50_percent_disc\\n0            None            None            None            None\\n1            None            None            None            None\\n  75_percent_cont 75_percent_disc\\n0            None            None\\n1            None            None\\nDescribe all columns in a DataFrame:\\n>>> df1.describe().collect()\\n            column  count  unique  nulls          mean           std\\n0               ID  16895   16895      0  21282.286652  12209.759725\\n1              AGE  16895      78      0     40.051376     10.716907\\n2         DURATION  16895    1267      0    263.965670    264.331384\\n3         CAMPAIGN  16895      35      0      2.344658      2.428449\\n4            PDAYS  16895      24      0    944.406688    226.331944\\n5         PREVIOUS  16895       7      0      0.209529      0.539450\\n6     EMP_VAR_RATE  16895      10      0     -0.038798      1.621945\\n7   CONS_PRICE_IDX  16895      26      0     93.538844      0.579189\\n8    CONS_CONF_IDX  16895      26      0    -40.334123      4.865720\\n9        EURIBOR3M  16895     283      0      3.499297      1.777986\\n10      NREMPLOYED  16895      11      0   5160.371885     75.320580\\n11             JOB  16895      12      0           NaN           NaN\\n12         MARITAL  16895       4      0           NaN           NaN\\n13       EDUCATION  16895       8      0           NaN           NaN\\n14     DBM_DEFAULT  16895       2      0           NaN           NaN\\n15         HOUSING  16895       3      0           NaN           NaN\\n16            LOAN  16895       3      0           NaN           NaN\\n17         CONTACT  16895       2      0           NaN           NaN\\n18       DBM_MONTH  16895      10      0           NaN           NaN\\n19     DAY_OF_WEEK  16895       5      0           NaN           NaN\\n20        POUTCOME  16895       3      0           NaN           NaN\\n21           LABEL  16895       2      0           NaN           NaN\\n         min        max     median  25_percent_cont  25_percent_disc\\n0      5.000  41187.000  21786.000        10583.500        10583.000\\n1     17.000     98.000     38.000           32.000           32.000\\n2      0.000   4918.000    184.000          107.000          107.000\\n3      1.000     43.000      2.000            1.000            1.000\\n4      0.000    999.000    999.000          999.000          999.000\\n5      0.000      6.000      0.000            0.000            0.000\\n6     -3.400      1.400      1.100           -1.800           -1.800\\n7     92.201     94.767     93.444           93.075           93.075\\n8    -50.800    -26.900    -41.800          -42.700          -42.700\\n9      0.634      5.045      4.856            1.313            1.313\\n10  4963.000   5228.000   5191.000         5099.000         5099.000\\n11       NaN        NaN        NaN              NaN              NaN\\n12       NaN        NaN        NaN              NaN              NaN\\n13       NaN        NaN        NaN              NaN              NaN\\n14       NaN        NaN        NaN              NaN              NaN\\n15       NaN        NaN        NaN              NaN              NaN\\n16       NaN        NaN        NaN              NaN              NaN\\n17       NaN        NaN        NaN              NaN              NaN\\n18       NaN        NaN        NaN              NaN              NaN\\n19       NaN        NaN        NaN              NaN              NaN\\n20       NaN        NaN        NaN              NaN              NaN\\n21       NaN        NaN        NaN              NaN              NaN\\n    50_percent_cont  50_percent_disc  75_percent_cont  75_percent_disc\\n0         21786.000        21786.000        32067.500        32068.000\\n1            38.000           38.000           47.000           47.000\\n2           184.000          184.000          324.000          324.000\\n3             2.000            2.000            3.000            3.000\\n4           999.000          999.000          999.000          999.000\\n5             0.000            0.000            0.000            0.000\\n6             1.100            1.100            1.400            1.400\\n7            93.444           93.444           93.994           93.994\\n8           -41.800          -41.800          -36.400          -36.400\\n9             4.856            4.856            4.961            4.961\\n10         5191.000         5191.000         5228.000         5228.000\\n11              NaN              NaN              NaN              NaN\\n12              NaN              NaN              NaN              NaN\\n13              NaN              NaN              NaN              NaN\\n14              NaN              NaN              NaN              NaN\\n15              NaN              NaN              NaN              NaN\\n16              NaN              NaN              NaN              NaN\\n17              NaN              NaN              NaN              NaN\\n18              NaN              NaN              NaN              NaN\\n19              NaN              NaN              NaN              NaN\\n20              NaN              NaN              NaN              NaN\\n21              NaN              NaN              NaN              NaN\\nbin(self, col, strategy=\\'uniform_number\\', bins=None, bin_width=None, bin_column=\\'BIN_NUMBER\\')¬∂\\nReturns a DataFrame with the original columns as well as bin assignments.\\nThe name of the columns in the returned DataFrame is the same as the\\noriginal column. Column ‚ÄúBIN_NUMBER‚Äù or the specified value in\\nbin_column is added and corresponds to the bin assigned.\\nParameters\\ncolstrThe column on which binning is performed.\\nThe column must be numeric.\\nstrategy{‚Äòuniform_number‚Äô, ‚Äòuniform_size‚Äô}, optionalBinning methods:\\n‚Äòuniform_number‚Äô: Equal widths based on the number of bins.\\n‚Äòuniform_size‚Äô: Equal widths based on the bin size.\\nDefault value is ‚Äòuniform_number‚Äô.\\nbinsint, optionalThe number of equal-width bins.\\nOnly valid when strategy is ‚Äòuniform_number‚Äô.\\nDefaults to 10.\\nbin_widthint, optionalThe interval width of each bin.\\nOnly valid when strategy is ‚Äòuniform_size‚Äô.\\nbin_columnstr, optionalThe name of the output column that contains the bin number.\\nReturns\\nDataFrameA binned dataset with the same data as this DataFrame,\\nas well as an additional column ‚ÄúBIN_NUMBER‚Äù or the value specified\\nin bin_column. This additional column contains the\\nassigned bin for each row.\\nExamples\\nInput:\\n>>> df.collect()\\n   C1   C2    C3       C4\\n0   1  1.2   2.0      1.0\\n1   2  1.4   4.0      3.0\\n2   3  1.6   6.0      9.0\\n3   4  1.8   8.0     27.0\\n4   5  2.0  10.0     81.0\\n5   6  2.2  12.0    243.0\\n6   7  2.4  14.0    729.0\\n7   8  2.6  16.0   2187.0\\n8   9  2.8  18.0   6561.0\\n9  10  3.0  20.0  19683.0\\nCreate five bins of equal widths on C1:\\n>>> df.bin(\\'C1\\', strategy=\\'uniform_number\\', bins=5).collect()\\n   C1   C2    C3       C4  BIN_NUMBER\\n0   1  1.2   2.0      1.0           1\\n1   2  1.4   4.0      3.0           1\\n2   3  1.6   6.0      9.0           2\\n3   4  1.8   8.0     27.0           2\\n4   5  2.0  10.0     81.0           3\\n5   6  2.2  12.0    243.0           3\\n6   7  2.4  14.0    729.0           4\\n7   8  2.6  16.0   2187.0           4\\n8   9  2.8  18.0   6561.0           5\\n9  10  3.0  20.0  19683.0           5\\nCreate five bins of equal widths on C2:\\n>>> df.bin(\\'C3\\', strategy=\\'uniform_number\\', bins=5).collect()\\n   C1   C2    C3       C4  BIN_NUMBER\\n0   1  1.2   2.0      1.0           1\\n1   2  1.4   4.0      3.0           1\\n2   3  1.6   6.0      9.0           2\\n3   4  1.8   8.0     27.0           2\\n4   5  2.0  10.0     81.0           3\\n5   6  2.2  12.0    243.0           3\\n6   7  2.4  14.0    729.0           4\\n7   8  2.6  16.0   2187.0           4\\n8   9  2.8  18.0   6561.0           5\\n9  10  3.0  20.0  19683.0           5\\nCreate five bins of equal widths on a column that varies significantly:\\n>>> df.bin(\\'C4\\', strategy=\\'uniform_number\\', bins=5).collect()\\n   C1   C2    C3       C4  BIN_NUMBER\\n0   1  1.2   2.0      1.0           1\\n1   2  1.4   4.0      3.0           1\\n2   3  1.6   6.0      9.0           1\\n3   4  1.8   8.0     27.0           1\\n4   5  2.0  10.0     81.0           1\\n5   6  2.2  12.0    243.0           1\\n6   7  2.4  14.0    729.0           1\\n7   8  2.6  16.0   2187.0           1\\n8   9  2.8  18.0   6561.0           2\\n9  10  3.0  20.0  19683.0           5\\nCreate bins of equal width:\\n>>> df.bin(\\'C1\\', strategy=\\'uniform_size\\', bin_width=3).collect()\\n   C1   C2    C3       C4  BIN_NUMBER\\n0   1  1.2   2.0      1.0           1\\n1   2  1.4   4.0      3.0           1\\n2   3  1.6   6.0      9.0           2\\n3   4  1.8   8.0     27.0           2\\n4   5  2.0  10.0     81.0           2\\n5   6  2.2  12.0    243.0           3\\n6   7  2.4  14.0    729.0           3\\n7   8  2.6  16.0   2187.0           3\\n8   9  2.8  18.0   6561.0           4\\n9  10  3.0  20.0  19683.0           4\\nagg(self, agg_list, group_by=None)¬∂\\nReturns a DataFrame with the group_by column along with the aggregates.\\nThe name of the column in the returned DataFrame is the same as the\\noriginal column.\\nParameters\\nagg_listA list of tuplesA list of tuples. Each tuple is a triplet.\\nThe triplet consists of (aggregate_operator, expression, name) where:\\naggregate_operator is one of [‚Äòmax‚Äô, ‚Äòmin‚Äô, ‚Äòcount‚Äô, ‚Äòavg‚Äô]\\nexpression is a str that is a column or column expression\\nname that is the name of this aggregate in the project list.\\ngroup_bystr or list of str, optionalThe group by column. Only a column is allowed although\\nexpressions are allowed in SQL. To group by an expression, create a\\nDataFrame  by providing the entire SQL.\\nSo, if you have a table T with columns C1, C2, and C3 that are all\\nintegers, to calculate the max(C1) grouped by (C2+C3) a DataFrame\\nwould need to be created as below:\\ncc.sql(‚ÄòSELECT ‚ÄúC2‚Äù+‚ÄùC3‚Äù, max(‚ÄúC1‚Äù) FROM ‚ÄúT‚Äù GROUP BY ‚ÄúC2‚Äù+‚ÄùC3‚Äù‚Äô)\\nReturns\\nDataFrameA DataFrame containing the group_by column (if it exists), as well as\\nthe aggregate expressions that are aliased with the specified names.\\nExamples\\nInput:\\n>>> df.collect()\\n    ID  SEPALLENGTHCM  SEPALWIDTHCM  PETALLENGTHCM  PETALWIDTHCM          SPECIES\\n0    1            5.1           3.5            1.4           0.2      Iris-setosa\\n1    2            4.9           3.0            1.4           0.2      Iris-setosa\\n2    3            4.7           3.2            1.3           0.2      Iris-setosa\\n3   51            7.0           3.2            4.7           1.4  Iris-versicolor\\n4   52            6.4           3.2            4.5           1.5  Iris-versicolor\\n5  101            6.3           3.3            6.0           2.5   Iris-virginica\\n6  102            5.8           2.7            5.1           1.9   Iris-virginica\\n7  103            7.1           3.0            5.9           2.1   Iris-virginica\\n8  104            6.3           2.9            5.6           1.8   Iris-virginica\\nAnother way to do a count:\\n>>> df.agg([(\\'count\\', \\'SPECIES\\', \\'COUNT\\')]).collect()\\n    COUNT\\n0      9\\nGet counts by SPECIES:\\n>>> df.agg([(\\'count\\', \\'SPECIES\\', \\'COUNT\\')], group_by=\\'SPECIES\\').collect()\\n           SPECIES  COUNT\\n0  Iris-versicolor      2\\n1   Iris-virginica      4\\n2      Iris-setosa      3\\nGet max values of SEPALLENGTHCM by SPECIES:\\n>>> df.agg([(\\'max\\', \\'SEPALLENGTHCM\\', \\'MAX_SEPAL_LENGTH\\')], group_by=\\'SPECIES\\').collect()\\n           SPECIES  MAX_SEPAL_LENGTH\\n0  Iris-versicolor               7.0\\n1   Iris-virginica               7.1\\n2      Iris-setosa               5.1\\nGet max and min values of SEPALLENGTHCM by SPECIES:\\n>>> df.agg([(\\'max\\', \\'SEPALLENGTHCM\\', \\'MAX_SEPAL_LENGTH\\'),\\n    (\\'min\\', \\'SEPALLENGTHCM\\', \\'MIN_SEPAL_LENGTH\\')], group_by=[\\'SPECIES\\']).collect()\\n           SPECIES  MAX_SEPAL_LENGTH  MIN_SEPAL_LENGTH\\n0  Iris-versicolor               7.0               6.4\\n1   Iris-virginica               7.1               5.8\\n2      Iris-setosa               5.1               4.7\\nGet aggregate grouping by multiple columns:\\n>>> df.agg([(\\'count\\', \\'SEPALLENGTHCM\\', \\'COUNT_SEPAL_LENGTH\\')],\\n            group_by=[\\'SPECIES\\', \\'PETALLENGTHCM\\']).collect()\\n           SPECIES  PETALLENGTHCM  COUNT_SEPAL_LENGTH\\n0   Iris-virginica            6.0                   1\\n1      Iris-setosa            1.3                   1\\n2   Iris-virginica            5.9                   1\\n3   Iris-virginica            5.6                   1\\n4      Iris-setosa            1.4                   2\\n5  Iris-versicolor            4.7                   1\\n6  Iris-versicolor            4.5                   1\\n7   Iris-virginica            5.1                   1\\nis_numeric(self, cols=None)¬∂\\nReturns True if the column(s) in the DataFrame are numeric.\\nParameters\\ncolsstr or list, optionalThe column(s) to be tested for being numeric. Defaults to all columns.\\nReturns\\nboolTrue if all the columns are numeric.\\nExamples\\nInput:\\n>>> df.head(5).collect()\\nID  SEPALLENGTHCM  SEPALWIDTHCM  PETALLENGTHCM  PETALWIDTHCM      SPECIES\\n0   1            5.1           3.5            1.4           0.2  Iris-setosa\\n1   2            4.9           3.0            1.4           0.2  Iris-setosa\\n2   3            4.7           3.2            1.3           0.2  Iris-setosa\\n3   4            4.6           3.1            1.5           0.2  Iris-setosa\\n4   5            5.0           3.6            1.4           0.2  Iris-setosa\\n>>> pprint.pprint(df.dtypes())\\n[(\\'ID\\', \\'INT\\', 10),\\n (\\'SEPALLENGTHCM\\', \\'DOUBLE\\', 15),\\n (\\'SEPALWIDTHCM\\', \\'DOUBLE\\', 15),\\n (\\'PETALLENGTHCM\\', \\'DOUBLE\\', 15),\\n (\\'PETALWIDTHCM\\', \\'DOUBLE\\', 15),\\n (\\'SPECIES\\', \\'NVARCHAR\\', 15)]\\nTest a single column:\\n>>> df.is_numeric(\\'ID\\')\\nTrue\\n>>> df.is_numeric(\\'SEPALLENGTHCM\\')\\nTrue\\n>>> df.is_numeric([\\'SPECIES\\'])\\nFalse\\nTest a list of columns:\\n>>> df.is_numeric([\\'SEPALLENGTHCM\\', \\'PETALLENGTHCM\\', \\'PETALWIDTHCM\\'])\\nTrue\\n>>> df.is_numeric([\\'SEPALLENGTHCM\\', \\'PETALLENGTHCM\\', \\'SPECIES\\'])\\nFalse\\ncorr(self, first_col, second_col)¬∂\\nReturns a DataFrame that gives the correlation coefficient between two\\nnumeric columns.\\nAll rows with NULL values for first_col or second_col are removed\\nprior to calculating the correlation coefficient.\\nThe correlation coefficient is:1/(n-1) * sum((col1_value - avg(col1)) * (col2 - avg(col2))) /\\n(stddev(col1) * stddev(col2))\\nParameters\\nfirst_colstrThe first column for calculating the correlation coefficient.\\nsecond_colstrThe second column for calculating the correlation coefficient.\\nReturns\\nDataFrameA DataFrame with one value that contains the correlation coefficient.\\nThe name of the column is CORR_COEFF.\\nExamples\\nInput:\\n>>> df.columns\\n[\\'C1\\', \\'C2\\', \\'C3\\', \\'C4\\']\\n>>> df.collect()\\n   C1   C2      C3       C4\\n0   1  1.2     2.0      1.0\\n1   2  1.4     4.0      3.0\\n2   3  1.6     8.0      9.0\\n3   4  1.8    16.0     27.0\\n4   5  2.0    32.0     81.0\\n5   6  2.2    64.0    243.0\\n6   7  2.4   128.0    729.0\\n7   8  2.6   256.0   2187.0\\n8   9  2.8   512.0   6561.0\\n9  10  3.0  1024.0  19683.0\\nCorrelation with columns that are well correlated:\\n>>> df.corr(\\'C1\\', \\'C2\\').collect()\\n      CORR_COEFF\\n0         1.0\\n>>> df.corr(\\'C1\\', \\'C3\\').collect()\\n    CORR_COEFF\\n0         1.0\\nCorrelation with a column whose value is three times its previous value:\\n>>> df.corr(\\'C1\\', \\'C4\\').collect()\\n    CORR_COEFF\\n0    0.696325\\npivot_table(self, values, index, columns, aggfunc=\\'avg\\')¬∂\\nReturns a DataFrame that gives the pivoted table.\\naggfunc is identical to SAP HANA aggregate functions.\\nhttps://help.sap.com/viewer/7c78579ce9b14a669c1f3295b0d8ca16/Cloud/en-US/6fff7f0ae9184d1db47a25791545a1b6.html\\nParameters\\nvaluesstr or list of strThe targeted values for pivoting.\\nindexstr or list of strThe index of the DataFrame.\\ncolumnsstr or list of strThe pivoting columns.\\naggfunc{‚Äòavg‚Äô, ‚Äòmax‚Äô, ‚Äòmin‚Äô,‚Ä¶ }, optionalaggfunc is identical to SAP HANA aggregate functions.\\nDefaults to ‚Äòavg‚Äô.\\nReturns\\nDataFrameA pivoted DataFrame.\\nExamples\\n>>> df.pivot_table(values=\\'C2\\', index=\\'C1\\', columns=\\'C3\\', aggfunc=\\'max\\')\\ngenerate_table_type(self)¬∂\\nGenerates an SAP HANA table type based on the dtypes function of the DataFrame. This is a convenience method for SQL tracing.\\nReturns\\nTable TypestrThe table type in string form.\\nhana_ml.dataframe.create_dataframe_from_pandas(connection_context, pandas_df, table_name, schema=None, force=False, replace=True, object_type_as_bin=False)¬∂\\nUploads data from a Pandas DataFrame to an SAP HANA database and returns an SAP HANA DataFrame.\\nParameters\\nconnection_contextConnectionContextA connection to the SAP HANA system.\\npandas_dfpandas.DataFrameA Pandas DataFrame for uploading to the HANA database.\\ntable_namestrThe table name in the SAP HANA database.\\nschemastr, optional, keyword-onlyThe schema name. If this value is not provided or set to None, then the value defaults to the\\nConnectionContext‚Äôs current schema.\\nDefaults to the current schema.\\nforcebool, optionalIf force is True, then the Sap HANA table with table_name is dropped.\\nDefaults to False.\\nreplacebool, optionalIf replace is True, then the SAP HANA table performs the missing value handling.\\nDefaults to True.\\nobject_type_as_binbool, optionalIf true, the object type will be considered CLOB in HANA.\\nDefaults to False.\\nReturns\\nDataFrameAn SAP HANA DataFrame that contains the data in the pandas_df.\\nExamples\\n>>> create_dataframe_from_pandas(connection_context, p_df, \\'test\\', force=False, replace=True)\\n<hana_ml.dataframe.DataFrame at 0x7efbcb26fbe0>\\nTable of Contents\\nSAP HANA Python Client API for Machine Learning Algorithms\\nChange Log for the SAP HANA Python Client API\\nhana_ml.dataframe\\nhana_ml.algorithms.apl package\\nhana_ml.algorithms.pal package\\nhana_ml.visualizers package\\nhana_ml.ml_exceptions\\nhana_ml.model_storage\\nPrevious topic\\nChange Log for the SAP HANA Python Client API\\nNext topic\\nhana_ml.algorithms.apl package\\nQuick search\\nNavigation\\nindex\\nmodules |\\nnext |\\nprevious |\\nhana_ml 1.0.8.post5 documentation ¬ª\\n', doc_id='492f68cc-2996-453d-a55c-e1c5cd14b69a', embedding=None, doc_hash='a14fa766e819722daa86aa2fab4cb3b1b8054b6dcce8596e0dba4b3fdc229bd4', extra_info=None),\n",
       "  Document(text=\"\\nHybridGradientBoostingClassifier ‚Äî hana-ml 2.16.230508 documentation\\n hana-ml\\n          \\n                2.16.230508\\n              \\nPython Machine Learning Client for SAP HANA\\nPrerequisites\\nSAP HANA DataFrame\\nMachine Learning API\\nEnd-to-End Example: Using SAP HANA Predictive Analysis Library (PAL) Module\\nEnd-to-End Example: Using SAP HANA Automated Predictive Library (APL) Module\\nVisualizers Module\\nSpatial and Graph Features\\nSummary\\nInstallation Guide\\nhana-ml Tutorials\\nChangelog\\nhana_ml.dataframe\\nhana_ml.algorithms.apl package\\nhana_ml.algorithms.apl.gradient_boosting_classification\\nhana_ml.algorithms.apl.gradient_boosting_regression\\nhana_ml.algorithms.apl.time_series\\nhana_ml.algorithms.apl.classification\\nhana_ml.algorithms.apl.regression\\nhana_ml.algorithms.apl.clustering\\nhana_ml.algorithms.pal package\\nAlgorithms\\nPAL Base\\nPALBase\\nAuto ML\\nAutomaticClassification\\nAutomaticRegression\\nAutomaticTimeSeries\\nPreprocessing\\nUnified Interface\\nUnifiedClassification\\nUnifiedRegression\\nUnifiedClustering\\nUnifiedExponentialSmoothing\\nClustering\\nAffinityPropagation\\nAgglomerateHierarchicalClustering\\nDBSCAN\\nGeometryDBSCAN\\nKMeans\\nKMedians\\nKMedoids\\nSpectralClustering\\nKMeansOutlier\\nGaussianMixture\\nSOM\\nSlightSilhouette\\noutlier_detection_kmeans\\nClassification\\nLinearDiscriminantAnalysis\\nLogisticRegression\\nOnlineMultiLogisticRegression\\nNaiveBayes\\nKNNClassifier\\nMLPClassifier\\nSVC\\nOneClassSVM\\nDecisionTreeClassifier\\nRDTClassifier\\nHybridGradientBoostingClassifier\\nRegression\\nLinearRegression\\nOnlineLinearRegression\\nKNNRegressor\\nMLPRegressor\\nPolynomialRegression\\nGLM\\nExponentialRegression\\nBiVariateGeometricRegression\\nBiVariateNaturalLogarithmicRegression\\nCoxProportionalHazardModel\\nSVR\\nDecisionTreeRegressor\\nRDTRegressor\\nHybridGradientBoostingRegressor\\nPrepocessing\\nFeatureNormalizer\\nFeatureSelection\\nIsolationForest\\nKBinsDiscretizer\\nImputer\\nDiscretize\\nMDS\\nSMOTE\\nSMOTETomek\\nTomekLinks\\nSampling\\nImputeTS\\nPCA\\nCATPCA\\ntrain_test_val_split\\nvariance_test\\nTime Series\\nAdditiveModelForecast\\nARIMA\\nAutoARIMA\\nCPD\\nBCPD\\nTimeSeriesClassification\\nSingleExponentialSmoothing\\nDoubleExponentialSmoothing\\nTripleExponentialSmoothing\\nAutoExponentialSmoothing\\nBrownExponentialSmoothing\\nCroston\\nCrostonTSB\\nGARCH\\nHierarchical_Forecast\\nLR_seasonal_adjust\\nLSTM\\nLTSF\\nOnlineARIMA\\nOutlierDetectionTS\\nGRUAttention\\nROCKET\\nVectorARIMA\\nDWT\\naccuracy_measure\\nBSTS\\ncorrelation\\nfft\\ndtw\\nfast_dtw\\nintermittent_forecast\\nperiodogram\\nstationarity_test\\nseasonal_decompose\\ntrend_test\\nwavedec\\nwaverec\\nwpdec\\nwprec\\nwhite_noise_test\\nStatistics\\nbernoulli\\nbeta\\nbinomial\\ncauchy\\nchi_squared\\nexponential\\ngumbel\\nf\\ngamma\\ngeometric\\nlognormal\\nnegative_binomial\\nnormal\\npert\\npoisson\\nstudent_t\\nuniform\\nweibull\\nmultinomial\\nmcmc\\nchi_squared_goodness_of_fit\\nchi_squared_independence\\nttest_1samp\\nttest_ind\\nttest_paired\\nf_oneway\\nf_oneway_repeated\\nunivariate_analysis\\ncovariance_matrix\\npearsonr_matrix\\niqr\\nwilcoxon\\nmedian_test_1samp\\ngrubbs_test\\nentropy\\ncondition_index\\ncdf\\nftest_equal_var\\nfactor_analysis\\nkaplan_meier_survival_analysis\\nquantile\\ndistribution_fit\\nks_test\\nKDE\\nAssociation\\nApriori\\nAprioriLite\\nFPGrowth\\nKORD\\nSPM\\nRecommender System\\nALS\\nFRM\\nFFMClassifier\\nFFMRegressor\\nFFMRanker\\nSocial Network Analysis\\nLinkPrediction\\nPageRank\\nRanking\\nSVRanking\\nMiscellaneous\\nabc_analysis\\nweighted_score_table\\nTSNE\\nMetrics\\naccuracy_score\\nauc\\nconfusion_matrix\\nmulticlass_auc\\nr2_score\\nModel and Pipeline\\nParamSearchCV\\nGridSearchCV\\nRandomSearchCV\\nPipeline\\nText Processing\\nCRF\\nLatentDirichletAllocation\\nTopics\\nModel Evaluation and Parameter Selection\\nResampling Methods\\nSearch Strategies\\nSuccessive Halving and Hyperband for Parameter Selection\\nKey Relevant Parameters\\nBiased Linear Models\\nModel State for Real-Time Scoring\\nLocal Interpretability of Models\\nSHAP\\nSurrogate\\nDirect Explanation\\nModels/Algorithms in hana_ml.algorithms.pal Packages that Support Local Interpretability\\nKey Relevant Parameters in hana-ml.algorithms.pal Package\\nExplaining the Forecasts of ARIMA\\nMethods for Residual Extraction in Time-Series Outlier Detection\\n1. Residual from Median Filter\\n2. Residual from Seasonal Decomposition\\n3. Residual Extraction from Median Filter and Seasonal Decomposition\\n4. Meaningless Parameter Combination to be Avoided\\nMethods of Outlier Detection from Residual\\n1. Z1 Score\\n2. Z2 Score\\n3. IQR Score\\n4. MAD Score\\n5. Isolation Forest Score\\n6. DBSCAN\\nGenetic Optimization in AutoML\\nIndividual Representation\\nSelection\\nCrossover\\nMutation\\nEvolutional Iteration Step\\nControl Parameters\\nProbability Density Functions for MCMC Sampling\\nMiscellaneous Topics\\nEarly Stop in HGBT\\nFeature Grouping in HGBT\\nHistogram Splitting in HGBT\\nModel Compression for Random Decision Trees\\nModel Compression for Support Vector Machine\\nSeasonalities in Additive Model Forecast\\nPrecomputed Distance Matrix as input data in UnifiedClustering\\nParameters for Misssing Value Handling in HANA DataFrame\\nParameter Mappings\\nhana_ml.visualizers package\\nhana_ml.visualizers.eda\\nhana_ml.visualizers.metrics\\nhana_ml.visualizers.m4_sampling\\nhana_ml.visualizers.model_debriefing\\nhana_ml.visualizers.dataset_report\\nhana_ml.visualizers.shap\\nhana_ml.visualizers.unified_report\\nhana_ml.visualizers.visualizer_base\\nhana_ml.visualizers.digraph\\nhana_ml.visualizers.word_cloud\\nhana_ml.visualizers.automl_progress\\nhana_ml.visualizers.automl_report\\nhana_ml.visualizers.time_series_report\\nhana_ml.ml_exceptions\\nhana_ml.model_storage\\nhana_ml.artifacts package\\nAMDP Examples\\nhana_ml.artifacts.deployers.amdp\\nhana_ml.artifacts.generators.abap\\nhana_ml.artifacts.generators.hana\\nhana_ml.docstore package\\nhana_ml.spatial package\\nhana_ml.graph package\\nhana_ml.graph.algorithms package\\nhana_ml.text.tm package\\nhana_ml.text.tm\\nFAQs\\nhana-ml\\n ¬ª\\nhana_ml.algorithms.pal package ¬ª\\nAlgorithms ¬ª\\nHybridGradientBoostingClassifier\\n View page source\\n Previous\\nNext \\nHybridGradientBoostingClassifier\\uf0c1\\nclass hana_ml.algorithms.pal.trees.HybridGradientBoostingClassifier(n_estimators=None, random_state=None, subsample=None, max_depth=None, split_threshold=None, learning_rate=None, split_method=None, sketch_eps=None, fold_num=None, min_sample_weight_leaf=None, min_samples_leaf=None, max_w_in_split=None, col_subsample_split=None, col_subsample_tree=None, lamb=None, alpha=None, base_score=None, adopt_prior=None, evaluation_metric=None, cv_metric=None, ref_metric=None, calculate_importance=None, calculate_cm=None, thread_ratio=None, resampling_method=None, param_search_strategy=None, repeat_times=None, timeout=None, progress_indicator_id=None, random_search_times=None, param_range=None, cross_validation_range=None, param_values=None, obj_func=None, replace_missing=None, default_missing_direction=None, feature_grouping=None, tol_rate=None, compression=None, max_bits=None, max_bin_num=None, resource=None, max_resource=None, reduction_rate=None, min_resource_rate=None, aggressive_elimination=None, validation_set_rate=None, stratified_validation_set=None, tolerant_iter_num=None, fg_min_zero_rate=None)\\uf0c1\\nHybrid Gradient Boosting trees model for classification.\\nParameters\\nn_estimatorsint, optionalSpecifies the number of trees in Gradient Boosting.\\nDefaults to 10.\\nsplit_method{'exact', 'sketch', 'sampling', 'histogram'}, optionalThe method to finding split point for numerical features.\\n'exact': the exact method, trying all possible points\\n'sketch': the sketch method, accounting for the distribution of the sum of hessian\\n'sampling': samples the split point randomly\\n'histogram': builds histogram upon data and uses it as split point\\nDefaults to 'exact'.\\nrandom_stateint, optionalThe seed for random number generating.\\n0: current time as seed.\\nOthers : the seed.\\nmax_depthint, optionalThe maximum depth of a tree.\\nDefaults to 6.\\nsplit_thresholdfloat, optionalSpecifies the stopping condition: if the improvement value of the best\\nsplit is less than this value, then the tree stops growing.\\nlearning_ratefloat, optional.Learning rate of each iteration, must be within the range (0, 1].\\nDefaults to 0.3.\\nsubsamplefloat, optionalThe fraction of samples to be used for fitting each base learner.\\nDefaults to 1.0.\\nfold_numint, optionalThe k value for k-fold cross-validation.\\nMandatory and valid only when resampling_method is set as of one the following:\\n'cv', 'cv_sha', 'cv_hyperband', 'stratified_cv', 'stratified_cv_sha',\\n'stratified_cv_hyperband'.\\nsketch_epsfloat, optionalThe value of the sketch method which sets up an upper limit for the sum of\\nsample weights between two split points.\\nBasically, the less this value is, the more number of split points are tried.\\nmin_sample_weight_leaffloat, optionalThe minimum summation of ample weights in a leaf node.\\nDefaults to 1.0.\\nmin_samples_leafint, optionalThe minimum number of data in a leaf node.\\nDefaults to 1.\\nmax_w_in_splitfloat, optionalThe maximum weight constraint assigned to each tree node.\\nDefaults to 0 (i.e. no constraint).\\ncol_subsample_splitfloat, optionalThe fraction of features used for each split, should be within range (0, 1].\\nDefaults to 1.0.\\ncol_subsample_treefloat, optionalThe fraction of features used for each tree growth, should be within range (0, 1]\\nDefaults to 1.0.\\nlambfloat, optionalL2 regularization weight for the target loss function.\\nShould be within range (0, 1].\\nDefaults to 1.0.\\nalphafloat, optionalWeight of L1 regularization for the target loss function.\\nDefaults to 1.0.\\nbase_scorefloat, optionalInitial prediction score for all instances. Global bias for sufficient number\\nof iterations(changing this value will not have too much effect).\\nDefaults to 0.5.\\nadopt_priorbool, optionalIndicates whether to adopt the prior distribution as the initial point.\\nFrequencies of class labels are used for classification problems.\\nbase_score is ignored if this parameter is set to True.\\nDefaults to False.\\nevaluation_metric{'rmse', 'mae', 'nll', 'error_rate', 'auc'}, optionalThe metric used for model evaluation or parameter selection.\\nMandatory if resampling_method is set.\\ncv_metric{'rmse', 'mae', 'nll', 'error_rate', 'auc'}, optional (deprecated)Same as evaluation_metric.\\nWill be deprecated in future release.\\nref_metricstr or list of str, optionalSpecifies a reference metric or a list of reference metrics.\\nAny reference metric must be a valid option of evaluation_metric.\\nDefaults to ['error_rate'].\\nthread_ratiofloat, optionalThe ratio of available threads used for training.\\n0: single thread;\\n(0,1]: percentage of available threads;\\nothers : heuristically determined.\\nDefaults to -1.\\ncalculate_importancebool, optionalDetermines whether to calculate variable importance.\\nDefaults to True.\\ncalculate_cmbool, optionalDetermines whether to calculate confusion matrix.\\nDefaults to True.\\nresampling_methodstr, optionalSpecifies the resampling method for model evaluation or parameter selection.\\nValid options include: 'cv', 'stratified_cv', 'bootstrap', 'stratified_bootstrap',\\n'cv_sha', 'stratified_cv_sha', 'bootstrap_sha', 'stratified_bootstrap_sha',\\n'cv_hyperband', 'stratified_cv_hyperband', 'bootstrap_hyperband',\\n'stratified_bootstrap_hyperband'.\\nIf no value is specified for this parameter, neither model evaluation nor parameter selection is activated.\\nNo default value.\\nNote\\nResampling methods that end with 'sha' or 'hyperband' are used for\\nparameter selection only, not for model evaluation.\\nparam_search_strategy: {'grid', 'random'}, optionalThe search strategy for parameter selection.\\nMandatory if resampling_method is specified and ends with 'sha'.\\nDefaults to 'random' and cannot be changed if resampling_method is specified and\\nends with 'hyperband'; otherwise no default value, and parameter selection\\ncannot be carried out if not specified.\\nrepeat_timesint, optionalSpecifies the repeat times for resampling.\\nDefaults to 1.\\nrandom_search_timesint, optionalSpecify number of times to randomly select candidate parameters in parameter selection.\\nMandatory and valid only when param_search_strategy is set to 'random'.\\ntimeoutint, optionalSpecify maximum running time for model evaluation/parameter selection in seconds.\\nDefaults to 0, which means no timeout.\\nprogress_indicator_idstr, optionalSet an ID of progress indicator for model evaluation or parameter selection.\\nNo progress indicator will be active if no value is provided.\\nparam_rangedict or ListOfTuples, optionalSpecifies the range of parameters involved for parameter selection.\\nValid only when resampling_method and param_search_strategy are both specified.\\nIf input is list of tuples, then each tuple must be a pair, with the first being parameter name of str type, and\\nthe second being the a list of numbers with the following structure:\\n[<begin-value>, <step-size>, <end-value>].\\n<step-size> can be omitted if param_search_strategy is 'random'.\\nOtherwise, if input is dict, then the key of each element must specify a parameter name, while\\nthe value of each element specifies the range of that parameter.\\nSupported parameters for range specification: n_estimators, max_depth, learning_rate,\\nmin_sample_weight_leaf, max_w_in_split, col_subsample_split, col_subsample_tree,\\nlamb, alpha, scale_pos_w, base_score.\\nA simple example for illustration:\\n[('n_estimators', [4, 3, 10]), ('learning_rate', [0.1, 0.3, 1.0])],\\nor\\n{'n_estimators': [4, 3, 10], 'learning_rate' : [0.1, 0.3, 1.0]}.\\nNo default value.\\ncross_validation_rangelist of tuples, optional(deprecated)Same as param_range.\\nWill be deprecated in future release.\\nparam_valuesdict or ListOfTuples, optionalSpecifies the values of parameters involved for parameter selection.\\nValid only when resampling_method and param_search_strategy are both specified.\\nIf input is list of tuple, then each tuple must be a pair, with the first being parameter name of str type, and\\nthe second be a list values for that parameter.\\nOtherwise, if input is dict, then the key of each element must specify a parameter name, while\\nthe value of each element specifies list of values of that parameter.\\nSupported parameters for values specification are same as those valid for range specification, see param_range.\\nA simple example for illustration:\\n[('n_estimators', [4, 7, 10]), ('learning_rate', [0.1, 0.4, 0.7, 1.0])],\\nor\\n{'n_estimators' : [4, 7, 10], 'learning_rate' : [0.1, 0.4, 0.7, 1.0]}.\\nNo default value.\\nobj_funcstr, optionalSpecifies the objective function to optimize, with valid options listed as follows:\\n'logistic' : The objective function for logistic regression(for binary classification)\\n'hinge' : The Hinge loss function(for binary classification)\\n'softmax' : The softmax function for multi-class classification\\nDefaults to 'logistic' for binary classification, and 'softmax' for multi-class classification.\\nreplace_missingbool, optionalSpecifies whether or not to replace missing value by another value in the feature.\\nIf True,  the replacement value is the mean value for a continuous feature,\\nand the mode(i.e. most frequent value) for a categorical feature.\\nDefaults to True.\\ndefault_missing_direction{'left', 'right'}, optionalDefine the default direction where missing value will go to while tree splitting.\\nDefaults to 'right'.\\nfeature_groupingbool, optionalSpecifies whether or not to group sparse features that contains only one significant value\\nin each row.\\nDefaults to False.\\ntol_ratefloat, optionalWhile feature grouping is enabled, still merging features when there are rows containing more than one significant value.\\nThis parameter specifies the rate of such rows allowed.\\nValid only when feature_grouping is set as True.\\nDefaults to 0.0001.\\ncompressionbool, optionalIndicates whether or not the trained model should be compressed.\\nDefaults to False.\\nmax_bitsint, optionalSpecifies the maximum number of bits to quantize continuous features, which is equivalent to\\nuse \\\\(2^{max\\\\_bits}\\\\) bins.\\nValid only when compression is set as True, and must be less than 31.\\nDefaults to 12.\\nmax_bin_numint, optionalSpecifies the maximum bin number for histogram method.\\nDecreasing this number gains better performance in terms of running time at a cost of accuracy degradation.\\nOnly valid when split_method is set to 'histogram'.\\nDefaults to 256.\\nresourcestr, optionalSpecifies the resource type used in successive-halving(SHA) and hyperband algorithm for parameter selection.\\nCurrently there are two valid options: 'n_estimators' and 'data_size'.\\nMandatory and valid only when resampling_method is set as one of the following values:\\n'cv_sha', 'stratified_cv_sha', 'bootstrap_sha', 'stratified_bootstrap_sha',\\n'cv_hyperband', 'stratified_cv_hyperband', 'bootstrap_hyperband', 'stratified_bootstrap_hyperband'.\\nDefaults to 'data_size'.\\nmax_resourceint, optionalSpecifies the maximum number of estimators that should be used in SHA or Hyperband method.\\nMandatory when resource is set as 'n_estimators', and invalid if resampling_method does not take one\\nof the following values: 'cv_sha', 'stratified_cv_sha', 'bootstrap_sha', 'stratified_bootstrap_sha',\\n'cv_hyperband', 'stratified_cv_hyperband', 'bootstrap_hyperband', 'stratified_bootstrap_hyperband'.\\nreduction_ratefloat, optionalSpecifies reduction rate in SHA or Hyperband method.\\nFor each round, the available parameter candidate size will be divided by value of this parameter.\\nThus valid value for this parameter must be greater than 1.0\\nValid only when resampling_method takes one of the following values:\\n'cv_sha', 'stratified_cv_sha', 'bootstrap_sha', 'stratified_bootstrap_sha',\\n'cv_hyperband', 'stratified_cv_hyperband', 'bootstrap_hyperband', 'stratified_bootstrap_hyperband'.\\nDefaults to 3.0.\\nmin_resource_ratefloat, optionalSpecifies the minimum resource rate that should be used in SHA or Hyperband iteration.\\nValid only when resampling_method takes one of the following values:\\n'cv_sha', 'stratified_cv_sha', 'bootstrap_sha', 'stratified_bootstrap_sha',\\n'cv_hyperband', 'stratified_cv_hyperband', 'bootstrap_hyperband', 'stratified_bootstrap_hyperband'.\\nDefaults to:\\n0.0 if resource is set as 'data_size'(i.e. the default value)\\n1/max_resource if resource is set as 'n_estimators'.\\naggressive_eliminationbool, optionalSpecifies whether to apply aggressive elimination while using SHA method.\\nAggressive elimination happens when the data size and parameters size to be searched does not match\\nand there are still bunch of parameters to be searched while data size reaches its upper limits.\\nIf aggressive elimination is applied, lower bound of limit of data size will be used multiple times\\nfirst to reduce number of parameters.\\nValid only when resampling_method is set as one of the following:\\n'cv_sha', 'stratified_cv_sha', 'bootstrap_sha', 'stratified_bootstrap_sha'.\\nDefaults to False.\\nvalidation_set_ratefloat, optionalSpecifies the sampling rate of validation set for model evaluation in early stopping.\\nValid range is [0, 1).\\nNeed to specify a positive value to activate early stopping.\\nDefaults to 0.\\nstratified_validation_setbool, optionalSpecifies whether or not to apply stratified sampling for getting the validation set for early stopping.\\nValid only when validation_set_rate is specified with a positive value.\\nDefaults to True.\\ntolerant_iter_numint, optionalSpecifies the number of successive deteriorating iterations before early stopping.\\nValid only when validation_set_rate is specified with a positive value.\\nDefaults to 5.\\nfg_min_zero_ratefloat, optionalSpecifies the minimum zero rate that is used to indicate sparse columns for feature grouping.\\nValid only when feature_grouping is True.\\nDefaults to 0.5.\\nReferences\\nEarly Stop\\nFeature Grouping\\nHistogram Split\\nSuccessive Halving and Hyperband for Parameter Selection\\nExamples\\nInput dataframe for training:\\n>>> df.head(7).collect()\\n   ATT1  ATT2   ATT3  ATT4 LABEL\\n0   1.0  10.0  100.0   1.0     A\\n1   1.1  10.1  100.0   1.0     A\\n2   1.2  10.2  100.0   1.0     A\\n3   1.3  10.4  100.0   1.0     A\\n4   1.2  10.3  100.0   1.0     A\\n5   4.0  40.0  400.0   4.0     B\\n6   4.1  40.1  400.0   4.0     B\\nCreating an instance of Hybrid Gradient Boosting Classifier:\\n>>> hgbc = HybridGradientBoostingClassifier(\\n...           n_estimators = 4, split_threshold=0,\\n...           learning_rate=0.5, fold_num=5, max_depth=6,\\n...           evaluation_metric = 'error_rate', ref_metric=['auc'],\\n...           param_range=[('learning_rate',[0.1, 0.45, 1.0]),\\n...                        ('n_estimators', [4, 3, 10]),\\n...                        ('split_threshold', [0.1, 0.45, 1.0])])\\nPerforming fit() on the given dataframe:\\n>>> hgbc.fit(df, features=['ATT1', 'ATT2', 'ATT3', 'ATT4'], label='LABEL')\\n>>> hgbc.stats_.collect()\\n         STAT_NAME STAT_VALUE\\n0  ERROR_RATE_MEAN   0.133333\\n1   ERROR_RATE_VAR  0.0266666\\n2         AUC_MEAN        0.9\\nInput dataframe for predict:\\n>>> df_predict.collect()\\n   ID  ATT1  ATT2   ATT3  ATT4\\n0   1   1.0  10.0  100.0   1.0\\n1   2   1.1  10.1  100.0   1.0\\n2   3   1.2  10.2  100.0   1.0\\n3   4   1.3  10.4  100.0   1.0\\n4   5   1.2  10.3  100.0   3.0\\n5   6   4.0  40.0  400.0   3.0\\n6   7   4.1  40.1  400.0   3.0\\n7   8   4.2  40.2  400.0   3.0\\n8   9   4.3  40.4  400.0   3.0\\n9  10   4.2  40.3  400.0   3.0\\nPerforming predict() on given dataframe:\\n>>> result = hgbc.fit(df_predict, key='ID', verbose=False)\\n>>> result.collect()\\n   ID SCORE  CONFIDENCE\\n0   1     A    0.852674\\n1   2     A    0.852674\\n2   3     A    0.852674\\n3   4     A    0.852674\\n4   5     A    0.751394\\n5   6     B    0.703119\\n6   7     B    0.703119\\n7   8     B    0.703119\\n8   9     B    0.830549\\n9  10     B    0.703119\\nAttributes\\nmodel_DataFrameTrained model content.\\nfeature_importances_DataFrameThe feature importance (the higher, the more import the feature)\\nconfusion_matrix_DataFrameConfusion matrix used to evaluate the performance of classification algorithm.\\nstats_DataFrameStatistics info.\\nselected_param_DataFrameBest choice of parameter selected.\\nMethods\\ncreate_model_state([model,\\xa0function,\\xa0...])\\nCreate PAL model state.\\ndelete_model_state([state])\\nDelete PAL model state.\\nfit(data[,\\xa0key,\\xa0features,\\xa0label,\\xa0...])\\nTrain the model on input data.\\npredict(data[,\\xa0key,\\xa0features,\\xa0verbose,\\xa0...])\\nPredict labels based on the trained HGBT classifier.\\nscore(data[,\\xa0key,\\xa0features,\\xa0label,\\xa0...])\\nReturns the mean accuracy on the given test data and labels.\\nset_model_state(state)\\nSet the model state by state information.\\nfit(data, key=None, features=None, label=None, categorical_variable=None, warm_start=None)\\uf0c1\\nTrain the model on input data.\\nParameters\\ndataDataFrameTraining data.\\nkeystr, optionalName of the ID column.\\nIf key is not provided, then:\\nif data is indexed by a single column, then key defaults\\nto that index column;\\notherwise, it is assumed that data contains no ID column.\\nfeatureslist of str, optionalNames of the feature columns.\\nIf features is not provided, it defaults to all non-ID,\\nnon-label columns.\\nlabelstr, optionalName of the dependent variable.\\nDefaults to the name of the last non-ID column.\\ncategorical_variablestr or list of str, optionalIndicates INTEGER variable(s) that should be treated as categorical.\\nValid only for INTEGER variables, omitted otherwise.\\nNote\\nBy default INTEGER variables are treated as numerical.\\nwarm_startbool, optionalWhen set to True, reuse the model_ of current object to fit and add more trees to the existing model.\\nOtherwise, just fit a new model.\\nDefaults to False.\\nReturns\\nFitted object.\\npredict(data, key=None, features=None, verbose=None, thread_ratio=None, missing_replacement=None)\\uf0c1\\nPredict labels based on the trained HGBT classifier.\\nParameters\\ndataDataFrameIndependent variable values to predict for.\\nkeystr, optionalName of the ID column.\\nMandatory if data is not indexed, or the index of data contains multiple columns.\\nDefaults to the single index column of data if not provided.\\nfeatureslist of str, optionalNames of the feature columns.\\nIf features is not provided, it defaults to all non-ID columns.\\nmissing_replacementstr, optionalThe missing replacement strategy:\\n'feature_marginalized': marginalise each missing feature out               independently.\\n'instance_marginalized': marginalise all missing features               in an instance as a whole corr\\nverbosebool, optionalIf True, output all classes and the corresponding confidences             for each data point.\\nDefaults to False.\\nReturns\\nDataFrameDataFrame of score and confidence, structured as follows:\\nID column, with same name and type as data's ID column.\\nSCORE, type DOUBLE, representing the predicted classes/values.\\nCONFIDENCE, type DOUBLE, representing the confidence of\\na class label assignment.\\nscore(data, key=None, features=None, label=None, missing_replacement=None)\\uf0c1\\nReturns the mean accuracy on the given test data and labels.\\nParameters\\ndataDataFrameData on which to assess model performance.\\nkeystr, optionalName of the ID column.\\nMandatory if data is not indexed, or the index of data contains multiple columns.\\nDefaults to the single index column of data if not provided.\\nfeatureslist of str, optionalNames of the feature columns.\\nIf features is not provided, it defaults to all non-ID,\\nnon-label columns.\\nlabelstr, optionalName of the dependent variable.\\nDefaults to the name of the last non-ID column.\\nmissing_replacementstr, optionalThe missing replacement strategy:\\n'feature_marginalized': marginalise each missing feature out\\nindependently.\\n'instance_marginalized': marginalise all missing features\\nin an instance as a whole corresponding to each category.\\nDefaults to 'feature_marginalized'.\\nReturns\\nfloatMean accuracy on the given test data and labels.\\ncreate_model_state(model=None, function=None, pal_funcname='PAL_HGBT', state_description=None, force=False)\\uf0c1\\nCreate PAL model state.\\nParameters\\nmodelDataFrame, optionalSpecify the model for AFL state.\\nDefaults to self.model_.\\nfunctionstr, optionalSpecify the function in the unified API.\\nA placeholder parameter, not effective for HGBT.\\npal_funcnameint or str, optionalPAL function name.\\nDefaults to 'PAL_HGBT'.\\nstate_descriptionstr, optionalDescription of the state as model container.\\nDefaults to None.\\nforcebool, optionalIf True it will delete the existing state.\\nDefaults to False.\\ndelete_model_state(state=None)\\uf0c1\\nDelete PAL model state.\\nParameters\\nstateDataFrame, optionalSpecified the state.\\nDefaults to self.state.\\nproperty fit_hdbprocedure\\uf0c1\\nReturns the generated hdbprocedure for fit.\\nproperty predict_hdbprocedure\\uf0c1\\nReturns the generated hdbprocedure for predict.\\nset_model_state(state)\\uf0c1\\nSet the model state by state information.\\nParameters\\nstate: DataFrame or dictIf state is DataFrame, it has the following structure:\\nNAME: VARCHAR(100), it mush have STATE_ID, HINT, HOST and PORT.\\nVALUE: VARCHAR(1000), the values according to NAME.\\nIf state is dict, the key must have STATE_ID, HINT, HOST and PORT.\\nInherited Methods from PALBase\\uf0c1\\nBesides those methods mentioned above, the HybridGradientBoostingClassifier class also inherits methods from PALBase class, please refer to PAL Base for more details.\\n¬© Copyright 2023, SAP.\\n  Built with Sphinx using a\\n\", doc_id='662cbc7c-cfa3-4df3-8fc3-641e017bd733', embedding=None, doc_hash='a839331e16bfbcf907b56a65f56104d7d47e79011775760b4a2300cb94dd4892', extra_info=None),\n",
       "  Document(text='\\nNotebook\\nIn\\xa0[1]:\\n# Created by: Sergiu Iatco | 2022.12.16 | Romania\\n# https://people.sap.com/sergiu.iatco\\n# https://ro.linkedin.com/in/sergiuiatco\\n# My first SAP HANA ML challendge and project\\n# My focus is on relaible best STATISTICS\\n# When SUPPORT No <> SUPPORT Yes I can rely for only on RECALL Yes and RECALL No.\\n# When SUPPORT No = SUPPORT Yes I can rely on all statistics.\\n# Collect statistics from all models, compare and select.\\n# Provide top of models by features to use in different scenarios.\\n# Provide voting of models and contribution of features to average stochastic results.\\n# Shake hand with developer and integrate in applications.\\nSAP HANA ML Challenge¬∂‚ÄúI quit!‚Äù ‚Äì How to predict employee churn | SAP HANA Cloud Machine Learning Challenge\\nhttps://blogs.sap.com/2022/11/28/i-quit-how-to-predict-employee-churn-sap-hana-cloud-machine-learning-challenge/\\nDocumentation¬∂\\nSAP HANA Python Client API for Machine Learning Algorithms:\\nhttps://pypi.org/project/hana-ml/\\nSAP HANA Automated Predictive Library (APL):\\nhttps://help.sap.com/doc/1d0ebfe5e8dd44d09606814d83308d4b/2.0.06/en-US/hana_ml.algorithms.apl.html\\nSAP HANA Predictive Analysis Library (PAL):\\nhttps://help.sap.com/doc/1d0ebfe5e8dd44d09606814d83308d4b/2.0.06/en-US/hana_ml.algorithms.pal.html\\nPackage Dependencies: \\nhttps://help.sap.com/doc/1d0ebfe5e8dd44d09606814d83308d4b/2.0.06/en-US/Installation.html\\nExamples:\\nhttps://github.com/SAP-samples/hana-ml-samples/tree/main/Python-API/pal/notebooks\\nhttps://github.com/SAP-samples/hana-ml-samples/tree/main/Python-API/usecase-examples/sapcommunity-hanaml-challenge\\nSAP HANA ML Library¬∂I will be using the \\'SAP HANA Python Client API for Machine Learning Algorithm\\'. See the notebook \"10 Connectivity Check\" that you are using the approviate version of that package.\\nIn\\xa0[2]:\\n# !pip install hana-ml --upgrade\\nIn\\xa0[3]:\\nimport hana_ml\\nprint(hana_ml.__version__)\\n2.14.22102800\\nLoad the CSV file into a Python object (Pandas DataFrame)¬∂Trainning data - Emp_Churn_Train.csv¬∂\\nIn\\xa0[4]:\\n# INITIAL DATA\\nimport pandas as pd\\ndf_data = pd.read_csv(r\\'Emp_Churn_Train.csv\\', sep = \\',\\')\\ndf_data.head(5)\\nOut[4]:\\nEMPLOYEE_ID\\nAGE\\nAGE_GROUP10\\nAGE_GROUP5\\nGENERATION\\nCRITICAL_JOB_ROLE\\nRISK_OF_LOSS\\nIMPACT_OF_LOSS\\nFUTURE_LEADER\\nGENDER\\n...\\nCURRENT_COUNTRY\\nCURCOUNTRYLAT\\nCURCOUNTRYLON\\nPROMOTION_WITHIN_LAST_3_YEARS\\nCHANGED_POSITION_WITHIN_LAST_2_YEARS\\nCHANGE_IN_PERFORMANCE_RATING\\nFUNCTIONALAREACHANGETYPE\\nJOBLEVELCHANGETYPE\\nHEADS\\nFLIGHT_RISK\\n0\\n10032\\n33\\n(25-35]\\n(30-35]\\nGeneration Y\\nCritical\\nHigh\\nHigh\\nNo Future Leader\\nFemale\\n...\\nUSA\\n39.78373\\n-100.445882\\nNo Promotion\\nNo Change\\n0 - Not available\\nNo change\\nNo change\\n1\\nNo\\n1\\n10033\\n43\\n(35-45]\\n(40-45]\\nGeneration X\\nCritical\\nLow\\nHigh\\nNo Future Leader\\nFemale\\n...\\nGermany\\n51.08342\\n10.423447\\nNo Promotion\\nNo Change\\n0 - Not available\\nNo change\\nNo change\\n1\\nNo\\n2\\n10034\\n33\\n(25-35]\\n(30-35]\\nGeneration Y\\nCritical\\nMedium\\nHigh\\nNo Future Leader\\nFemale\\n...\\nUSA\\n39.78373\\n-100.445882\\nNo Promotion\\nNo Change\\n0 - Not available\\nExternal Hire\\nExternal Hire\\n1\\nNo\\n3\\n10035\\n33\\n(25-35]\\n(30-35]\\nGeneration Y\\nCritical\\nHigh\\nHigh\\nNo Future Leader\\nMale\\n...\\nUSA\\n39.78373\\n-100.445882\\nNo Promotion\\nNo Change\\n0 - Not available\\nNo change\\nNo change\\n1\\nNo\\n4\\n10036\\n33\\n(25-35]\\n(30-35]\\nGeneration Y\\nCritical\\nLow\\nLow\\nNo Future Leader\\nMale\\n...\\nUSA\\n39.78373\\n-100.445882\\nNo Promotion\\nNo Change\\n0 - Not available\\nNo change\\nNo change\\n1\\nYes\\n5 rows √ó 43 columns\\nIn\\xa0[5]:\\n# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.duplicated.html\\ndf_data[df_data.duplicated()]\\nOut[5]:\\nEMPLOYEE_ID\\nAGE\\nAGE_GROUP10\\nAGE_GROUP5\\nGENERATION\\nCRITICAL_JOB_ROLE\\nRISK_OF_LOSS\\nIMPACT_OF_LOSS\\nFUTURE_LEADER\\nGENDER\\n...\\nCURRENT_COUNTRY\\nCURCOUNTRYLAT\\nCURCOUNTRYLON\\nPROMOTION_WITHIN_LAST_3_YEARS\\nCHANGED_POSITION_WITHIN_LAST_2_YEARS\\nCHANGE_IN_PERFORMANCE_RATING\\nFUNCTIONALAREACHANGETYPE\\nJOBLEVELCHANGETYPE\\nHEADS\\nFLIGHT_RISK\\n0 rows √ó 43 columns\\nIn\\xa0[6]:\\ndf_data[[\\'HEADS\\']].value_counts()\\nOut[6]:\\nHEADS\\n1        19115\\ndtype: int64\\nIn\\xa0[7]:\\ndf_data.shape # rows and columns\\nOut[7]:\\n(19115, 43)\\nIn\\xa0[8]:\\ndf_data.dtypes\\nOut[8]:\\nEMPLOYEE_ID                               int64\\nAGE                                       int64\\nAGE_GROUP10                              object\\nAGE_GROUP5                               object\\nGENERATION                               object\\nCRITICAL_JOB_ROLE                        object\\nRISK_OF_LOSS                             object\\nIMPACT_OF_LOSS                           object\\nFUTURE_LEADER                            object\\nGENDER                                   object\\nMGR_EMP                                  object\\nMINORITY                                 object\\nTENURE_MONTHS                             int64\\nTENURE_INTERVAL_YEARS                    object\\nTENURE_INTERVALL_DESC                    object\\nSALARY                                    int64\\nEMPLOYMENT_TYPE                          object\\nEMPLOYMENT_TYPE_2                        object\\nHIGH_POTENTIAL                           object\\nPREVIOUS_FUNCTIONAL_AREA                 object\\nPREVIOUS_JOB_LEVEL                       object\\nPREVIOUS_CAREER_PATH                     object\\nPREVIOUS_PERFORMANCE_RATING              object\\nPREVIOUS_COUNTRY                         object\\nPREVCOUNTRYLAT                          float64\\nPREVCOUNTRYLON                          float64\\nPREVIOUS_REGION                          object\\nTIMEINPREVPOSITIONMONTH                   int64\\nCURRENT_FUNCTIONAL_AREA                  object\\nCURRENT_JOB_LEVEL                        object\\nCURRENT_CAREER_PATH                      object\\nCURRENT_PERFORMANCE_RATING               object\\nCURRENT_REGION                           object\\nCURRENT_COUNTRY                          object\\nCURCOUNTRYLAT                           float64\\nCURCOUNTRYLON                           float64\\nPROMOTION_WITHIN_LAST_3_YEARS            object\\nCHANGED_POSITION_WITHIN_LAST_2_YEARS     object\\nCHANGE_IN_PERFORMANCE_RATING             object\\nFUNCTIONALAREACHANGETYPE                 object\\nJOBLEVELCHANGETYPE                       object\\nHEADS                                     int64\\nFLIGHT_RISK                              object\\ndtype: object\\nTransform the data¬∂Before uploading the data to SAP HANA Cloud, carry out a few transformations. Turn the column headers into upper case.\\nIn\\xa0[9]:\\ndf_data.columns = map(str.upper, df_data.columns)\\ndf_data.head(5)\\nOut[9]:\\nEMPLOYEE_ID\\nAGE\\nAGE_GROUP10\\nAGE_GROUP5\\nGENERATION\\nCRITICAL_JOB_ROLE\\nRISK_OF_LOSS\\nIMPACT_OF_LOSS\\nFUTURE_LEADER\\nGENDER\\n...\\nCURRENT_COUNTRY\\nCURCOUNTRYLAT\\nCURCOUNTRYLON\\nPROMOTION_WITHIN_LAST_3_YEARS\\nCHANGED_POSITION_WITHIN_LAST_2_YEARS\\nCHANGE_IN_PERFORMANCE_RATING\\nFUNCTIONALAREACHANGETYPE\\nJOBLEVELCHANGETYPE\\nHEADS\\nFLIGHT_RISK\\n0\\n10032\\n33\\n(25-35]\\n(30-35]\\nGeneration Y\\nCritical\\nHigh\\nHigh\\nNo Future Leader\\nFemale\\n...\\nUSA\\n39.78373\\n-100.445882\\nNo Promotion\\nNo Change\\n0 - Not available\\nNo change\\nNo change\\n1\\nNo\\n1\\n10033\\n43\\n(35-45]\\n(40-45]\\nGeneration X\\nCritical\\nLow\\nHigh\\nNo Future Leader\\nFemale\\n...\\nGermany\\n51.08342\\n10.423447\\nNo Promotion\\nNo Change\\n0 - Not available\\nNo change\\nNo change\\n1\\nNo\\n2\\n10034\\n33\\n(25-35]\\n(30-35]\\nGeneration Y\\nCritical\\nMedium\\nHigh\\nNo Future Leader\\nFemale\\n...\\nUSA\\n39.78373\\n-100.445882\\nNo Promotion\\nNo Change\\n0 - Not available\\nExternal Hire\\nExternal Hire\\n1\\nNo\\n3\\n10035\\n33\\n(25-35]\\n(30-35]\\nGeneration Y\\nCritical\\nHigh\\nHigh\\nNo Future Leader\\nMale\\n...\\nUSA\\n39.78373\\n-100.445882\\nNo Promotion\\nNo Change\\n0 - Not available\\nNo change\\nNo change\\n1\\nNo\\n4\\n10036\\n33\\n(25-35]\\n(30-35]\\nGeneration Y\\nCritical\\nLow\\nLow\\nNo Future Leader\\nMale\\n...\\nUSA\\n39.78373\\n-100.445882\\nNo Promotion\\nNo Change\\n0 - Not available\\nNo change\\nNo change\\n1\\nYes\\n5 rows √ó 43 columns\\nUpload the data to SAP HANA¬∂I am happy with the data, so upload it to SAP HANA. Establish a connection with the hana_ml wrapper‚Ä¶\\nIn\\xa0[10]:\\ndf_data.isnull().sum()\\nOut[10]:\\nEMPLOYEE_ID                                0\\nAGE                                        0\\nAGE_GROUP10                                0\\nAGE_GROUP5                                 0\\nGENERATION                                 0\\nCRITICAL_JOB_ROLE                          0\\nRISK_OF_LOSS                               0\\nIMPACT_OF_LOSS                             0\\nFUTURE_LEADER                              0\\nGENDER                                     0\\nMGR_EMP                                    0\\nMINORITY                                   0\\nTENURE_MONTHS                              0\\nTENURE_INTERVAL_YEARS                      0\\nTENURE_INTERVALL_DESC                      0\\nSALARY                                     0\\nEMPLOYMENT_TYPE                            0\\nEMPLOYMENT_TYPE_2                          0\\nHIGH_POTENTIAL                             0\\nPREVIOUS_FUNCTIONAL_AREA                6521\\nPREVIOUS_JOB_LEVEL                      6521\\nPREVIOUS_CAREER_PATH                    6521\\nPREVIOUS_PERFORMANCE_RATING             6521\\nPREVIOUS_COUNTRY                        6521\\nPREVCOUNTRYLAT                          6521\\nPREVCOUNTRYLON                          6521\\nPREVIOUS_REGION                         6521\\nTIMEINPREVPOSITIONMONTH                    0\\nCURRENT_FUNCTIONAL_AREA                    0\\nCURRENT_JOB_LEVEL                          0\\nCURRENT_CAREER_PATH                        0\\nCURRENT_PERFORMANCE_RATING                 0\\nCURRENT_REGION                             0\\nCURRENT_COUNTRY                            0\\nCURCOUNTRYLAT                              0\\nCURCOUNTRYLON                              0\\nPROMOTION_WITHIN_LAST_3_YEARS              0\\nCHANGED_POSITION_WITHIN_LAST_2_YEARS       0\\nCHANGE_IN_PERFORMANCE_RATING               0\\nFUNCTIONALAREACHANGETYPE                   0\\nJOBLEVELCHANGETYPE                         0\\nHEADS                                      0\\nFLIGHT_RISK                                0\\ndtype: int64\\nIn\\xa0[11]:\\n# import numpy as np\\n# df_data = df_data.replace(to_replace = np.nan, value = \\'999\\')\\n# perform with hana\\nIn\\xa0[12]:\\nhana_address = \\'********-****-****-****-************.hna1.prod-eu10.hanacloud.ondemand.com\\' \\nhana_port = 443 # Adjust if needed / as advised\\nhana_user = \\'******\\' \\nhana_password = \\'****** \\nhana_encrypt = \\'true\\' # Adjust if needed / as advised\\nimport hana_ml.dataframe as dataframe\\n# Instantiate connection object\\nconn = dataframe.ConnectionContext(address = hana_address,\\n                                   port = 443, \\n                                   user = hana_user, \\n                                   password = hana_password, \\n                                   encrypt = hana_encrypt,\\n                                   sslValidateCertificate = \\'false\\' \\n                                  )\\n# Control connection\\nconn.connection.isconnected()\\nOut[12]:\\nTrue\\n...and upload the Pandas DataFrame into a table called after your Username.\\nIn\\xa0[13]:\\nhana_tab = \"CHURN\"\\ndf_remote = dataframe.create_dataframe_from_pandas(connection_context = conn, \\n                                                   pandas_df = df_data, \\n                                                   table_name = hana_tab,\\n                                                   force = True,\\n                                                   replace = False)\\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.15s/it]\\nClose the connection.\\nIn\\xa0[14]:\\n# Creating a dataframe using SAP HANA Cloud table specification\\ndf_remote = conn.table(hana_tab)\\ndf_remote.select_statement\\nOut[14]:\\n\\'SELECT * FROM \"CHURN\"\\'\\nIn\\xa0[15]:\\ndf_remote.hasna()\\nOut[15]:\\nTrue\\nIn\\xa0[16]:\\ndf_remote.head(3).collect()\\nOut[16]:\\nEMPLOYEE_ID\\nAGE\\nAGE_GROUP10\\nAGE_GROUP5\\nGENERATION\\nCRITICAL_JOB_ROLE\\nRISK_OF_LOSS\\nIMPACT_OF_LOSS\\nFUTURE_LEADER\\nGENDER\\n...\\nCURRENT_COUNTRY\\nCURCOUNTRYLAT\\nCURCOUNTRYLON\\nPROMOTION_WITHIN_LAST_3_YEARS\\nCHANGED_POSITION_WITHIN_LAST_2_YEARS\\nCHANGE_IN_PERFORMANCE_RATING\\nFUNCTIONALAREACHANGETYPE\\nJOBLEVELCHANGETYPE\\nHEADS\\nFLIGHT_RISK\\n0\\n10032\\n33\\n(25-35]\\n(30-35]\\nGeneration Y\\nCritical\\nHigh\\nHigh\\nNo Future Leader\\nFemale\\n...\\nUSA\\n39.78373\\n-100.445882\\nNo Promotion\\nNo Change\\n0 - Not available\\nNo change\\nNo change\\n1\\nNo\\n1\\n10033\\n43\\n(35-45]\\n(40-45]\\nGeneration X\\nCritical\\nLow\\nHigh\\nNo Future Leader\\nFemale\\n...\\nGermany\\n51.08342\\n10.423447\\nNo Promotion\\nNo Change\\n0 - Not available\\nNo change\\nNo change\\n1\\nNo\\n2\\n10034\\n33\\n(25-35]\\n(30-35]\\nGeneration Y\\nCritical\\nMedium\\nHigh\\nNo Future Leader\\nFemale\\n...\\nUSA\\n39.78373\\n-100.445882\\nNo Promotion\\nNo Change\\n0 - Not available\\nExternal Hire\\nExternal Hire\\n1\\nNo\\n3 rows √ó 43 columns\\nIn\\xa0[17]:\\n# ADDITIONAL DATA COLLECTED\\nimport pandas as pd\\ndf_data_add = pd.read_csv(r\\'Emp_Churn_Additional_Features.csv\\', sep = \\',\\')\\ndf_data_add.head(3)\\nOut[17]:\\nID\\nLINKEDIN\\nHRTRAINING\\nSICKDAYS\\n0\\n10032\\nYes\\nYes\\n10\\n1\\n10033\\nNo\\nNo\\n6\\n2\\n10034\\nYes\\nNo\\n5\\nIn\\xa0[18]:\\ndf_data_add.isnull().sum() # check null data\\nOut[18]:\\nID            0\\nLINKEDIN      0\\nHRTRAINING    0\\nSICKDAYS      0\\ndtype: int64\\nIn\\xa0[19]:\\ndf_data_add.shape, df_remote.shape # check rows and columns\\nOut[19]:\\n((19115, 4), [19115, 43])\\nIn\\xa0[20]:\\nhana_tab_add = \"CHURN_ADD\"\\ndf_remote_add = dataframe.create_dataframe_from_pandas(connection_context = conn, \\n                                                   pandas_df = df_data_add, \\n                                                   table_name = hana_tab_add,\\n                                                   force = True,\\n                                                   replace = False)\\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.92it/s]\\nIn\\xa0[21]:\\n# Creating a dataframe using SAP HANA Cloud table specification\\ndf_remote_add = conn.table(hana_tab_add)\\ndf_remote_add.select_statement\\nOut[21]:\\n\\'SELECT * FROM \"CHURN_ADD\"\\'\\nIn\\xa0[22]:\\ndf_remote.head(3).collect()\\nOut[22]:\\nEMPLOYEE_ID\\nAGE\\nAGE_GROUP10\\nAGE_GROUP5\\nGENERATION\\nCRITICAL_JOB_ROLE\\nRISK_OF_LOSS\\nIMPACT_OF_LOSS\\nFUTURE_LEADER\\nGENDER\\n...\\nCURRENT_COUNTRY\\nCURCOUNTRYLAT\\nCURCOUNTRYLON\\nPROMOTION_WITHIN_LAST_3_YEARS\\nCHANGED_POSITION_WITHIN_LAST_2_YEARS\\nCHANGE_IN_PERFORMANCE_RATING\\nFUNCTIONALAREACHANGETYPE\\nJOBLEVELCHANGETYPE\\nHEADS\\nFLIGHT_RISK\\n0\\n10032\\n33\\n(25-35]\\n(30-35]\\nGeneration Y\\nCritical\\nHigh\\nHigh\\nNo Future Leader\\nFemale\\n...\\nUSA\\n39.78373\\n-100.445882\\nNo Promotion\\nNo Change\\n0 - Not available\\nNo change\\nNo change\\n1\\nNo\\n1\\n10033\\n43\\n(35-45]\\n(40-45]\\nGeneration X\\nCritical\\nLow\\nHigh\\nNo Future Leader\\nFemale\\n...\\nGermany\\n51.08342\\n10.423447\\nNo Promotion\\nNo Change\\n0 - Not available\\nNo change\\nNo change\\n1\\nNo\\n2\\n10034\\n33\\n(25-35]\\n(30-35]\\nGeneration Y\\nCritical\\nMedium\\nHigh\\nNo Future Leader\\nFemale\\n...\\nUSA\\n39.78373\\n-100.445882\\nNo Promotion\\nNo Change\\n0 - Not available\\nExternal Hire\\nExternal Hire\\n1\\nNo\\n3 rows √ó 43 columns\\nIn\\xa0[23]:\\ndf_remote_add.shape # hana df rows and columns\\nOut[23]:\\n[19115, 4]\\nIn\\xa0[24]:\\ndf_remote_add.head(3).collect()\\nOut[24]:\\nID\\nLINKEDIN\\nHRTRAINING\\nSICKDAYS\\n0\\n10032\\nYes\\nYes\\n10\\n1\\n10033\\nNo\\nNo\\n6\\n2\\n10034\\nYes\\nNo\\n5\\nIn\\xa0[25]:\\ndf_remote_add.hasna()\\nOut[25]:\\nFalse\\nIn\\xa0[26]:\\n# EDA features distributions\\ncol_exc = [\\'ID\\']\\nfor col in df_remote_add.columns:\\n    if col not in col_exc:\\n        print(df_remote_add.agg([(\\'count\\', col, \\'Count\\')], group_by=col).collect())\\n  LINKEDIN  Count\\n0      Yes   9627\\n1       No   9488\\n  HRTRAINING  Count\\n0        Yes   6859\\n1         No  12256\\n    SICKDAYS  Count\\n0          8   1947\\n1         16    431\\n2          7   1476\\n3          2     77\\n4         11   2278\\n5         17    262\\n6         21     16\\n7         15    774\\n8         24      1\\n9         26      2\\n10        23      6\\n11        18    154\\n12        27      1\\n13        19     96\\n14         0      7\\n15         4    304\\n16         1     34\\n17         9   2291\\n18         6    989\\n19        10   2446\\n20         3    164\\n21        22     15\\n22        20     32\\n23         5    635\\n24        13   1529\\n25        12   1935\\n26        25      1\\n27        14   1212\\nIn\\xa0[27]:\\n# Replace ID with EMPLOYEE_ID\\ndf_remote_add = df_remote_add.rename_columns({\\'ID\\':\\'EMPLOYEE_ID\\'})\\ndf_remote_add.head(3).collect()\\nOut[27]:\\nEMPLOYEE_ID\\nLINKEDIN\\nHRTRAINING\\nSICKDAYS\\n0\\n10032\\nYes\\nYes\\n10\\n1\\n10033\\nNo\\nNo\\n6\\n2\\n10034\\nYes\\nNo\\n5\\nIn\\xa0[28]:\\ndf_remote.distinct().shape, df_remote.shape #check row duplicates\\nOut[28]:\\n([19115, 43], [19115, 43])\\nIn\\xa0[29]:\\n# EDA features distributions\\ncol_exc = [\\'EMPLOYEE_ID\\']\\nfor col in df_remote.columns:\\n    if col not in col_exc:\\n        print(df_remote.agg([(\\'count\\', col, \\'Count\\')], group_by=col).collect())\\n    AGE  Count\\n0    32    561\\n1    46    497\\n2    48    562\\n3    36    481\\n4    45    646\\n5    33    674\\n6    28    489\\n7    42    652\\n8    53    253\\n9    21    289\\n10   44    685\\n11   24    249\\n12   31    497\\n13   26    427\\n14   23    289\\n15   30    437\\n16   55    274\\n17   29    436\\n18   18     69\\n19   41    799\\n20   39    830\\n21   37    734\\n22   40    634\\n23   38    875\\n24   34    643\\n25   43    750\\n26   49    651\\n27   51    418\\n28   50    746\\n29   56    275\\n30   22    222\\n31   47    734\\n32   57    131\\n33   20    155\\n34   54    324\\n35   25    264\\n36   52    391\\n37   19     80\\n38   27    367\\n39   35    625\\n  AGE_GROUP10  Count\\n0     (45-55]   4850\\n1     (25-35]   5156\\n2     (35-45]   7086\\n3      (0-25]   1617\\n4       (55+]    406\\n  AGE_GROUP5  Count\\n0     (0-20]    304\\n1    (45-50]   3190\\n2    (40-45]   3532\\n3    (35-40]   3554\\n4    (25-30]   2156\\n5      (55+]    406\\n6    (30-35]   3000\\n7    (50-55]   1660\\n8    (20-25]   1313\\n     GENERATION  Count\\n0  Generation Z   1353\\n1  Generation Y   7510\\n2  Generation X   9248\\n3   Late Boomer   1004\\n  CRITICAL_JOB_ROLE  Count\\n0          Critical  13651\\n1      Non-Critical   5464\\n  RISK_OF_LOSS  Count\\n0          Low   6395\\n1         High   6473\\n2  Unallocated   2569\\n3       Medium   3678\\n  IMPACT_OF_LOSS  Count\\n0            Low   1699\\n1           High   9223\\n2    Unallocated   2602\\n3         Medium   5591\\n      FUTURE_LEADER  Count\\n0  No Future Leader  14743\\n1     Future Leader   4372\\n   GENDER  Count\\n0    Male   8699\\n1  Female  10416\\n  MGR_EMP  Count\\n0  No Mgr  17027\\n1     Mgr   2088\\n       MINORITY  Count\\n0      Minority   1539\\n1  Non-Minority  17576\\n     TENURE_MONTHS  Count\\n0               67     72\\n1              117     73\\n2              189     48\\n3               81     85\\n4               23    101\\n..             ...    ...\\n467            454      2\\n468            444      2\\n469            458      1\\n470            469      1\\n471            420      1\\n[472 rows x 2 columns]\\n  TENURE_INTERVAL_YEARS  Count\\n0                 (20+]   3432\\n1             (10 - 20]   6093\\n2            ( 0 - 1/2]    598\\n3             ( 5 - 10]   4376\\n4            ( 1/2 - 2]   1587\\n5              ( 2 - 5]   3029\\n  TENURE_INTERVALL_DESC  Count\\n0            2 - newbie   1587\\n1         1 - probation    598\\n2            4 - senior   4376\\n3             5 - loyal   6093\\n4       3 - established   3029\\n5        6 - very loyal   3432\\n    SALARY  Count\\n0   120000   2010\\n1    30000   1324\\n2    65000   1638\\n3    40000   3777\\n4    50000   3328\\n5    20000     46\\n6    10000      3\\n7    29000      3\\n8    63000   3356\\n9     4900      6\\n10   23000    145\\n11   42000     18\\n12   85000   1431\\n13   70000   2030\\n  EMPLOYMENT_TYPE  Count\\n0       Full-Time  18181\\n1       Part-Time    931\\n2       Temporary      3\\n  EMPLOYMENT_TYPE_2  Count\\n0           Regular  16835\\n1         Temporary   2280\\n  HIGH_POTENTIAL  Count\\n0       High Pot   1975\\n1    No High Pot  17140\\n  PREVIOUS_FUNCTIONAL_AREA  Count\\n0                    Sales   3285\\n1                  Support   2054\\n2                     None      0\\n3                Marketing    971\\n4                      R&D   3955\\n5                  Service   2329\\n                   PREVIOUS_JOB_LEVEL  Count\\n0                       6 - Executive      8\\n1                          3 - Senior   3750\\n2  5 - Senior Expert / Senior Manager    841\\n3                                None      0\\n4                4 - Expert / Manager   3575\\n5                      2 - Specialist   3906\\n6                       1 - Associate    514\\n   PREVIOUS_CAREER_PATH  Count\\n0            Functional  10600\\n1  Project / Management   1994\\n2                  None      0\\n             PREVIOUS_PERFORMANCE_RATING  Count\\n0       2 - Partially Meets Expectations    394\\n1                 3 - Meets Expectations   5358\\n2                                   None      0\\n3               4 - Exceeds Expectations   4929\\n4  5 - Consistently Exceeds Expectations   1875\\n5         1 - Does Not Meet Expectations     38\\n   PREVIOUS_COUNTRY  Count\\n0             Japan    349\\n1           Croatia     10\\n2           Germany   3901\\n3             India   1392\\n4       Puerto Rico     17\\n5          Portugal     53\\n6             Spain    156\\n7         Australia    232\\n8           Estonia     11\\n9           Austria     84\\n10        Lithuania     12\\n11            China    577\\n12          Nigeria     21\\n13     South Africa     95\\n14        Indonesia     19\\n15          Belgium    102\\n16           Sweden     33\\n17           Poland     26\\n18           Russia     94\\n19           Canada    480\\n20          Finland     29\\n21          Ireland    229\\n22         Malaysia     31\\n23           Greece     21\\n24          Romania     33\\n25           Brazil    125\\n26         Bulgaria     73\\n27          Denmark     33\\n28      Netherlands    105\\n29            Chile     28\\n30           Serbia     22\\n31             None      0\\n32               UK    256\\n33      South Korea     77\\n34              USA   2223\\n35        Singapore    296\\n36   Czech Republic     39\\n37            Italy    121\\n38      Switzerland    104\\n39        Venezuela     28\\n40           Israel    175\\n41           Mexico     82\\n42           Turkey     27\\n43      New Zealand     18\\n44           France    385\\n45           Norway     24\\n46     Saudi Arabia     62\\n47          Ukraine     11\\n48             Peru     24\\n49      Philippines     18\\n50         Thailand     33\\n51        Argentina     59\\n52         Colombia     32\\n53          Hungary     40\\n54         Slovenia     15\\n55         Slovakia     24\\n56           Taiwan     28\\n    PREVCOUNTRYLAT  Count\\n0        59.674971     33\\n1        51.083420   3901\\n2         9.600036     21\\n3        52.097718     26\\n4        46.798562    104\\n5        22.351115   1392\\n6        49.487197     11\\n7        47.200034     84\\n8        38.959759     27\\n9       -24.776109    232\\n10       14.897192     33\\n11       25.624262     62\\n12       45.564344     10\\n13       61.066692    480\\n14       60.500021     24\\n15       42.607398     73\\n16       49.816700     39\\n17        2.889443     32\\n18       55.350000     12\\n19       48.741152     24\\n20       19.432601     82\\n21       52.237989    105\\n22       18.221417     17\\n23       46.603354    385\\n24       47.181759     40\\n25       52.865196    229\\n26       36.574844    349\\n27       63.246778     29\\n28       39.783730   2245\\n29       40.033265     53\\n30       54.702355    256\\n31       30.876027    175\\n32       -4.799336     19\\n33       45.813311     15\\n34       50.640735    102\\n35       64.686314     94\\n36        2.392376     31\\n37      -10.333333    125\\n38      -41.500083     18\\n39        8.001871     28\\n40       35.000074    577\\n41      -28.816624     95\\n42       42.638426    121\\n43       16.039765     28\\n44       55.670249     33\\n45       -6.869970     24\\n46        1.290475    296\\n47       12.750349     18\\n48       37.532600     77\\n49       45.985213     33\\n50       40.002803    156\\n51       38.995368     21\\n52       58.752378     11\\n53      -34.996496     59\\n54             NaN      0\\n55      -31.761336     28\\n    PREVCOUNTRYLON  Count\\n0         9.099972     24\\n1        21.987713     21\\n2       111.780899     28\\n3      -107.991707    480\\n4        12.674297    121\\n5         8.231974    104\\n6       100.832730     33\\n7       112.847194     31\\n8        -7.889626     53\\n9       -66.110932     28\\n10      -75.045851     24\\n11      172.834408     18\\n12      134.755000    232\\n13       23.750000     12\\n14        4.666960    102\\n15      127.024612     77\\n16     -100.445882   2245\\n17       10.333328     33\\n18       19.025816     26\\n19       31.271832     11\\n20       35.001520    175\\n21       19.452865     24\\n22       25.920916     29\\n23       17.011895     10\\n24       15.474954     39\\n25       -4.003104    156\\n26      122.731210     18\\n27       13.199959     84\\n28      -64.967282     59\\n29      -73.783892     32\\n30             NaN      0\\n31      -99.133342     82\\n32        5.534607    105\\n33      -71.318770     28\\n34       -7.979460    229\\n35       10.423447   3901\\n36      -66.413282     17\\n37       78.667743   1392\\n38        1.888334    385\\n39      114.563203     19\\n40       25.331908     11\\n41       42.352833     62\\n42       34.924965     27\\n43        7.999972     21\\n44       -3.276575    256\\n45      103.852036    296\\n46       24.991639     95\\n47       24.685923     33\\n48      139.239418    349\\n49       19.506094     40\\n50       14.480837     15\\n51      104.999927    577\\n52      -53.200000    125\\n53       14.520858     33\\n54       25.485662     73\\n55       97.745306     94\\n  PREVIOUS_REGION  Count\\n0        Americas   3098\\n1             APJ   3070\\n2            EMEA   6426\\n3            None      0\\n    TIMEINPREVPOSITIONMONTH  Count\\n0                        32     16\\n1                         8    264\\n2                       240      1\\n3                        16     23\\n4                         7    973\\n5                        48     54\\n6                         2   1115\\n7                        36    928\\n8                        33    635\\n9                        28     37\\n10                       42     24\\n11                       53     55\\n12                       72     10\\n13                       17     37\\n14                       21     19\\n15                       55   1000\\n16                       24    884\\n17                       31     59\\n18                       26    245\\n19                       23     20\\n20                       30     18\\n21                       15    354\\n22                       29    305\\n23                       44      1\\n24                       18    675\\n25                       41     22\\n26                       39     37\\n27                       37    286\\n28                       40    261\\n29                       38     21\\n30                        0   2367\\n31                       34     16\\n32                        4    993\\n33                        1    285\\n34                        9    469\\n35                        6   1119\\n36                       43      9\\n37                      396      1\\n38                       51    375\\n39                      324      1\\n40                       50     48\\n41                       56     27\\n42                      180      1\\n43                        3   1028\\n44                       22     68\\n45                       57     43\\n46                       20     55\\n47                        5   1113\\n48                       12    362\\n49                       54     33\\n50                       25    227\\n51                       84      3\\n52                       52   1845\\n53                       14     82\\n54                       60     36\\n55                       19     58\\n56                       27     53\\n57                       35     19\\n   CURRENT_FUNCTIONAL_AREA  Count\\n0                    Sales   4167\\n1                  Support   3229\\n2                Marketing   2311\\n3                  Service   4807\\n4                      R&D   4241\\n5              Internal IT     24\\n6  F&A. HR. Administration    336\\n                    CURRENT_JOB_LEVEL  Count\\n0                       6 - Executive    118\\n1                          3 - Senior   6201\\n2                       1 - Associate    368\\n3                4 - Expert / Manager   6551\\n4                      2 - Specialist   4155\\n5  5 - Senior Expert / Senior Manager   1722\\n  CURRENT_CAREER_PATH  Count\\n0          Functional  16200\\n1           Proj/Mgmt   2915\\n              CURRENT_PERFORMANCE_RATING  Count\\n0       2 - Partially Meets Expectations    644\\n1                 3 - Meets Expectations   5798\\n2         0 - No performance Measurement   7185\\n3  5 - Consistently Exceeds Expectations    929\\n4         1 - Does Not Meet Expectations    177\\n5               4 - Exceeds Expectations   4382\\n  CURRENT_REGION  Count\\n0       Americas   4667\\n1            APJ   4330\\n2           EMEA  10118\\n   CURRENT_COUNTRY  Count\\n0            Japan    505\\n1          Croatia      5\\n2          Germany   7055\\n3           Brazil    193\\n4      Puerto Rico     10\\n5         Portugal     39\\n6            Spain    217\\n7        Australia    344\\n8          Estonia      2\\n9          Austria    101\\n10       Lithuania      2\\n11           China    827\\n12         Nigeria      8\\n13    South Africa    142\\n14       Indonesia     13\\n15         Belgium    138\\n16          Sweden     32\\n17        Malaysia     38\\n18          Russia    108\\n19          Canada    764\\n20         Finland     29\\n21         Ireland    329\\n22          Poland     30\\n23          Greece     17\\n24         Romania     23\\n25           India   2002\\n26        Bulgaria     71\\n27         Denmark     34\\n28     Netherlands    128\\n29           Chile     22\\n30          Serbia      4\\n31              UK    336\\n32     South Korea     90\\n33             USA   3350\\n34       Singapore    442\\n35  Czech Republic     37\\n36           Italy    134\\n37     Switzerland    138\\n38       Venezuela     31\\n39          Israel    219\\n40          Mexico    150\\n41          Turkey     26\\n42     New Zealand     11\\n43          France    557\\n44          Norway     13\\n45    Saudi Arabia     85\\n46         Ukraine      5\\n47            Peru     21\\n48     Philippines      8\\n49        Thailand     20\\n50       Argentina     78\\n51        Colombia     48\\n52         Hungary     31\\n53          Taiwan     30\\n54        Slovenia      3\\n55        Slovakia     20\\n    CURCOUNTRYLAT  Count\\n0       59.674971     32\\n1       51.083420   7055\\n2        9.600036      8\\n3       52.097718     30\\n4       46.798562    138\\n5       22.351115   2002\\n6       49.487197      5\\n7       47.200034    101\\n8       38.959759     26\\n9      -24.776109    344\\n10      14.897192     20\\n11      25.624262     85\\n12      45.564344      5\\n13      61.066692    764\\n14      60.500021     13\\n15      42.607398     71\\n16      49.816700     37\\n17       2.889443     48\\n18      55.350000      2\\n19      48.741152     20\\n20      19.432601    150\\n21      52.237989    128\\n22      18.221417     10\\n23      46.603354    557\\n24      47.181759     31\\n25      52.865196    329\\n26      36.574844    505\\n27      63.246778     29\\n28      39.783730   3354\\n29      40.033265     39\\n30      54.702355    336\\n31      30.876027    219\\n32      -4.799336     13\\n33      45.813311      3\\n34      50.640735    138\\n35      64.686314    108\\n36       2.392376     38\\n37     -10.333333    193\\n38     -34.996496     78\\n39       8.001871     31\\n40      35.000074    827\\n41     -28.816624    142\\n42      42.638426    134\\n43      16.039765     30\\n44      55.670249     34\\n45      -6.869970     21\\n46       1.290475    442\\n47      12.750349      8\\n48      37.532600     90\\n49      45.985213     23\\n50      40.002803    217\\n51      38.995368     17\\n52      58.752378      2\\n53     -31.761336     22\\n54     -41.500083     11\\n    CURCOUNTRYLON  Count\\n0        9.099972     13\\n1       21.987713     17\\n2      111.780899     30\\n3     -107.991707    764\\n4       12.674297    134\\n5        8.231974    138\\n6      100.832730     20\\n7      112.847194     38\\n8       -7.889626     39\\n9      -66.110932     31\\n10     -75.045851     21\\n11     172.834408     11\\n12     134.755000    344\\n13      23.750000      2\\n14       4.666960    138\\n15     127.024612     90\\n16    -100.445882   3354\\n17      10.333328     34\\n18      19.025816     30\\n19      31.271832      5\\n20      35.001520    219\\n21      19.452865     20\\n22      25.920916     29\\n23      17.011895      5\\n24      15.474954     37\\n25      -4.003104    217\\n26     122.731210      8\\n27      13.199959    101\\n28     -64.967282     78\\n29     -73.783892     48\\n30     -99.133342    150\\n31       5.534607    128\\n32     -71.318770     22\\n33      -7.979460    329\\n34      10.423447   7055\\n35     -66.413282     10\\n36      78.667743   2002\\n37       1.888334    557\\n38     114.563203     13\\n39      25.331908      2\\n40      42.352833     85\\n41      34.924965     26\\n42       7.999972      8\\n43      -3.276575    336\\n44     103.852036    442\\n45      24.991639    142\\n46      24.685923     23\\n47     139.239418    505\\n48      19.506094     31\\n49      14.520858     32\\n50     104.999927    827\\n51     -53.200000    193\\n52      14.480837      3\\n53      25.485662     71\\n54      97.745306    108\\n  PROMOTION_WITHIN_LAST_3_YEARS  Count\\n0                  No Promotion  13692\\n1                     Promotion   5423\\n  CHANGED_POSITION_WITHIN_LAST_2_YEARS  Count\\n0                            No Change  12553\\n1                               Change   6562\\n  CHANGE_IN_PERFORMANCE_RATING  Count\\n0            0 - Not available  10140\\n1                 2 - Constant   4888\\n2               1 - Increasing   1674\\n3               3 - Decreasing   2413\\n  FUNCTIONALAREACHANGETYPE  Count\\n0                No change   5220\\n1    Cross-Functional Move   5995\\n2            External Hire   1301\\n3    Intra-Functional Move   6599\\n  JOBLEVELCHANGETYPE  Count\\n0           Demotion    733\\n1         Same Level   8436\\n2          No change   5220\\n3          Promotion   3425\\n4      External Hire   1301\\n   HEADS  Count\\n0      1  19115\\n  FLIGHT_RISK  Count\\n0          No  17015\\n1         Yes   2100\\nIn\\xa0[30]:\\ndf_remote.hasna()\\nOut[30]:\\nTrue\\nIn\\xa0[31]:\\n# replace NA\\ndf_remote = df_remote.fillna(-999)\\ndf_remote = df_remote.fillna(\"-999\")\\nIn\\xa0[32]:\\n# diabetes_hdf.hasna()\\ndf_remote.hasna()\\nOut[32]:\\nFalse\\nIn\\xa0[33]:\\n# !pip install seaborn\\n# EDA TIMEINPREVPOSITIONMONTH\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\ncol_to_bin = \\'TIMEINPREVPOSITIONMONTH\\'\\ndf_time_prev = df_remote.agg([(\\'count\\', col_to_bin, \\'COUNT\\')], group_by=col_to_bin).collect().sort_values(by = \\'COUNT\\')\\nsns.lineplot(x=col_to_bin, y=\\'COUNT\\', data=df_time_prev)\\nplt.show()\\nIn\\xa0[34]:\\n# TIMEINPREVPOSITIONMONTH - feature engineering group in intervals\\n# I will check by importance if this mattters\\n    \\ndf_remote = df_remote.select \\\\\\n    (\\'*\\',(\"\"\"CASE WHEN TIMEINPREVPOSITIONMONTH <=12 THEN \\'PREV_POS (,12]\\'\\n                  WHEN TIMEINPREVPOSITIONMONTH > 12 AND TIMEINPREVPOSITIONMONTH <=24 THEN \\'PREV_POS (12,24]\\'\\n                  WHEN TIMEINPREVPOSITIONMONTH > 24 AND TIMEINPREVPOSITIONMONTH <=36 THEN \\'PREV_POS (24,36]\\'\\n                  WHEN TIMEINPREVPOSITIONMONTH > 36 AND TIMEINPREVPOSITIONMONTH <=48 THEN \\'PREV_POS (36,48]\\'\\n                  WHEN TIMEINPREVPOSITIONMONTH > 48 AND TIMEINPREVPOSITIONMONTH <=60 THEN \\'PREV_POS (48,60]\\'\\n                  WHEN TIMEINPREVPOSITIONMONTH > 60 THEN \\'TIMEPREV_POS (60,]\\'\\n              END\"\"\",\\'TIMEINPREVPOS_INT\\'))\\ndf_remote = df_remote.to_tail(\\'FLIGHT_RISK\\')\\ndf_remote.head(3).collect()\\nOut[34]:\\nEMPLOYEE_ID\\nAGE\\nAGE_GROUP10\\nAGE_GROUP5\\nGENERATION\\nCRITICAL_JOB_ROLE\\nRISK_OF_LOSS\\nIMPACT_OF_LOSS\\nFUTURE_LEADER\\nGENDER\\n...\\nCURCOUNTRYLAT\\nCURCOUNTRYLON\\nPROMOTION_WITHIN_LAST_3_YEARS\\nCHANGED_POSITION_WITHIN_LAST_2_YEARS\\nCHANGE_IN_PERFORMANCE_RATING\\nFUNCTIONALAREACHANGETYPE\\nJOBLEVELCHANGETYPE\\nHEADS\\nTIMEINPREVPOS_INT\\nFLIGHT_RISK\\n0\\n10032\\n33\\n(25-35]\\n(30-35]\\nGeneration Y\\nCritical\\nHigh\\nHigh\\nNo Future Leader\\nFemale\\n...\\n39.78373\\n-100.445882\\nNo Promotion\\nNo Change\\n0 - Not available\\nNo change\\nNo change\\n1\\nPREV_POS (,12]\\nNo\\n1\\n10033\\n43\\n(35-45]\\n(40-45]\\nGeneration X\\nCritical\\nLow\\nHigh\\nNo Future Leader\\nFemale\\n...\\n51.08342\\n10.423447\\nNo Promotion\\nNo Change\\n0 - Not available\\nNo change\\nNo change\\n1\\nPREV_POS (,12]\\nNo\\n2\\n10034\\n33\\n(25-35]\\n(30-35]\\nGeneration Y\\nCritical\\nMedium\\nHigh\\nNo Future Leader\\nFemale\\n...\\n39.78373\\n-100.445882\\nNo Promotion\\nNo Change\\n0 - Not available\\nExternal Hire\\nExternal Hire\\n1\\nPREV_POS (,12]\\nNo\\n3 rows √ó 44 columns\\nIn\\xa0[35]:\\n# EDA created groups\\ncol = \\'TIMEINPREVPOS_INT\\'\\ndf_remote.agg([(\\'count\\', col, \\'Count\\')], group_by=col).collect()\\nOut[35]:\\nTIMEINPREVPOS_INT\\nCount\\n0\\nTIMEPREV_POS (60,]\\n17\\n1\\nPREV_POS (48,60]\\n3462\\n2\\nPREV_POS (,12]\\n10088\\n3\\nPREV_POS (12,24]\\n2275\\n4\\nPREV_POS (36,48]\\n715\\n5\\nPREV_POS (24,36]\\n2558\\nIn\\xa0[36]:\\n# Check header\\ndf_remote.head(3).collect()\\nOut[36]:\\nEMPLOYEE_ID\\nAGE\\nAGE_GROUP10\\nAGE_GROUP5\\nGENERATION\\nCRITICAL_JOB_ROLE\\nRISK_OF_LOSS\\nIMPACT_OF_LOSS\\nFUTURE_LEADER\\nGENDER\\n...\\nCURCOUNTRYLAT\\nCURCOUNTRYLON\\nPROMOTION_WITHIN_LAST_3_YEARS\\nCHANGED_POSITION_WITHIN_LAST_2_YEARS\\nCHANGE_IN_PERFORMANCE_RATING\\nFUNCTIONALAREACHANGETYPE\\nJOBLEVELCHANGETYPE\\nHEADS\\nTIMEINPREVPOS_INT\\nFLIGHT_RISK\\n0\\n10032\\n33\\n(25-35]\\n(30-35]\\nGeneration Y\\nCritical\\nHigh\\nHigh\\nNo Future Leader\\nFemale\\n...\\n39.78373\\n-100.445882\\nNo Promotion\\nNo Change\\n0 - Not available\\nNo change\\nNo change\\n1\\nPREV_POS (,12]\\nNo\\n1\\n10033\\n43\\n(35-45]\\n(40-45]\\nGeneration X\\nCritical\\nLow\\nHigh\\nNo Future Leader\\nFemale\\n...\\n51.08342\\n10.423447\\nNo Promotion\\nNo Change\\n0 - Not available\\nNo change\\nNo change\\n1\\nPREV_POS (,12]\\nNo\\n2\\n10034\\n33\\n(25-35]\\n(30-35]\\nGeneration Y\\nCritical\\nMedium\\nHigh\\nNo Future Leader\\nFemale\\n...\\n39.78373\\n-100.445882\\nNo Promotion\\nNo Change\\n0 - Not available\\nExternal Hire\\nExternal Hire\\n1\\nPREV_POS (,12]\\nNo\\n3 rows √ó 44 columns\\nIn\\xa0[37]:\\n# Define variables for ID and target\\ntab_id = \\'EMPLOYEE_ID\\'\\ntarget = \"FLIGHT_RISK\"\\nIn\\xa0[38]:\\n# Show column descriptive statistics using the describe method\\ndf_remote.describe().head(10).collect()\\nOut[38]:\\ncolumn\\ncount\\nunique\\nnulls\\nmean\\nstd\\nmin\\nmax\\nmedian\\n25_percent_cont\\n25_percent_disc\\n50_percent_cont\\n50_percent_disc\\n75_percent_cont\\n75_percent_disc\\n0\\nEMPLOYEE_ID\\n19115\\n19115\\n0\\n19589.000000\\n5518.169533\\n10032.000000\\n29146.000000\\n19589.000000\\n14810.500000\\n14810.000000\\n19589.000000\\n19589.000000\\n24367.500000\\n24368.000000\\n1\\nAGE\\n19115\\n40\\n0\\n39.074915\\n9.188863\\n18.000000\\n57.000000\\n39.000000\\n32.000000\\n32.000000\\n39.000000\\n39.000000\\n46.000000\\n46.000000\\n2\\nTENURE_MONTHS\\n19115\\n472\\n0\\n140.363484\\n103.163766\\n1.000000\\n486.000000\\n120.000000\\n55.000000\\n55.000000\\n120.000000\\n120.000000\\n209.000000\\n209.000000\\n3\\nSALARY\\n19115\\n14\\n0\\n62003.159822\\n24708.791838\\n4900.000000\\n120000.000000\\n63000.000000\\n40000.000000\\n40000.000000\\n63000.000000\\n63000.000000\\n70000.000000\\n70000.000000\\n4\\nPREVCOUNTRYLAT\\n19115\\n56\\n0\\n-315.261767\\n492.253418\\n-999.000000\\n64.686314\\n35.000074\\n-999.000000\\n-999.000000\\n35.000074\\n35.000074\\n51.083420\\n51.083420\\n5\\nPREVCOUNTRYLON\\n19115\\n56\\n0\\n-336.877735\\n479.979973\\n-999.000000\\n172.834408\\n-53.200000\\n-999.000000\\n-999.000000\\n-53.200000\\n-53.200000\\n10.423447\\n10.423447\\n6\\nTIMEINPREVPOSITIONMONTH\\n19115\\n58\\n0\\n19.885901\\n19.864530\\n0.000000\\n396.000000\\n9.000000\\n3.000000\\n3.000000\\n9.000000\\n9.000000\\n36.000000\\n36.000000\\n7\\nCURCOUNTRYLAT\\n19115\\n55\\n0\\n39.749109\\n18.354538\\n-41.500083\\n64.686314\\n46.603354\\n36.574844\\n36.574844\\n46.603354\\n46.603354\\n51.083420\\n51.083420\\n8\\nCURCOUNTRYLON\\n19115\\n55\\n0\\n4.282763\\n69.918683\\n-107.991707\\n172.834408\\n10.423447\\n-7.979460\\n-7.979460\\n10.423447\\n10.423447\\n25.920916\\n25.920916\\n9\\nHEADS\\n19115\\n1\\n0\\n1.000000\\n0.000000\\n1.000000\\n1.000000\\n1.000000\\n1.000000\\n1.000000\\n1.000000\\n1.000000\\n1.000000\\n1.000000\\nIn\\xa0[39]:\\n# CLASSES count\\ndf_remote.agg([(\\'count\\', tab_id, \\'COUNT\\')], group_by=target).collect()\\nOut[39]:\\nFLIGHT_RISK\\nCOUNT\\n0\\nNo\\n17015\\n1\\nYes\\n2100\\nIn\\xa0[40]:\\n# CLASSES are imbalanced\\nIn\\xa0[41]:\\n# Quick tests with less data\\ntest_rows = 100000\\ntry:\\n    df_remote_keep_shape = df_remote_keep.shape\\nexcept:\\n    df_remote_keep = df_remote\\n    \\ndf_remote = df_remote_keep.head(test_rows) # quick tests\\ndf_remote.shape, df_remote_keep.shape\\nOut[41]:\\n([19115, 44], [19115, 44])\\nIn\\xa0[42]:\\n# Partition the input data set\\n# default: training_percentage = 0.8, testing_percentage = 0.1, validation_percentage = 0.1\\nfrom hana_ml.algorithms.pal.partition import train_test_val_split as split\\nd_train, d_test, d_val = split( data=df_remote, partition_method=\\'stratified\\', stratified_column=target, validation_percentage = 0)\\nIn\\xa0[43]:\\n# help(split)\\nIn\\xa0[44]:\\nd_train.shape, d_test.shape, d_val.shape\\nOut[44]:\\n([15292, 44], [1912, 44], [0, 44])\\nIn\\xa0[45]:\\n# This is a classification task - I will use a classification predictive modeling \\nIn\\xa0[46]:\\n# Use UnifiedClassification PAL procedure interface in Python\\nfrom hana_ml.algorithms.pal.unified_classification import UnifiedClassification\\n# Iterate over different Paremeter Settings of the Algorithm, find best setting\\nfrom hana_ml.algorithms.pal.model_selection import GridSearchCV\\nIn\\xa0[47]:\\n# Function to use repeatedly\\ndef model_fit(d_train):\\n#     global target, tab_id\\n    HGBT_MODEL = UnifiedClassification(\\'HybridGradientBoostingTree\\')\\n    MODEL_SEARCH = GridSearchCV(estimator=HGBT_MODEL, \\n                        param_grid={\\'learning_rate\\': [0.1, 0.4, 0.7, 1],\\n                                    \\'n_estimators\\': [4, 6, 8, 10],\\n                                    \\'split_threshold\\': [0.1, 0.4, 0.7, 1]},\\n                        train_control=dict(fold_num=5,\\n                                           resampling_method=\\'cv\\',\\n                                           random_state=1,\\n                                           ref_metric=[\\'auc\\']),\\n                        scoring=\\'error_rate\\')\\n    MODEL_SEARCH.fit(data=d_train, key= tab_id,\\n             label=target,\\n             partition_method=\\'stratified\\',\\n             partition_random_state=1,\\n             stratified_column=target,\\n             build_report=False,\\n             training_percent = 0.8 ) # added SIATCO default 0.8)\\n    return HGBT_MODEL\\nIn\\xa0[48]:\\n# help(HGBT_MODEL)\\nIn\\xa0[49]:\\n%%time\\n# First FIT of INITIAL DATA\\nHGBT_MODEL = model_fit(d_train)\\nCPU times: total: 0 ns\\nWall time: 49.4 s\\nConfusion matrix - Wikipedia¬∂\\nIn\\xa0[50]:\\n# CHEAT SHEET\\n# HGBT_MODEL.confusion_matrix_.collect()\\n# HGBT_MODEL.statistics_.collect()\\n# HGBT_MODEL.metrics_.collect()\\n# HGBT_MODEL.optimal_param_.collect()\\n# HGBT_MODEL.model_[3].collect() #confusion matrix from model_ # method from list\\nHGBT_MODEL.confusion_matrix_.collect()\\nOut[50]:\\nACTUAL_CLASS\\nPREDICTED_CLASS\\nCOUNT\\n0\\nNo\\nNo\\n2711\\n1\\nNo\\nYes\\n11\\n2\\nYes\\nNo\\n220\\n3\\nYes\\nYes\\n116\\nIn\\xa0[51]:\\nHGBT_MODEL.confusion_matrix_.agg([(\\'sum\\', \\'COUNT\\', \\'COUNT\\')], group_by=\\'ACTUAL_CLASS\\').collect()\\nOut[51]:\\nACTUAL_CLASS\\nCOUNT\\n0\\nNo\\n2722\\n1\\nYes\\n336\\nIn\\xa0[52]:\\n# Majority class: Minority class = No : Yes\\nIn\\xa0[53]:\\n# The task is to predict Minority class No. Classes are imbalanced.\\n# I will rely on RECALL for Yes. TRUE Yes rate. Missed rate. Underestimation.\\n# PRECISION for Yes. False alarm. Overestimation. Increased because of No class increased RECALL. Msseed rate.\\nIn\\xa0[54]:\\nHGBT_MODEL.confusion_matrix_.collect()[\\'COUNT\\'].sum()\\nOut[54]:\\n3058\\nIn\\xa0[55]:\\nd_train.shape, int(d_train.shape[0] * 0.2) #check the validation test rows\\nOut[55]:\\n([15292, 44], 3058)\\nIn\\xa0[56]:\\nHGBT_MODEL.statistics_.collect() # MODEL statistics\\nOut[56]:\\nSTAT_NAME\\nSTAT_VALUE\\nCLASS_NAME\\n0\\nAUC\\n0.974370583799326\\nNone\\n1\\nRECALL\\n0.9959588537839824\\nNo\\n2\\nPRECISION\\n0.9249402934152167\\nNo\\n3\\nF1_SCORE\\n0.9591367415531578\\nNo\\n4\\nSUPPORT\\n2722\\nNo\\n5\\nRECALL\\n0.34523809523809523\\nYes\\n6\\nPRECISION\\n0.9133858267716536\\nYes\\n7\\nF1_SCORE\\n0.5010799136069115\\nYes\\n8\\nSUPPORT\\n336\\nYes\\n9\\nACCURACY\\n0.9244604316546763\\nNone\\n10\\nKAPPA\\n0.46907727112159997\\nNone\\n11\\nMCC\\n0.5348217595547845\\nNone\\nIn\\xa0[57]:\\n# MODEL RECALL Yes  ~ 0.37. low\\nIn\\xa0[58]:\\n# function for predicted score to use repeatedly\\ndef f_score_res(model,d_test): \\n    return model.score(d_test, key=tab_id,  top_k_attributions=10)\\nIn\\xa0[59]:\\nd_test.shape\\nOut[59]:\\n[1912, 44]\\nIn\\xa0[60]:\\nd_test.agg([(\\'count\\', tab_id, \\'COUNT\\')], group_by=target).collect() # check SUPPORT No and SUPPORT Yes\\nOut[60]:\\nFLIGHT_RISK\\nCOUNT\\n0\\nNo\\n1702\\n1\\nYes\\n210\\nIn\\xa0[61]:\\nf_score_res(HGBT_MODEL,d_test)[1].collect()\\nOut[61]:\\nSTAT_NAME\\nSTAT_VALUE\\nCLASS_NAME\\n0\\nAUC\\n0.9733009204145586\\nNone\\n1\\nRECALL\\n0.9964747356051704\\nNo\\n2\\nPRECISION\\n0.9247546346782988\\nNo\\n3\\nF1_SCORE\\n0.9592760180995475\\nNo\\n4\\nSUPPORT\\n1702\\nNo\\n5\\nRECALL\\n0.34285714285714286\\nYes\\n6\\nPRECISION\\n0.9230769230769231\\nYes\\n7\\nF1_SCORE\\n0.5000000000000001\\nYes\\n8\\nSUPPORT\\n210\\nYes\\n9\\nACCURACY\\n0.9246861924686193\\nNone\\n10\\nKAPPA\\n0.4683720283609064\\nNone\\n11\\nMCC\\n0.5363732609971424\\nNone\\nIn\\xa0[62]:\\n# Predicted RECALL Yes  ~ 0.31.  low. True Yes rate. Missed. Underestimation.\\n# Predicted PRECISON Yes. False Yes rate. False alarm. Overestimation. Not reliable SUPPORT Yes < SUPPORT No \\nIn\\xa0[63]:\\n# # p_HGBT_MODEL = HGBT_MODEL\\n# # p_d_test = d_test\\n# try:\\n#     del p_HGBT_MODEL\\n#     del p_d_test\\n# except:\\n#     pass\\nIn\\xa0[64]:\\ndf_model_stat_all = pd.DataFrame()\\ndf_pred_stat_all = pd.DataFrame()\\ndef f_stat_all(p_model, p_d_test, p_col_name):\\n    global df_model_stat_all\\n    global df_pred_stat_all\\n    stat_value = \\'STAT_VALUE\\' \\n    cols_order = [\\'STAT_NAME\\', \\'CLASS_NAME\\']\\n    score_res = f_score_res(p_model,p_d_test) # predicted statistics\\n    if df_model_stat_all.empty and df_pred_stat_all.empty:\\n        df_model_stat_all = p_model.statistics_.collect()[cols_order]\\n        df_pred_stat_all = score_res[1].collect()[cols_order]\\n        \\n    df_model_stat_all[p_col_name] = p_model.statistics_.collect()[stat_value]\\n    df_pred_stat_all[p_col_name] = score_res[1].collect()[stat_value]\\n    \\n    df_model_stat_all[p_col_name] = df_model_stat_all[p_col_name].str[:6]\\n    df_pred_stat_all[p_col_name] = df_pred_stat_all[p_col_name].str[:6]\\n    \\n    return df_model_stat_all, df_pred_stat_all\\ndf_model_stat_all.empty, df_pred_stat_all.empty\\nOut[64]:\\n(True, True)\\nIn\\xa0[65]:\\ndf_model_stat_last, df_pred_stat_last = f_stat_all(HGBT_MODEL, d_test, \\'INIT\\')\\nIn\\xa0[66]:\\ndf_model_stat_last\\nOut[66]:\\nSTAT_NAME\\nCLASS_NAME\\nINIT\\n0\\nAUC\\nNone\\n0.9743\\n1\\nRECALL\\nNo\\n0.9959\\n2\\nPRECISION\\nNo\\n0.9249\\n3\\nF1_SCORE\\nNo\\n0.9591\\n4\\nSUPPORT\\nNo\\n2722\\n5\\nRECALL\\nYes\\n0.3452\\n6\\nPRECISION\\nYes\\n0.9133\\n7\\nF1_SCORE\\nYes\\n0.5010\\n8\\nSUPPORT\\nYes\\n336\\n9\\nACCURACY\\nNone\\n0.9244\\n10\\nKAPPA\\nNone\\n0.4690\\n11\\nMCC\\nNone\\n0.5348\\nIn\\xa0[67]:\\ndf_pred_stat_last\\nOut[67]:\\nSTAT_NAME\\nCLASS_NAME\\nINIT\\n0\\nAUC\\nNone\\n0.9733\\n1\\nRECALL\\nNo\\n0.9964\\n2\\nPRECISION\\nNo\\n0.9247\\n3\\nF1_SCORE\\nNo\\n0.9592\\n4\\nSUPPORT\\nNo\\n1702\\n5\\nRECALL\\nYes\\n0.3428\\n6\\nPRECISION\\nYes\\n0.9230\\n7\\nF1_SCORE\\nYes\\n0.5000\\n8\\nSUPPORT\\nYes\\n210\\n9\\nACCURACY\\nNone\\n0.9246\\n10\\nKAPPA\\nNone\\n0.4683\\n11\\nMCC\\nNone\\n0.5363\\nIn\\xa0[68]:\\n# Feature importance\\nHGBT_MODEL.importance_.sort(\\'IMPORTANCE\\', desc=True).collect()\\nOut[68]:\\nVARIABLE_NAME\\nIMPORTANCE\\n0\\nFUNCTIONALAREACHANGETYPE\\n0.200987\\n1\\nTIMEINPREVPOSITIONMONTH\\n0.181448\\n2\\nEMPLOYMENT_TYPE_2\\n0.149898\\n3\\nPREVCOUNTRYLON\\n0.064370\\n4\\nPROMOTION_WITHIN_LAST_3_YEARS\\n0.058553\\n5\\nSALARY\\n0.048992\\n6\\nJOBLEVELCHANGETYPE\\n0.047154\\n7\\nAGE\\n0.032812\\n8\\nRISK_OF_LOSS\\n0.027660\\n9\\nCHANGE_IN_PERFORMANCE_RATING\\n0.023107\\n10\\nIMPACT_OF_LOSS\\n0.021065\\n11\\nTENURE_MONTHS\\n0.018194\\n12\\nMINORITY\\n0.010992\\n13\\nFUTURE_LEADER\\n0.010602\\n14\\nCURCOUNTRYLON\\n0.010455\\n15\\nAGE_GROUP10\\n0.008304\\n16\\nHIGH_POTENTIAL\\n0.007732\\n17\\nGENDER\\n0.007719\\n18\\nTENURE_INTERVAL_YEARS\\n0.007150\\n19\\nAGE_GROUP5\\n0.007101\\n20\\nCURRENT_FUNCTIONAL_AREA\\n0.006967\\n21\\nPREVIOUS_COUNTRY\\n0.006336\\n22\\nCRITICAL_JOB_ROLE\\n0.006248\\n23\\nGENERATION\\n0.005629\\n24\\nMGR_EMP\\n0.005409\\n25\\nCURRENT_JOB_LEVEL\\n0.003733\\n26\\nPREVIOUS_PERFORMANCE_RATING\\n0.003720\\n27\\nEMPLOYMENT_TYPE\\n0.003252\\n28\\nPREVIOUS_JOB_LEVEL\\n0.003242\\n29\\nCURCOUNTRYLAT\\n0.002803\\n30\\nCURRENT_PERFORMANCE_RATING\\n0.002132\\n31\\nPREVCOUNTRYLAT\\n0.001987\\n32\\nCURRENT_COUNTRY\\n0.001707\\n33\\nPREVIOUS_REGION\\n0.001463\\n34\\nPREVIOUS_FUNCTIONAL_AREA\\n0.000491\\n35\\nCHANGED_POSITION_WITHIN_LAST_2_YEARS\\n0.000491\\n36\\nCURRENT_REGION\\n0.000095\\n37\\nCURRENT_CAREER_PATH\\n0.000000\\n38\\nPREVIOUS_CAREER_PATH\\n0.000000\\n39\\nTENURE_INTERVALL_DESC\\n0.000000\\n40\\nHEADS\\n0.000000\\n41\\nTIMEINPREVPOS_INT\\n0.000000\\nIn\\xa0[69]:\\n# TIMEINPREVPOS_INT ~ 22 top\\nIn\\xa0[70]:\\n# JOIN ADDITIONAL DATA with INITIAL DATA\\ndf_remote_new = df_remote.set_index(tab_id).join(df_remote_add.set_index(tab_id)) \\ndf_remote_new.head(3).collect()\\nOut[70]:\\nEMPLOYEE_ID\\nAGE\\nAGE_GROUP10\\nAGE_GROUP5\\nGENERATION\\nCRITICAL_JOB_ROLE\\nRISK_OF_LOSS\\nIMPACT_OF_LOSS\\nFUTURE_LEADER\\nGENDER\\n...\\nCHANGED_POSITION_WITHIN_LAST_2_YEARS\\nCHANGE_IN_PERFORMANCE_RATING\\nFUNCTIONALAREACHANGETYPE\\nJOBLEVELCHANGETYPE\\nHEADS\\nTIMEINPREVPOS_INT\\nFLIGHT_RISK\\nLINKEDIN\\nHRTRAINING\\nSICKDAYS\\n0\\n10032\\n33\\n(25-35]\\n(30-35]\\nGeneration Y\\nCritical\\nHigh\\nHigh\\nNo Future Leader\\nFemale\\n...\\nNo Change\\n0 - Not available\\nNo change\\nNo change\\n1\\nPREV_POS (,12]\\nNo\\nYes\\nYes\\n10\\n1\\n10033\\n43\\n(35-45]\\n(40-45]\\nGeneration X\\nCritical\\nLow\\nHigh\\nNo Future Leader\\nFemale\\n...\\nNo Change\\n0 - Not available\\nNo change\\nNo change\\n1\\nPREV_POS (,12]\\nNo\\nNo\\nNo\\n6\\n2\\n10034\\n33\\n(25-35]\\n(30-35]\\nGeneration Y\\nCritical\\nMedium\\nHigh\\nNo Future Leader\\nFemale\\n...\\nNo Change\\n0 - Not available\\nExternal Hire\\nExternal Hire\\n1\\nPREV_POS (,12]\\nNo\\nYes\\nNo\\n5\\n3 rows √ó 47 columns\\nIn\\xa0[71]:\\ndf_remote = df_remote_new\\ndf_remote = df_remote.to_tail(target)\\ndf_remote.dtypes() # () list of columns as parameter\\nOut[71]:\\n[(\\'EMPLOYEE_ID\\', \\'INT\\', 10, 10, 10, 0),\\n (\\'AGE\\', \\'INT\\', 10, 10, 10, 0),\\n (\\'AGE_GROUP10\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'AGE_GROUP5\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'GENERATION\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'CRITICAL_JOB_ROLE\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'RISK_OF_LOSS\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'IMPACT_OF_LOSS\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'FUTURE_LEADER\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'GENDER\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'MGR_EMP\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'MINORITY\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'TENURE_MONTHS\\', \\'INT\\', 10, 10, 10, 0),\\n (\\'TENURE_INTERVAL_YEARS\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'TENURE_INTERVALL_DESC\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'SALARY\\', \\'INT\\', 10, 10, 10, 0),\\n (\\'EMPLOYMENT_TYPE\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'EMPLOYMENT_TYPE_2\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'HIGH_POTENTIAL\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'PREVIOUS_FUNCTIONAL_AREA\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'PREVIOUS_JOB_LEVEL\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'PREVIOUS_CAREER_PATH\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'PREVIOUS_PERFORMANCE_RATING\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'PREVIOUS_COUNTRY\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'PREVCOUNTRYLAT\\', \\'DOUBLE\\', 15, 15, 15, 0),\\n (\\'PREVCOUNTRYLON\\', \\'DOUBLE\\', 15, 15, 15, 0),\\n (\\'PREVIOUS_REGION\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'TIMEINPREVPOSITIONMONTH\\', \\'INT\\', 10, 10, 10, 0),\\n (\\'CURRENT_FUNCTIONAL_AREA\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'CURRENT_JOB_LEVEL\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'CURRENT_CAREER_PATH\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'CURRENT_PERFORMANCE_RATING\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'CURRENT_REGION\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'CURRENT_COUNTRY\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'CURCOUNTRYLAT\\', \\'DOUBLE\\', 15, 15, 15, 0),\\n (\\'CURCOUNTRYLON\\', \\'DOUBLE\\', 15, 15, 15, 0),\\n (\\'PROMOTION_WITHIN_LAST_3_YEARS\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'CHANGED_POSITION_WITHIN_LAST_2_YEARS\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'CHANGE_IN_PERFORMANCE_RATING\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'FUNCTIONALAREACHANGETYPE\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'JOBLEVELCHANGETYPE\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'HEADS\\', \\'INT\\', 10, 10, 10, 0),\\n (\\'TIMEINPREVPOS_INT\\', \\'NVARCHAR\\', 18, 18, 18, 0),\\n (\\'LINKEDIN\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'HRTRAINING\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'SICKDAYS\\', \\'INT\\', 10, 10, 10, 0),\\n (\\'FLIGHT_RISK\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0)]\\nIn\\xa0[72]:\\n# Partition the input data set\\n# default: training_percentage = 0.8, testing_percentage = 0.1, validation_percentage = 0.1\\nfrom hana_ml.algorithms.pal.partition import train_test_val_split as split\\nd_train, d_test, d_val = split( data=df_remote, partition_method=\\'stratified\\', stratified_column=target, validation_percentage = 0)\\nIn\\xa0[73]:\\n# shapes with all features\\nd_train.shape, d_test.shape, d_val.shape\\nOut[73]:\\n([15292, 47], [1912, 47], [0, 47])\\nIn\\xa0[74]:\\n%%time\\n# fit model with all features\\nHGBT_MODEL = model_fit(d_train)\\nCPU times: total: 15.6 ms\\nWall time: 52.7 s\\nIn\\xa0[75]:\\nHGBT_MODEL.statistics_.collect() # MODEL statistics\\nOut[75]:\\nSTAT_NAME\\nSTAT_VALUE\\nCLASS_NAME\\n0\\nAUC\\n0.9830368596495656\\nNone\\n1\\nRECALL\\n0.979059515062454\\nNo\\n2\\nPRECISION\\n0.9531473533619457\\nNo\\n3\\nF1_SCORE\\n0.965929684668358\\nNo\\n4\\nSUPPORT\\n2722\\nNo\\n5\\nRECALL\\n0.6101190476190477\\nYes\\n6\\nPRECISION\\n0.7824427480916031\\nYes\\n7\\nF1_SCORE\\n0.68561872909699\\nYes\\n8\\nSUPPORT\\n336\\nYes\\n9\\nACCURACY\\n0.9385219097449313\\nNone\\n10\\nKAPPA\\n0.6521257155304909\\nNone\\n11\\nMCC\\n0.6583266048832767\\nNone\\nIn\\xa0[76]:\\ndf_imp = HGBT_MODEL.importance_.sort(\\'IMPORTANCE\\', desc=True).collect()\\ndf_imp\\nOut[76]:\\nVARIABLE_NAME\\nIMPORTANCE\\n0\\nHRTRAINING\\n0.251717\\n1\\nEMPLOYMENT_TYPE_2\\n0.126701\\n2\\nSICKDAYS\\n0.123949\\n3\\nFUNCTIONALAREACHANGETYPE\\n0.110139\\n4\\nTIMEINPREVPOSITIONMONTH\\n0.080114\\n5\\nPROMOTION_WITHIN_LAST_3_YEARS\\n0.058529\\n6\\nPREVCOUNTRYLON\\n0.031346\\n7\\nSALARY\\n0.027805\\n8\\nJOBLEVELCHANGETYPE\\n0.026785\\n9\\nLINKEDIN\\n0.024140\\n10\\nRISK_OF_LOSS\\n0.020218\\n11\\nCHANGE_IN_PERFORMANCE_RATING\\n0.019540\\n12\\nAGE\\n0.014719\\n13\\nGENDER\\n0.012422\\n14\\nCURCOUNTRYLON\\n0.009911\\n15\\nFUTURE_LEADER\\n0.006836\\n16\\nPREVCOUNTRYLAT\\n0.005588\\n17\\nTENURE_MONTHS\\n0.005014\\n18\\nIMPACT_OF_LOSS\\n0.004954\\n19\\nCURRENT_COUNTRY\\n0.004256\\n20\\nCHANGED_POSITION_WITHIN_LAST_2_YEARS\\n0.004069\\n21\\nCURRENT_FUNCTIONAL_AREA\\n0.003920\\n22\\nCURCOUNTRYLAT\\n0.003880\\n23\\nPREVIOUS_PERFORMANCE_RATING\\n0.002883\\n24\\nPREVIOUS_FUNCTIONAL_AREA\\n0.002861\\n25\\nTENURE_INTERVAL_YEARS\\n0.002486\\n26\\nPREVIOUS_REGION\\n0.002298\\n27\\nCURRENT_PERFORMANCE_RATING\\n0.002048\\n28\\nPREVIOUS_COUNTRY\\n0.001721\\n29\\nAGE_GROUP5\\n0.001693\\n30\\nMGR_EMP\\n0.001629\\n31\\nAGE_GROUP10\\n0.001586\\n32\\nTIMEINPREVPOS_INT\\n0.001441\\n33\\nHIGH_POTENTIAL\\n0.000977\\n34\\nPREVIOUS_CAREER_PATH\\n0.000785\\n35\\nCRITICAL_JOB_ROLE\\n0.000566\\n36\\nCURRENT_JOB_LEVEL\\n0.000474\\n37\\nHEADS\\n0.000000\\n38\\nGENERATION\\n0.000000\\n39\\nCURRENT_REGION\\n0.000000\\n40\\nTENURE_INTERVALL_DESC\\n0.000000\\n41\\nMINORITY\\n0.000000\\n42\\nPREVIOUS_JOB_LEVEL\\n0.000000\\n43\\nEMPLOYMENT_TYPE\\n0.000000\\n44\\nCURRENT_CAREER_PATH\\n0.000000\\nIn\\xa0[77]:\\n# features from additional data at top\\nIn\\xa0[78]:\\nf_score_res(HGBT_MODEL,d_test)[1].collect() # all features\\nOut[78]:\\nSTAT_NAME\\nSTAT_VALUE\\nCLASS_NAME\\n0\\nAUC\\n0.9834299119413176\\nNone\\n1\\nRECALL\\n0.9800235017626322\\nNo\\n2\\nPRECISION\\n0.9488054607508533\\nNo\\n3\\nF1_SCORE\\n0.9641618497109826\\nNo\\n4\\nSUPPORT\\n1702\\nNo\\n5\\nRECALL\\n0.5714285714285714\\nYes\\n6\\nPRECISION\\n0.7792207792207793\\nYes\\n7\\nF1_SCORE\\n0.6593406593406593\\nYes\\n8\\nSUPPORT\\n210\\nYes\\n9\\nACCURACY\\n0.9351464435146444\\nNone\\n10\\nKAPPA\\n0.6244376576142743\\nNone\\n11\\nMCC\\n0.6336178496301642\\nNone\\nIn\\xa0[79]:\\ndf_model_stat_last, df_pred_stat_last = f_stat_all(HGBT_MODEL, d_test, \\'ADD\\')\\nIn\\xa0[80]:\\ndf_model_stat_last\\nOut[80]:\\nSTAT_NAME\\nCLASS_NAME\\nINIT\\nADD\\n0\\nAUC\\nNone\\n0.9743\\n0.9830\\n1\\nRECALL\\nNo\\n0.9959\\n0.9790\\n2\\nPRECISION\\nNo\\n0.9249\\n0.9531\\n3\\nF1_SCORE\\nNo\\n0.9591\\n0.9659\\n4\\nSUPPORT\\nNo\\n2722\\n2722\\n5\\nRECALL\\nYes\\n0.3452\\n0.6101\\n6\\nPRECISION\\nYes\\n0.9133\\n0.7824\\n7\\nF1_SCORE\\nYes\\n0.5010\\n0.6856\\n8\\nSUPPORT\\nYes\\n336\\n336\\n9\\nACCURACY\\nNone\\n0.9244\\n0.9385\\n10\\nKAPPA\\nNone\\n0.4690\\n0.6521\\n11\\nMCC\\nNone\\n0.5348\\n0.6583\\nIn\\xa0[81]:\\n# MODEL RECALL Yes jumps\\nIn\\xa0[82]:\\ndf_pred_stat_last\\nOut[82]:\\nSTAT_NAME\\nCLASS_NAME\\nINIT\\nADD\\n0\\nAUC\\nNone\\n0.9733\\n0.9834\\n1\\nRECALL\\nNo\\n0.9964\\n0.9800\\n2\\nPRECISION\\nNo\\n0.9247\\n0.9488\\n3\\nF1_SCORE\\nNo\\n0.9592\\n0.9641\\n4\\nSUPPORT\\nNo\\n1702\\n1702\\n5\\nRECALL\\nYes\\n0.3428\\n0.5714\\n6\\nPRECISION\\nYes\\n0.9230\\n0.7792\\n7\\nF1_SCORE\\nYes\\n0.5000\\n0.6593\\n8\\nSUPPORT\\nYes\\n210\\n210\\n9\\nACCURACY\\nNone\\n0.9246\\n0.9351\\n10\\nKAPPA\\nNone\\n0.4683\\n0.6244\\n11\\nMCC\\nNone\\n0.5363\\n0.6336\\nIn\\xa0[83]:\\n# Predicted RECAL Yes jumps\\nIn\\xa0[84]:\\nd_test.shape\\nOut[84]:\\n[1912, 47]\\nIn\\xa0[85]:\\n# Reducing the number of features by importance\\n# Grouping by meaning\\nIn\\xa0[86]:\\ncol_imp = \\'IMPORTANCE\\'\\ncol_vn = \\'VARIABLE_NAME\\'\\ndf_imp_zero = df_imp[df_imp[col_imp] == 0]\\nCOL_IMP_ZERO = df_imp_zero[col_vn].to_list()\\n# [\\'HEADS\\'] - constant expected zero importance\\nCOL_IMP_ZERO\\nOut[86]:\\n[\\'HEADS\\',\\n \\'GENERATION\\',\\n \\'CURRENT_REGION\\',\\n \\'TENURE_INTERVALL_DESC\\',\\n \\'MINORITY\\',\\n \\'PREVIOUS_JOB_LEVEL\\',\\n \\'EMPLOYMENT_TYPE\\',\\n \\'CURRENT_CAREER_PATH\\']\\nIn\\xa0[87]:\\ncol_age_dep = [\\'AGE\\', \\'AGE_GROUP5\\', \\'AGE_GROUP10\\', \\'GENERATION\\'] # age group\\ncol_age_dep\\nOut[87]:\\n[\\'AGE\\', \\'AGE_GROUP5\\', \\'AGE_GROUP10\\', \\'GENERATION\\']\\nIn\\xa0[88]:\\ndf_imp_age = df_imp[df_imp[col_vn].isin(col_age_dep)].sort_values(by = col_imp, ascending = False) # importance desc\\ndf_imp_age\\nOut[88]:\\nVARIABLE_NAME\\nIMPORTANCE\\n12\\nAGE\\n0.014719\\n29\\nAGE_GROUP5\\n0.001693\\n31\\nAGE_GROUP10\\n0.001586\\n38\\nGENERATION\\n0.000000\\nIn\\xa0[89]:\\nCOL_IMP_AGE = df_imp_age[col_vn].to_list() # importance desc to list\\nCOL_IMP_AGE\\nOut[89]:\\n[\\'AGE\\', \\'AGE_GROUP5\\', \\'AGE_GROUP10\\', \\'GENERATION\\']\\nIn\\xa0[90]:\\ncol_imp_prev_loc = [\\'PREVIOUS_COUNTRY\\', \\'PREVCOUNTRYLAT\\', \\'PREVCOUNTRYLON\\', \\'PREVIOUS_REGION\\'] # previous country group\\ndf_imp_prev_loc = df_imp[df_imp[col_vn].isin(col_imp_prev_loc)].sort_values(by = col_imp, ascending = False)\\ndf_imp_prev_loc\\nOut[90]:\\nVARIABLE_NAME\\nIMPORTANCE\\n6\\nPREVCOUNTRYLON\\n0.031346\\n16\\nPREVCOUNTRYLAT\\n0.005588\\n26\\nPREVIOUS_REGION\\n0.002298\\n28\\nPREVIOUS_COUNTRY\\n0.001721\\nIn\\xa0[91]:\\nCOL_IMP_PREVIOUS_COUNTRY = df_imp_prev_loc[col_vn].to_list() # importance desc to list\\nCOL_IMP_PREVIOUS_COUNTRY\\nOut[91]:\\n[\\'PREVCOUNTRYLON\\', \\'PREVCOUNTRYLAT\\', \\'PREVIOUS_REGION\\', \\'PREVIOUS_COUNTRY\\']\\nIn\\xa0[92]:\\ncol_imp_curr_loc = [\\'CURRENT_REGION\\', \\'CURRENT_COUNTRY\\', \\'CURCOUNTRYLAT\\', \\'CURCOUNTRYLON\\'] # current country group\\ndf_imp_curr_loc = df_imp[df_imp[col_vn].isin(col_imp_curr_loc)].sort_values(by = col_imp, ascending = False)\\ndf_imp_curr_loc\\nOut[92]:\\nVARIABLE_NAME\\nIMPORTANCE\\n14\\nCURCOUNTRYLON\\n0.009911\\n19\\nCURRENT_COUNTRY\\n0.004256\\n22\\nCURCOUNTRYLAT\\n0.003880\\n39\\nCURRENT_REGION\\n0.000000\\nIn\\xa0[93]:\\nCOL_IMP_CURRENT_REGION = df_imp_curr_loc[col_vn].to_list() # importance desc to list \\nCOL_IMP_CURRENT_REGION\\nOut[93]:\\n[\\'CURCOUNTRYLON\\', \\'CURRENT_COUNTRY\\', \\'CURCOUNTRYLAT\\', \\'CURRENT_REGION\\']\\nIn\\xa0[94]:\\ncol_imp_tenure = [\\'TENURE_MONTHS\\',\\'TENURE_INTERVAL_YEARS\\',\\'TENURE_INTERVALL_DESC\\'] # tenure group\\ndf_imp_tenure = df_imp[df_imp[col_vn].isin(col_imp_tenure)].sort_values(by = col_imp, ascending = False)\\ndf_imp_tenure\\nOut[94]:\\nVARIABLE_NAME\\nIMPORTANCE\\n17\\nTENURE_MONTHS\\n0.005014\\n25\\nTENURE_INTERVAL_YEARS\\n0.002486\\n40\\nTENURE_INTERVALL_DESC\\n0.000000\\nIn\\xa0[95]:\\nCOL_IMP_TENURE = df_imp_tenure[col_vn].to_list() # importance desc to list\\nCOL_IMP_TENURE\\nOut[95]:\\n[\\'TENURE_MONTHS\\', \\'TENURE_INTERVAL_YEARS\\', \\'TENURE_INTERVALL_DESC\\']\\nIn\\xa0[96]:\\n# function to remove list from another list\\ndef ls_remove(ls, ls_remove):\\n    for e in ls_remove:\\n        try:\\n            ls.remove(e)\\n        except ValueError:\\n            print(f\"Element {e} is not list\") # already deleted or not in found\\n    return ls\\nIn\\xa0[97]:\\ncols_imp = d_train.columns # all columns to list\\ncols_imp\\nOut[97]:\\n[\\'EMPLOYEE_ID\\',\\n \\'AGE\\',\\n \\'AGE_GROUP10\\',\\n \\'AGE_GROUP5\\',\\n \\'GENERATION\\',\\n \\'CRITICAL_JOB_ROLE\\',\\n \\'RISK_OF_LOSS\\',\\n \\'IMPACT_OF_LOSS\\',\\n \\'FUTURE_LEADER\\',\\n \\'GENDER\\',\\n \\'MGR_EMP\\',\\n \\'MINORITY\\',\\n \\'TENURE_MONTHS\\',\\n \\'TENURE_INTERVAL_YEARS\\',\\n \\'TENURE_INTERVALL_DESC\\',\\n \\'SALARY\\',\\n \\'EMPLOYMENT_TYPE\\',\\n \\'EMPLOYMENT_TYPE_2\\',\\n \\'HIGH_POTENTIAL\\',\\n \\'PREVIOUS_FUNCTIONAL_AREA\\',\\n \\'PREVIOUS_JOB_LEVEL\\',\\n \\'PREVIOUS_CAREER_PATH\\',\\n \\'PREVIOUS_PERFORMANCE_RATING\\',\\n \\'PREVIOUS_COUNTRY\\',\\n \\'PREVCOUNTRYLAT\\',\\n \\'PREVCOUNTRYLON\\',\\n \\'PREVIOUS_REGION\\',\\n \\'TIMEINPREVPOSITIONMONTH\\',\\n \\'CURRENT_FUNCTIONAL_AREA\\',\\n \\'CURRENT_JOB_LEVEL\\',\\n \\'CURRENT_CAREER_PATH\\',\\n \\'CURRENT_PERFORMANCE_RATING\\',\\n \\'CURRENT_REGION\\',\\n \\'CURRENT_COUNTRY\\',\\n \\'CURCOUNTRYLAT\\',\\n \\'CURCOUNTRYLON\\',\\n \\'PROMOTION_WITHIN_LAST_3_YEARS\\',\\n \\'CHANGED_POSITION_WITHIN_LAST_2_YEARS\\',\\n \\'CHANGE_IN_PERFORMANCE_RATING\\',\\n \\'FUNCTIONALAREACHANGETYPE\\',\\n \\'JOBLEVELCHANGETYPE\\',\\n \\'HEADS\\',\\n \\'TIMEINPREVPOS_INT\\',\\n \\'LINKEDIN\\',\\n \\'HRTRAINING\\',\\n \\'SICKDAYS\\',\\n \\'FLIGHT_RISK\\']\\nIn\\xa0[98]:\\nd_train[cols_imp].head().collect() #checking columns\\nOut[98]:\\nEMPLOYEE_ID\\nAGE\\nAGE_GROUP10\\nAGE_GROUP5\\nGENERATION\\nCRITICAL_JOB_ROLE\\nRISK_OF_LOSS\\nIMPACT_OF_LOSS\\nFUTURE_LEADER\\nGENDER\\n...\\nCHANGED_POSITION_WITHIN_LAST_2_YEARS\\nCHANGE_IN_PERFORMANCE_RATING\\nFUNCTIONALAREACHANGETYPE\\nJOBLEVELCHANGETYPE\\nHEADS\\nTIMEINPREVPOS_INT\\nLINKEDIN\\nHRTRAINING\\nSICKDAYS\\nFLIGHT_RISK\\n0\\n10032\\n33\\n(25-35]\\n(30-35]\\nGeneration Y\\nCritical\\nHigh\\nHigh\\nNo Future Leader\\nFemale\\n...\\nNo Change\\n0 - Not available\\nNo change\\nNo change\\n1\\nPREV_POS (,12]\\nYes\\nYes\\n10\\nNo\\n1 rows √ó 47 columns\\nIn\\xa0[99]:\\nd_test.shape\\nOut[99]:\\n[1912, 47]\\nIn\\xa0[100]:\\n# Reducing features by group importance\\n# COL_IMP_ZERO\\n# COL_IMP_AGE\\n# COL_IMP_PREVIOUS_COUNTRY\\n# COL_IMP_CURRENT_REGION\\n# COL_IMP_TENURE\\n# COL_IMP_PREVIOUS_COUNTRY = [\\'PREVIOUS_COUNTRY\\',\\'PREVCOUNTRYLAT\\', \\'PREVCOUNTRYLON\\', \\'PREVIOUS_REGION\\']\\n# COL_IMP_CURRENT_REGION = [\\'CURRENT_COUNTRY\\',\\'CURRENT_REGION\\', \\'CURCOUNTRYLAT\\', \\'CURCOUNTRYLON\\']\\ncols_imp = d_train.columns\\ncols_imp = ls_remove(cols_imp, COL_IMP_ZERO) # remove zero importance features\\ncols_imp = ls_remove(cols_imp, COL_IMP_AGE[1:]) # keep first\\ncols_imp = ls_remove(cols_imp, COL_IMP_PREVIOUS_COUNTRY[1:]) # keep first\\ncols_imp = ls_remove(cols_imp, COL_IMP_CURRENT_REGION[1:]) # keep first\\ncols_imp = ls_remove(cols_imp, COL_IMP_TENURE[1:]) # keep first\\n# printed element - already removed or not in list\\nlen(cols_imp)\\nElement GENERATION is not list\\nElement CURRENT_REGION is not list\\nElement TENURE_INTERVALL_DESC is not list\\nOut[100]:\\n31\\nIn\\xa0[101]:\\ncols_imp\\nOut[101]:\\n[\\'EMPLOYEE_ID\\',\\n \\'AGE\\',\\n \\'CRITICAL_JOB_ROLE\\',\\n \\'RISK_OF_LOSS\\',\\n \\'IMPACT_OF_LOSS\\',\\n \\'FUTURE_LEADER\\',\\n \\'GENDER\\',\\n \\'MGR_EMP\\',\\n \\'TENURE_MONTHS\\',\\n \\'SALARY\\',\\n \\'EMPLOYMENT_TYPE_2\\',\\n \\'HIGH_POTENTIAL\\',\\n \\'PREVIOUS_FUNCTIONAL_AREA\\',\\n \\'PREVIOUS_CAREER_PATH\\',\\n \\'PREVIOUS_PERFORMANCE_RATING\\',\\n \\'PREVCOUNTRYLON\\',\\n \\'TIMEINPREVPOSITIONMONTH\\',\\n \\'CURRENT_FUNCTIONAL_AREA\\',\\n \\'CURRENT_JOB_LEVEL\\',\\n \\'CURRENT_PERFORMANCE_RATING\\',\\n \\'CURCOUNTRYLON\\',\\n \\'PROMOTION_WITHIN_LAST_3_YEARS\\',\\n \\'CHANGED_POSITION_WITHIN_LAST_2_YEARS\\',\\n \\'CHANGE_IN_PERFORMANCE_RATING\\',\\n \\'FUNCTIONALAREACHANGETYPE\\',\\n \\'JOBLEVELCHANGETYPE\\',\\n \\'TIMEINPREVPOS_INT\\',\\n \\'LINKEDIN\\',\\n \\'HRTRAINING\\',\\n \\'SICKDAYS\\',\\n \\'FLIGHT_RISK\\']\\nIn\\xa0[102]:\\n%%time\\n# Fit model with important features\\nHGBT_MODEL = model_fit(d_train[cols_imp])\\nd_train[cols_imp].shape\\nCPU times: total: 0 ns\\nWall time: 36.2 s\\nOut[102]:\\n[15292, 31]\\nIn\\xa0[103]:\\nHGBT_MODEL.statistics_.collect()\\nOut[103]:\\nSTAT_NAME\\nSTAT_VALUE\\nCLASS_NAME\\n0\\nAUC\\n0.9856558358759214\\nNone\\n1\\nRECALL\\n0.9753857457751653\\nNo\\n2\\nPRECISION\\n0.9567567567567568\\nNo\\n3\\nF1_SCORE\\n0.9659814444242315\\nNo\\n4\\nSUPPORT\\n2722\\nNo\\n5\\nRECALL\\n0.6428571428571429\\nYes\\n6\\nPRECISION\\n0.7632508833922261\\nYes\\n7\\nF1_SCORE\\n0.6978998384491115\\nYes\\n8\\nSUPPORT\\n336\\nYes\\n9\\nACCURACY\\n0.9388489208633094\\nNone\\n10\\nKAPPA\\n0.664158531672154\\nNone\\n11\\nMCC\\n0.667187832085567\\nNone\\nIn\\xa0[104]:\\nf_score_res(HGBT_MODEL, d_test[cols_imp])[1].collect()\\nOut[104]:\\nSTAT_NAME\\nSTAT_VALUE\\nCLASS_NAME\\n0\\nAUC\\n0.9857221949895835\\nNone\\n1\\nRECALL\\n0.9782608695652174\\nNo\\n2\\nPRECISION\\n0.9541547277936963\\nNo\\n3\\nF1_SCORE\\n0.9660574412532638\\nNo\\n4\\nSUPPORT\\n1702\\nNo\\n5\\nRECALL\\n0.6190476190476191\\nYes\\n6\\nPRECISION\\n0.7784431137724551\\nYes\\n7\\nF1_SCORE\\n0.689655172413793\\nYes\\n8\\nSUPPORT\\n210\\nYes\\n9\\nACCURACY\\n0.9388075313807531\\nNone\\n10\\nKAPPA\\n0.6562017815099188\\nNone\\n11\\nMCC\\n0.6615035219157219\\nNone\\nIn\\xa0[105]:\\ndf_model_stat_last, df_pred_stat_last = f_stat_all(HGBT_MODEL, d_test[cols_imp], \\'cols_imp\\')\\nIn\\xa0[106]:\\ndf_model_stat_last\\nOut[106]:\\nSTAT_NAME\\nCLASS_NAME\\nINIT\\nADD\\ncols_imp\\n0\\nAUC\\nNone\\n0.9743\\n0.9830\\n0.9856\\n1\\nRECALL\\nNo\\n0.9959\\n0.9790\\n0.9753\\n2\\nPRECISION\\nNo\\n0.9249\\n0.9531\\n0.9567\\n3\\nF1_SCORE\\nNo\\n0.9591\\n0.9659\\n0.9659\\n4\\nSUPPORT\\nNo\\n2722\\n2722\\n2722\\n5\\nRECALL\\nYes\\n0.3452\\n0.6101\\n0.6428\\n6\\nPRECISION\\nYes\\n0.9133\\n0.7824\\n0.7632\\n7\\nF1_SCORE\\nYes\\n0.5010\\n0.6856\\n0.6978\\n8\\nSUPPORT\\nYes\\n336\\n336\\n336\\n9\\nACCURACY\\nNone\\n0.9244\\n0.9385\\n0.9388\\n10\\nKAPPA\\nNone\\n0.4690\\n0.6521\\n0.6641\\n11\\nMCC\\nNone\\n0.5348\\n0.6583\\n0.6671\\nIn\\xa0[107]:\\ndf_pred_stat_last\\nOut[107]:\\nSTAT_NAME\\nCLASS_NAME\\nINIT\\nADD\\ncols_imp\\n0\\nAUC\\nNone\\n0.9733\\n0.9834\\n0.9857\\n1\\nRECALL\\nNo\\n0.9964\\n0.9800\\n0.9782\\n2\\nPRECISION\\nNo\\n0.9247\\n0.9488\\n0.9541\\n3\\nF1_SCORE\\nNo\\n0.9592\\n0.9641\\n0.9660\\n4\\nSUPPORT\\nNo\\n1702\\n1702\\n1702\\n5\\nRECALL\\nYes\\n0.3428\\n0.5714\\n0.6190\\n6\\nPRECISION\\nYes\\n0.9230\\n0.7792\\n0.7784\\n7\\nF1_SCORE\\nYes\\n0.5000\\n0.6593\\n0.6896\\n8\\nSUPPORT\\nYes\\n210\\n210\\n210\\n9\\nACCURACY\\nNone\\n0.9246\\n0.9351\\n0.9388\\n10\\nKAPPA\\nNone\\n0.4683\\n0.6244\\n0.6562\\n11\\nMCC\\nNone\\n0.5363\\n0.6336\\n0.6615\\nIn\\xa0[108]:\\nd_test[cols_imp].shape\\nOut[108]:\\n[1912, 31]\\nIn\\xa0[109]:\\n# Predicted RECALL Yes ~ 0.65 > 0.63.\\n# Reducing features increases a bit the prediction\\nIn\\xa0[110]:\\n# Reducing features with meaning preference\\nCOL_IMP_PREVIOUS_COUNTRY_MEANING = [\\'PREVIOUS_COUNTRY\\',\\'PREVCOUNTRYLAT\\', \\'PREVCOUNTRYLON\\', \\'PREVIOUS_REGION\\']\\nCOL_IMP_CURRENT_REGION_MEANING = [\\'CURRENT_COUNTRY\\',\\'CURRENT_REGION\\', \\'CURCOUNTRYLAT\\', \\'CURCOUNTRYLON\\']\\ncols_imp_mn = d_train.columns\\ncols_imp_mn = ls_remove(cols_imp_mn, COL_IMP_ZERO)\\ncols_imp_mn = ls_remove(cols_imp_mn, COL_IMP_AGE[1:])\\ncols_imp_mn = ls_remove(cols_imp_mn, COL_IMP_PREVIOUS_COUNTRY_MEANING[1:])\\ncols_imp_mn = ls_remove(cols_imp_mn, COL_IMP_CURRENT_REGION_MEANING[1:])\\ncols_imp_mn = ls_remove(cols_imp_mn, COL_IMP_TENURE[1:])\\n# printed element - already removed or not in list\\nlen(cols_imp_mn)\\nElement GENERATION is not list\\nElement CURRENT_REGION is not list\\nElement TENURE_INTERVALL_DESC is not list\\nOut[110]:\\n31\\nIn\\xa0[111]:\\n%%time\\n# Fit model with important features by meaning preference\\nHGBT_MODEL = model_fit(d_train[cols_imp_mn])\\nCPU times: total: 0 ns\\nWall time: 34.3 s\\nIn\\xa0[112]:\\nHGBT_MODEL.statistics_.collect() # meaning preference\\nOut[112]:\\nSTAT_NAME\\nSTAT_VALUE\\nCLASS_NAME\\n0\\nAUC\\n0.9843125559009359\\nNone\\n1\\nRECALL\\n0.9739162380602499\\nNo\\n2\\nPRECISION\\n0.9529115744069016\\nNo\\n3\\nF1_SCORE\\n0.9632994186046512\\nNo\\n4\\nSUPPORT\\n2722\\nNo\\n5\\nRECALL\\n0.6101190476190477\\nYes\\n6\\nPRECISION\\n0.7427536231884058\\nYes\\n7\\nF1_SCORE\\n0.6699346405228759\\nYes\\n8\\nSUPPORT\\n336\\nYes\\n9\\nACCURACY\\n0.933943754087639\\nNone\\n10\\nKAPPA\\n0.6336256186151561\\nNone\\n11\\nMCC\\n0.6374111878644116\\nNone\\nIn\\xa0[113]:\\nd_train[cols_imp_mn].shape\\nOut[113]:\\n[15292, 31]\\nIn\\xa0[114]:\\nf_score_res(HGBT_MODEL, d_test[cols_imp_mn])[1].collect() # meaning preference\\nOut[114]:\\nSTAT_NAME\\nSTAT_VALUE\\nCLASS_NAME\\n0\\nAUC\\n0.9870257600094536\\nNone\\n1\\nRECALL\\n0.981198589894242\\nNo\\n2\\nPRECISION\\n0.9548313321898227\\nNo\\n3\\nF1_SCORE\\n0.9678354100260794\\nNo\\n4\\nSUPPORT\\n1702\\nNo\\n5\\nRECALL\\n0.6238095238095238\\nYes\\n6\\nPRECISION\\n0.803680981595092\\nYes\\n7\\nF1_SCORE\\n0.7024128686327078\\nYes\\n8\\nSUPPORT\\n210\\nYes\\n9\\nACCURACY\\n0.9419456066945606\\nNone\\n10\\nKAPPA\\n0.6708131952673736\\nNone\\n11\\nMCC\\n0.6774260876170111\\nNone\\nIn\\xa0[115]:\\ndf_model_stat_last, df_pred_stat_last = f_stat_all(HGBT_MODEL, d_test[cols_imp_mn], \\'cols_imp_mn\\')\\nIn\\xa0[116]:\\ndf_model_stat_last\\nOut[116]:\\nSTAT_NAME\\nCLASS_NAME\\nINIT\\nADD\\ncols_imp\\ncols_imp_mn\\n0\\nAUC\\nNone\\n0.9743\\n0.9830\\n0.9856\\n0.9843\\n1\\nRECALL\\nNo\\n0.9959\\n0.9790\\n0.9753\\n0.9739\\n2\\nPRECISION\\nNo\\n0.9249\\n0.9531\\n0.9567\\n0.9529\\n3\\nF1_SCORE\\nNo\\n0.9591\\n0.9659\\n0.9659\\n0.9632\\n4\\nSUPPORT\\nNo\\n2722\\n2722\\n2722\\n2722\\n5\\nRECALL\\nYes\\n0.3452\\n0.6101\\n0.6428\\n0.6101\\n6\\nPRECISION\\nYes\\n0.9133\\n0.7824\\n0.7632\\n0.7427\\n7\\nF1_SCORE\\nYes\\n0.5010\\n0.6856\\n0.6978\\n0.6699\\n8\\nSUPPORT\\nYes\\n336\\n336\\n336\\n336\\n9\\nACCURACY\\nNone\\n0.9244\\n0.9385\\n0.9388\\n0.9339\\n10\\nKAPPA\\nNone\\n0.4690\\n0.6521\\n0.6641\\n0.6336\\n11\\nMCC\\nNone\\n0.5348\\n0.6583\\n0.6671\\n0.6374\\nIn\\xa0[117]:\\ndf_pred_stat_last\\nOut[117]:\\nSTAT_NAME\\nCLASS_NAME\\nINIT\\nADD\\ncols_imp\\ncols_imp_mn\\n0\\nAUC\\nNone\\n0.9733\\n0.9834\\n0.9857\\n0.9870\\n1\\nRECALL\\nNo\\n0.9964\\n0.9800\\n0.9782\\n0.9811\\n2\\nPRECISION\\nNo\\n0.9247\\n0.9488\\n0.9541\\n0.9548\\n3\\nF1_SCORE\\nNo\\n0.9592\\n0.9641\\n0.9660\\n0.9678\\n4\\nSUPPORT\\nNo\\n1702\\n1702\\n1702\\n1702\\n5\\nRECALL\\nYes\\n0.3428\\n0.5714\\n0.6190\\n0.6238\\n6\\nPRECISION\\nYes\\n0.9230\\n0.7792\\n0.7784\\n0.8036\\n7\\nF1_SCORE\\nYes\\n0.5000\\n0.6593\\n0.6896\\n0.7024\\n8\\nSUPPORT\\nYes\\n210\\n210\\n210\\n210\\n9\\nACCURACY\\nNone\\n0.9246\\n0.9351\\n0.9388\\n0.9419\\n10\\nKAPPA\\nNone\\n0.4683\\n0.6244\\n0.6562\\n0.6708\\n11\\nMCC\\nNone\\n0.5363\\n0.6336\\n0.6615\\n0.6774\\nIn\\xa0[118]:\\n# Model RECALL Yes ~ 0.64 almost same\\nIn\\xa0[119]:\\n# Predicted RECALL Yes ~ almost same\\n# cols_imp_mn - prefered to use countries in a natural way\\ncols_imp_original = cols_imp # keep for tests\\ncols_imp = cols_imp_mn # swap \\nIn\\xa0[120]:\\n# pred_res.select(tab_id, \\'SCORE\\', \\'CONFIDENCE\\', \\'REASON_CODE\\', \\n# # pred_res.select(\\'ID\\', \\'SCORE\\', \\'CONFIDENCE\\', \\'REASON_CODE\\', \\n#                 (\\'json_query(\"REASON_CODE\", \\\\\\'$[0].attr\\\\\\')\\', \\'Top1\\'), \\n#                 (\\'json_query(\"REASON_CODE\", \\\\\\'$[0].pct\\\\\\')\\', \\'PCT_1\\'), \\n#                 (\\'json_query(\"REASON_CODE\", \\\\\\'$[1].attr\\\\\\')\\', \\'Top2\\'), \\n#                 (\\'json_query(\"REASON_CODE\", \\\\\\'$[1].pct\\\\\\')\\', \\'PCT_2\\') ).head(5).collect()\\nSMOTE Synthetic Minority Over-sampling Technique¬∂\\nIn\\xa0[121]:\\nsm = hana_ml.algorithms.pal.preprocessing.SMOTE\\nIn\\xa0[122]:\\n# d_train.agg([(\\'count\\', target, \\'Count\\')], group_by=target).collect()\\nIn\\xa0[123]:\\n# return hana dfh group by class count    \\ndef f_class_count(dfh):\\n#     global target\\n    dfh_group_by_class = dfh.agg([(\\'count\\', target, \\'Count\\')], group_by=target)\\n    return dfh_group_by_class\\nIn\\xa0[124]:\\nf_class_count(d_train).collect()\\nOut[124]:\\nFLIGHT_RISK\\nCount\\n0\\nNo\\n13612\\n1\\nYes\\n1680\\nIn\\xa0[125]:\\nd_train.shape\\nOut[125]:\\n[15292, 47]\\nIn\\xa0[126]:\\n# return max smote amount\\ndef f_smote_nmax(dfh):\\n#     global target\\n    df_class_count = f_class_count(dfh).collect() #class count\\n    class_no = df_class_count.loc[df_class_count[target] == \\'No\\'][\\'Count\\'].values[0] #class No\\n    class_yes = df_class_count.loc[df_class_count[target] == \\'Yes\\'][\\'Count\\'].values[0] #class Yes\\n    smote_nmax = int((class_no / class_yes)*100) # Majority No:Minority Yes\\n    smote_nmax -= 100 # less 100%\\n    return smote_nmax\\nIn\\xa0[127]:\\nf_smote_nmax(d_train)\\nOut[127]:\\n710\\nIn\\xa0[128]:\\n# return hana dfh for given smote_amount    \\ndef f_smote(dfh,smote_amount):\\n#     global target\\n    churn_minority_class = \\'Yes\\'\\n    sm_par = sm(smote_amount=smote_amount, k_nearest_neighbours=2,\\n                  search_method=\\'kd-tree\\')\\n    dfh_sm = sm_par.fit_transform(data=dfh, label = target, minority_class=churn_minority_class)\\n    return dfh_sm\\nIn\\xa0[129]:\\nd_train_sm = f_smote(d_train,f_smote_nmax(d_train)) # generate smote with max amount\\nd_train_sm.shape, d_train.shape\\nOut[129]:\\n([27220, 47], [15292, 47])\\nIn\\xa0[130]:\\nf_class_count(d_train_sm).collect() #check classes count for Majority No:Minority Yes = 1:1\\nOut[130]:\\nFLIGHT_RISK\\nCount\\n0\\nNo\\n13612\\n1\\nYes\\n13608\\nIn\\xa0[131]:\\nf_class_count(d_test).collect() #check classes count for Majority No > Minority Yes\\nOut[131]:\\nFLIGHT_RISK\\nCount\\n0\\nNo\\n1702\\n1\\nYes\\n210\\nIn\\xa0[132]:\\n# return hana dfh with reindexed \\'EMPLOYEE_ID\\'\\ndef f_employee_id_unique(dfh):\\n    dfh = dfh.add_id(id_col=\\'ID\\')\\n    dfh = dfh.drop([\\'EMPLOYEE_ID\\'])\\n    dfh = dfh.rename_columns({\\'ID\\':\\'EMPLOYEE_ID\\' })\\n    return dfh\\nIn\\xa0[133]:\\nd_train_sm = f_employee_id_unique(d_train_sm) # all features\\nd_train_sm.shape\\nOut[133]:\\n[27220, 47]\\nIn\\xa0[134]:\\n%%time\\n# Fit with smote balanced No:Yes with all features\\nHGBT_MODEL=model_fit(d_train_sm)\\nCPU times: total: 0 ns\\nWall time: 1min 32s\\nIn\\xa0[135]:\\nHGBT_MODEL.statistics_.collect()\\nOut[135]:\\nSTAT_NAME\\nSTAT_VALUE\\nCLASS_NAME\\n0\\nAUC\\n0.9868342204185991\\nNone\\n1\\nRECALL\\n0.9320352681851579\\nNo\\n2\\nPRECISION\\n0.9606209769026883\\nNo\\n3\\nF1_SCORE\\n0.9461122506060041\\nNo\\n4\\nSUPPORT\\n2722\\nNo\\n5\\nRECALL\\n0.9617927994121969\\nYes\\n6\\nPRECISION\\n0.9339992864787727\\nYes\\n7\\nF1_SCORE\\n0.9476923076923077\\nYes\\n8\\nSUPPORT\\n2722\\nYes\\n9\\nACCURACY\\n0.9469140337986774\\nNone\\n10\\nKAPPA\\n0.8938280675973549\\nNone\\n11\\nMCC\\n0.8942240777633356\\nNone\\nIn\\xa0[136]:\\nd_train_sm.shape\\nOut[136]:\\n[27220, 47]\\nIn\\xa0[137]:\\nf_score_res(HGBT_MODEL, d_test)[1].collect()\\nOut[137]:\\nSTAT_NAME\\nSTAT_VALUE\\nCLASS_NAME\\n0\\nAUC\\n0.9784333913972094\\nNone\\n1\\nRECALL\\n0.945358401880141\\nNo\\n2\\nPRECISION\\n0.9751515151515151\\nNo\\n3\\nF1_SCORE\\n0.9600238663484487\\nNo\\n4\\nSUPPORT\\n1702\\nNo\\n5\\nRECALL\\n0.8047619047619048\\nYes\\n6\\nPRECISION\\n0.6450381679389313\\nYes\\n7\\nF1_SCORE\\n0.7161016949152541\\nYes\\n8\\nSUPPORT\\n210\\nYes\\n9\\nACCURACY\\n0.9299163179916318\\nNone\\n10\\nKAPPA\\n0.6766781420047853\\nNone\\n11\\nMCC\\n0.682068086964959\\nNone\\nIn\\xa0[138]:\\nd_test.shape # smote all features\\nOut[138]:\\n[1912, 47]\\nIn\\xa0[139]:\\ndf_model_stat_last, df_pred_stat_last = f_stat_all(HGBT_MODEL, d_test, \\'sm_max_cols_all\\')\\nIn\\xa0[140]:\\ndf_model_stat_last\\nOut[140]:\\nSTAT_NAME\\nCLASS_NAME\\nINIT\\nADD\\ncols_imp\\ncols_imp_mn\\nsm_max_cols_all\\n0\\nAUC\\nNone\\n0.9743\\n0.9830\\n0.9856\\n0.9843\\n0.9868\\n1\\nRECALL\\nNo\\n0.9959\\n0.9790\\n0.9753\\n0.9739\\n0.9320\\n2\\nPRECISION\\nNo\\n0.9249\\n0.9531\\n0.9567\\n0.9529\\n0.9606\\n3\\nF1_SCORE\\nNo\\n0.9591\\n0.9659\\n0.9659\\n0.9632\\n0.9461\\n4\\nSUPPORT\\nNo\\n2722\\n2722\\n2722\\n2722\\n2722\\n5\\nRECALL\\nYes\\n0.3452\\n0.6101\\n0.6428\\n0.6101\\n0.9617\\n6\\nPRECISION\\nYes\\n0.9133\\n0.7824\\n0.7632\\n0.7427\\n0.9339\\n7\\nF1_SCORE\\nYes\\n0.5010\\n0.6856\\n0.6978\\n0.6699\\n0.9476\\n8\\nSUPPORT\\nYes\\n336\\n336\\n336\\n336\\n2722\\n9\\nACCURACY\\nNone\\n0.9244\\n0.9385\\n0.9388\\n0.9339\\n0.9469\\n10\\nKAPPA\\nNone\\n0.4690\\n0.6521\\n0.6641\\n0.6336\\n0.8938\\n11\\nMCC\\nNone\\n0.5348\\n0.6583\\n0.6671\\n0.6374\\n0.8942\\nIn\\xa0[141]:\\ndf_pred_stat_last\\nOut[141]:\\nSTAT_NAME\\nCLASS_NAME\\nINIT\\nADD\\ncols_imp\\ncols_imp_mn\\nsm_max_cols_all\\n0\\nAUC\\nNone\\n0.9733\\n0.9834\\n0.9857\\n0.9870\\n0.9784\\n1\\nRECALL\\nNo\\n0.9964\\n0.9800\\n0.9782\\n0.9811\\n0.9453\\n2\\nPRECISION\\nNo\\n0.9247\\n0.9488\\n0.9541\\n0.9548\\n0.9751\\n3\\nF1_SCORE\\nNo\\n0.9592\\n0.9641\\n0.9660\\n0.9678\\n0.9600\\n4\\nSUPPORT\\nNo\\n1702\\n1702\\n1702\\n1702\\n1702\\n5\\nRECALL\\nYes\\n0.3428\\n0.5714\\n0.6190\\n0.6238\\n0.8047\\n6\\nPRECISION\\nYes\\n0.9230\\n0.7792\\n0.7784\\n0.8036\\n0.6450\\n7\\nF1_SCORE\\nYes\\n0.5000\\n0.6593\\n0.6896\\n0.7024\\n0.7161\\n8\\nSUPPORT\\nYes\\n210\\n210\\n210\\n210\\n210\\n9\\nACCURACY\\nNone\\n0.9246\\n0.9351\\n0.9388\\n0.9419\\n0.9299\\n10\\nKAPPA\\nNone\\n0.4683\\n0.6244\\n0.6562\\n0.6708\\n0.6766\\n11\\nMCC\\nNone\\n0.5363\\n0.6336\\n0.6615\\n0.6774\\n0.6820\\nIn\\xa0[142]:\\n# Predicted RECALL up\\n# Predicted PRECISION ~ not reliable SUPPORT Yes < SUPPORT No\\nIn\\xa0[143]:\\nd_train_sm[cols_imp].shape, d_train_sm.shape\\nOut[143]:\\n([27220, 31], [27220, 47])\\nIn\\xa0[144]:\\n%%time\\n# Fit with smote balanced No:Yes with important features\\nHGBT_MODEL=model_fit(d_train_sm[cols_imp])\\nCPU times: total: 0 ns\\nWall time: 59.5 s\\nIn\\xa0[145]:\\nHGBT_MODEL.statistics_.collect()\\nOut[145]:\\nSTAT_NAME\\nSTAT_VALUE\\nCLASS_NAME\\n0\\nAUC\\n0.9776841966106307\\nNone\\n1\\nRECALL\\n0.9015429831006613\\nNo\\n2\\nPRECISION\\n0.945664739884393\\nNo\\n3\\nF1_SCORE\\n0.923076923076923\\nNo\\n4\\nSUPPORT\\n2722\\nNo\\n5\\nRECALL\\n0.9481998530492285\\nYes\\n6\\nPRECISION\\n0.905931905931906\\nYes\\n7\\nF1_SCORE\\n0.9265840962125291\\nYes\\n8\\nSUPPORT\\n2722\\nYes\\n9\\nACCURACY\\n0.9248714180749449\\nNone\\n10\\nKAPPA\\n0.8497428361498898\\nNone\\n11\\nMCC\\n0.8506692359969737\\nNone\\nIn\\xa0[146]:\\nf_score_res(HGBT_MODEL,d_test[cols_imp])[1].collect()\\nOut[146]:\\nSTAT_NAME\\nSTAT_VALUE\\nCLASS_NAME\\n0\\nAUC\\n0.9709726665762154\\nNone\\n1\\nRECALL\\n0.9195064629847238\\nNo\\n2\\nPRECISION\\n0.9726538222498446\\nNo\\n3\\nF1_SCORE\\n0.9453337360314105\\nNo\\n4\\nSUPPORT\\n1702\\nNo\\n5\\nRECALL\\n0.7904761904761904\\nYes\\n6\\nPRECISION\\n0.5478547854785478\\nYes\\n7\\nF1_SCORE\\n0.6471734892787523\\nYes\\n8\\nSUPPORT\\n210\\nYes\\n9\\nACCURACY\\n0.9053347280334728\\nNone\\n10\\nKAPPA\\n0.5945716709075488\\nNone\\n11\\nMCC\\n0.6079079555855889\\nNone\\nIn\\xa0[147]:\\n[cols_imp]\\nOut[147]:\\n[[\\'EMPLOYEE_ID\\',\\n  \\'AGE\\',\\n  \\'CRITICAL_JOB_ROLE\\',\\n  \\'RISK_OF_LOSS\\',\\n  \\'IMPACT_OF_LOSS\\',\\n  \\'FUTURE_LEADER\\',\\n  \\'GENDER\\',\\n  \\'MGR_EMP\\',\\n  \\'TENURE_MONTHS\\',\\n  \\'SALARY\\',\\n  \\'EMPLOYMENT_TYPE_2\\',\\n  \\'HIGH_POTENTIAL\\',\\n  \\'PREVIOUS_FUNCTIONAL_AREA\\',\\n  \\'PREVIOUS_CAREER_PATH\\',\\n  \\'PREVIOUS_PERFORMANCE_RATING\\',\\n  \\'PREVIOUS_COUNTRY\\',\\n  \\'TIMEINPREVPOSITIONMONTH\\',\\n  \\'CURRENT_FUNCTIONAL_AREA\\',\\n  \\'CURRENT_JOB_LEVEL\\',\\n  \\'CURRENT_PERFORMANCE_RATING\\',\\n  \\'CURRENT_COUNTRY\\',\\n  \\'PROMOTION_WITHIN_LAST_3_YEARS\\',\\n  \\'CHANGED_POSITION_WITHIN_LAST_2_YEARS\\',\\n  \\'CHANGE_IN_PERFORMANCE_RATING\\',\\n  \\'FUNCTIONALAREACHANGETYPE\\',\\n  \\'JOBLEVELCHANGETYPE\\',\\n  \\'TIMEINPREVPOS_INT\\',\\n  \\'LINKEDIN\\',\\n  \\'HRTRAINING\\',\\n  \\'SICKDAYS\\',\\n  \\'FLIGHT_RISK\\']]\\nIn\\xa0[148]:\\nf_score_res(HGBT_MODEL,d_test[cols_imp])[1].collect()\\nOut[148]:\\nSTAT_NAME\\nSTAT_VALUE\\nCLASS_NAME\\n0\\nAUC\\n0.9709726665762154\\nNone\\n1\\nRECALL\\n0.9195064629847238\\nNo\\n2\\nPRECISION\\n0.9726538222498446\\nNo\\n3\\nF1_SCORE\\n0.9453337360314105\\nNo\\n4\\nSUPPORT\\n1702\\nNo\\n5\\nRECALL\\n0.7904761904761904\\nYes\\n6\\nPRECISION\\n0.5478547854785478\\nYes\\n7\\nF1_SCORE\\n0.6471734892787523\\nYes\\n8\\nSUPPORT\\n210\\nYes\\n9\\nACCURACY\\n0.9053347280334728\\nNone\\n10\\nKAPPA\\n0.5945716709075488\\nNone\\n11\\nMCC\\n0.6079079555855889\\nNone\\nIn\\xa0[149]:\\nd_test[cols_imp].shape, d_test.shape\\nOut[149]:\\n([1912, 31], [1912, 47])\\nIn\\xa0[150]:\\n# f_score_res(HGBT_MODEL,d_test)[1].collect() # QQQ\\nIn\\xa0[151]:\\ndf_model_stat_last, df_pred_stat_last = f_stat_all(HGBT_MODEL, d_test[cols_imp], \\'sm_max_cols_imp\\')\\nIn\\xa0[152]:\\ndf_model_stat_last\\nOut[152]:\\nSTAT_NAME\\nCLASS_NAME\\nINIT\\nADD\\ncols_imp\\ncols_imp_mn\\nsm_max_cols_all\\nsm_max_cols_imp\\n0\\nAUC\\nNone\\n0.9743\\n0.9830\\n0.9856\\n0.9843\\n0.9868\\n0.9776\\n1\\nRECALL\\nNo\\n0.9959\\n0.9790\\n0.9753\\n0.9739\\n0.9320\\n0.9015\\n2\\nPRECISION\\nNo\\n0.9249\\n0.9531\\n0.9567\\n0.9529\\n0.9606\\n0.9456\\n3\\nF1_SCORE\\nNo\\n0.9591\\n0.9659\\n0.9659\\n0.9632\\n0.9461\\n0.9230\\n4\\nSUPPORT\\nNo\\n2722\\n2722\\n2722\\n2722\\n2722\\n2722\\n5\\nRECALL\\nYes\\n0.3452\\n0.6101\\n0.6428\\n0.6101\\n0.9617\\n0.9481\\n6\\nPRECISION\\nYes\\n0.9133\\n0.7824\\n0.7632\\n0.7427\\n0.9339\\n0.9059\\n7\\nF1_SCORE\\nYes\\n0.5010\\n0.6856\\n0.6978\\n0.6699\\n0.9476\\n0.9265\\n8\\nSUPPORT\\nYes\\n336\\n336\\n336\\n336\\n2722\\n2722\\n9\\nACCURACY\\nNone\\n0.9244\\n0.9385\\n0.9388\\n0.9339\\n0.9469\\n0.9248\\n10\\nKAPPA\\nNone\\n0.4690\\n0.6521\\n0.6641\\n0.6336\\n0.8938\\n0.8497\\n11\\nMCC\\nNone\\n0.5348\\n0.6583\\n0.6671\\n0.6374\\n0.8942\\n0.8506\\nIn\\xa0[153]:\\n# SUPPORT Yes = SUPPORT No -> all statistics are reliable\\n# F1_SCORE Yes ~ bit up\\n# ACCURACY ~ bit up\\nIn\\xa0[154]:\\ndf_pred_stat_last\\nOut[154]:\\nSTAT_NAME\\nCLASS_NAME\\nINIT\\nADD\\ncols_imp\\ncols_imp_mn\\nsm_max_cols_all\\nsm_max_cols_imp\\n0\\nAUC\\nNone\\n0.9733\\n0.9834\\n0.9857\\n0.9870\\n0.9784\\n0.9709\\n1\\nRECALL\\nNo\\n0.9964\\n0.9800\\n0.9782\\n0.9811\\n0.9453\\n0.9195\\n2\\nPRECISION\\nNo\\n0.9247\\n0.9488\\n0.9541\\n0.9548\\n0.9751\\n0.9726\\n3\\nF1_SCORE\\nNo\\n0.9592\\n0.9641\\n0.9660\\n0.9678\\n0.9600\\n0.9453\\n4\\nSUPPORT\\nNo\\n1702\\n1702\\n1702\\n1702\\n1702\\n1702\\n5\\nRECALL\\nYes\\n0.3428\\n0.5714\\n0.6190\\n0.6238\\n0.8047\\n0.7904\\n6\\nPRECISION\\nYes\\n0.9230\\n0.7792\\n0.7784\\n0.8036\\n0.6450\\n0.5478\\n7\\nF1_SCORE\\nYes\\n0.5000\\n0.6593\\n0.6896\\n0.7024\\n0.7161\\n0.6471\\n8\\nSUPPORT\\nYes\\n210\\n210\\n210\\n210\\n210\\n210\\n9\\nACCURACY\\nNone\\n0.9246\\n0.9351\\n0.9388\\n0.9419\\n0.9299\\n0.9053\\n10\\nKAPPA\\nNone\\n0.4683\\n0.6244\\n0.6562\\n0.6708\\n0.6766\\n0.5945\\n11\\nMCC\\nNone\\n0.5363\\n0.6336\\n0.6615\\n0.6774\\n0.6820\\n0.6079\\nIn\\xa0[155]:\\n# SUPPORT   Yes < SUPPORT No\\n# RECALL    Yes bit up\\n# PRECISION Yes - not reliable\\nIn\\xa0[156]:\\n#<<<<<<<<<<<<<<<<<<< d_test imbalanced classes\\n# Balance d_test and increase d_train\\n# test_classes = d_test.agg([(\\'count\\', target, \\'Count\\')], group_by=target).collect()\\ntest_classes = f_class_count(d_test).collect()\\ntest_classes\\nOut[156]:\\nFLIGHT_RISK\\nCount\\n0\\nNo\\n1702\\n1\\nYes\\n210\\nIn\\xa0[157]:\\ntest_class_yes = test_classes.loc[test_classes[target] == \\'Yes\\'][\\'Count\\'].values[0]\\ntest_class_no = test_classes.loc[test_classes[target] == \\'No\\'][\\'Count\\'].values[0]\\ntest_class_no, test_class_yes, test_class_no - test_class_yes  # No, Yes, No - Yeas\\nOut[157]:\\n(1702, 210, 1492)\\nIn\\xa0[158]:\\nd_train.shape, d_test.shape # train and test\\nOut[158]:\\n([15292, 47], [1912, 47])\\nIn\\xa0[159]:\\nd_test_no = d_test.filter(\"FLIGHT_RISK = \\'No\\'\") # all No\\nd_test_yes = d_test.filter(\"FLIGHT_RISK = \\'Yes\\'\") # all Yes\\nd_test_no_eq_yes = d_test_no.head(test_class_yes) # select No head (Yes rows)\\nd_test_no_rest = d_test_no.tail(test_class_no-test_class_yes) # select No tail (No rows- Yes rows)\\nd_test_bl = d_test_yes.union(d_test_no_eq_yes, all=True)\\nIn\\xa0[160]:\\n# d_test_no.agg([(\\'count\\', target, \\'Count\\')], group_by=target).collect()\\nf_class_count(d_test_no).collect()\\nOut[160]:\\nFLIGHT_RISK\\nCount\\n0\\nNo\\n1702\\nIn\\xa0[161]:\\n# d_test_yes.agg([(\\'count\\', target, \\'Count\\')], group_by=target).collect()\\nf_class_count(d_test_yes).collect()\\nOut[161]:\\nFLIGHT_RISK\\nCount\\n0\\nYes\\n210\\nIn\\xa0[162]:\\n# d_test_no_eq_yes.agg([(\\'count\\', target, \\'Count\\')], group_by=target).collect()\\nf_class_count(d_test_no_eq_yes).collect()\\nOut[162]:\\nFLIGHT_RISK\\nCount\\n0\\nNo\\n210\\nIn\\xa0[163]:\\n# d_test_no_rest.agg([(\\'count\\', target, \\'Count\\')], group_by=target).collect()\\nf_class_count(d_test_no_rest).collect()\\nOut[163]:\\nFLIGHT_RISK\\nCount\\n0\\nNo\\n1492\\nIn\\xa0[164]:\\n# d_test_bl.agg([(\\'count\\', target, \\'Count\\')], group_by=target).collect()\\nf_class_count(d_test_bl).collect() # check test classes are balanced\\nOut[164]:\\nFLIGHT_RISK\\nCount\\n0\\nYes\\n210\\n1\\nNo\\n210\\nIn\\xa0[165]:\\nd_train_bl = d_train.union(d_test_no_rest)\\nd_train_bl.shape, d_train.shape # train_bl with increased rows\\nOut[165]:\\n([16784, 47], [15292, 47])\\nIn\\xa0[166]:\\nd_train_target_count = f_class_count(d_train_bl) # added No from d_test that are not used (unseen)\\nd_train_target_count.collect()\\nOut[166]:\\nFLIGHT_RISK\\nCount\\n0\\nNo\\n15104\\n1\\nYes\\n1680\\nIn\\xa0[167]:\\nd_train_bl_sm = f_smote(d_train_bl,f_smote_nmax(d_train_bl))\\nd_train_bl_sm = f_employee_id_unique(d_train_bl_sm)\\nd_train_bl_sm.shape\\nOut[167]:\\n[30207, 47]\\nIn\\xa0[168]:\\nf_class_count(d_train_bl_sm).collect() # #check classes count for Majority No:Minority Yes = 1:1\\nOut[168]:\\nFLIGHT_RISK\\nCount\\n0\\nNo\\n15104\\n1\\nYes\\n15103\\nIn\\xa0[169]:\\ncols_imp\\nOut[169]:\\n[\\'EMPLOYEE_ID\\',\\n \\'AGE\\',\\n \\'CRITICAL_JOB_ROLE\\',\\n \\'RISK_OF_LOSS\\',\\n \\'IMPACT_OF_LOSS\\',\\n \\'FUTURE_LEADER\\',\\n \\'GENDER\\',\\n \\'MGR_EMP\\',\\n \\'TENURE_MONTHS\\',\\n \\'SALARY\\',\\n \\'EMPLOYMENT_TYPE_2\\',\\n \\'HIGH_POTENTIAL\\',\\n \\'PREVIOUS_FUNCTIONAL_AREA\\',\\n \\'PREVIOUS_CAREER_PATH\\',\\n \\'PREVIOUS_PERFORMANCE_RATING\\',\\n \\'PREVIOUS_COUNTRY\\',\\n \\'TIMEINPREVPOSITIONMONTH\\',\\n \\'CURRENT_FUNCTIONAL_AREA\\',\\n \\'CURRENT_JOB_LEVEL\\',\\n \\'CURRENT_PERFORMANCE_RATING\\',\\n \\'CURRENT_COUNTRY\\',\\n \\'PROMOTION_WITHIN_LAST_3_YEARS\\',\\n \\'CHANGED_POSITION_WITHIN_LAST_2_YEARS\\',\\n \\'CHANGE_IN_PERFORMANCE_RATING\\',\\n \\'FUNCTIONALAREACHANGETYPE\\',\\n \\'JOBLEVELCHANGETYPE\\',\\n \\'TIMEINPREVPOS_INT\\',\\n \\'LINKEDIN\\',\\n \\'HRTRAINING\\',\\n \\'SICKDAYS\\',\\n \\'FLIGHT_RISK\\']\\nIn\\xa0[170]:\\nd_train_bl_sm[cols_imp].shape, d_train.shape\\nOut[170]:\\n([30207, 31], [15292, 47])\\nIn\\xa0[171]:\\n%%time\\n# Fit with balanced d_train_bl_sm important features\\nHGBT_MODEL=model_fit(d_train_bl_sm[cols_imp])\\nCPU times: total: 0 ns\\nWall time: 1min 3s\\nIn\\xa0[172]:\\nHGBT_MODEL.statistics_.collect()\\nOut[172]:\\nSTAT_NAME\\nSTAT_VALUE\\nCLASS_NAME\\n0\\nAUC\\n0.9818204280351988\\nNone\\n1\\nRECALL\\n0.9152598477325389\\nNo\\n2\\nPRECISION\\n0.9518072289156626\\nNo\\n3\\nF1_SCORE\\n0.9331758353020586\\nNo\\n4\\nSUPPORT\\n3021\\nNo\\n5\\nRECALL\\n0.9536423841059603\\nYes\\n6\\nPRECISION\\n0.9183673469387755\\nYes\\n7\\nF1_SCORE\\n0.9356725146198831\\nYes\\n8\\nSUPPORT\\n3020\\nYes\\n9\\nACCURACY\\n0.9344479390829333\\nNone\\n10\\nKAPPA\\n0.8688967080317397\\nNone\\n11\\nMCC\\n0.8695381711282378\\nNone\\nIn\\xa0[173]:\\nf_score_res(HGBT_MODEL, d_test_bl[cols_imp])[1].collect()\\nOut[173]:\\nSTAT_NAME\\nSTAT_VALUE\\nCLASS_NAME\\n0\\nAUC\\n0.9422789115646258\\nNone\\n1\\nRECALL\\n0.919047619047619\\nNo\\n2\\nPRECISION\\n0.8427947598253275\\nNo\\n3\\nF1_SCORE\\n0.8792710706150342\\nNo\\n4\\nSUPPORT\\n210\\nNo\\n5\\nRECALL\\n0.8285714285714286\\nYes\\n6\\nPRECISION\\n0.9109947643979057\\nYes\\n7\\nF1_SCORE\\n0.8678304239401496\\nYes\\n8\\nSUPPORT\\n210\\nYes\\n9\\nACCURACY\\n0.8738095238095238\\nNone\\n10\\nKAPPA\\n0.7476190476190476\\nNone\\n11\\nMCC\\n0.7506979460508658\\nNone\\nIn\\xa0[174]:\\nd_test_bl[cols_imp].shape, d_test.shape\\nOut[174]:\\n([420, 31], [1912, 47])\\nIn\\xa0[175]:\\ndf_model_stat_last, df_pred_stat_last = f_stat_all(HGBT_MODEL,  d_test_bl[cols_imp], \\'sm_max_bl_cols_imp\\')\\nIn\\xa0[176]:\\ndf_model_stat_last\\nOut[176]:\\nSTAT_NAME\\nCLASS_NAME\\nINIT\\nADD\\ncols_imp\\ncols_imp_mn\\nsm_max_cols_all\\nsm_max_cols_imp\\nsm_max_bl_cols_imp\\n0\\nAUC\\nNone\\n0.9743\\n0.9830\\n0.9856\\n0.9843\\n0.9868\\n0.9776\\n0.9818\\n1\\nRECALL\\nNo\\n0.9959\\n0.9790\\n0.9753\\n0.9739\\n0.9320\\n0.9015\\n0.9152\\n2\\nPRECISION\\nNo\\n0.9249\\n0.9531\\n0.9567\\n0.9529\\n0.9606\\n0.9456\\n0.9518\\n3\\nF1_SCORE\\nNo\\n0.9591\\n0.9659\\n0.9659\\n0.9632\\n0.9461\\n0.9230\\n0.9331\\n4\\nSUPPORT\\nNo\\n2722\\n2722\\n2722\\n2722\\n2722\\n2722\\n3021\\n5\\nRECALL\\nYes\\n0.3452\\n0.6101\\n0.6428\\n0.6101\\n0.9617\\n0.9481\\n0.9536\\n6\\nPRECISION\\nYes\\n0.9133\\n0.7824\\n0.7632\\n0.7427\\n0.9339\\n0.9059\\n0.9183\\n7\\nF1_SCORE\\nYes\\n0.5010\\n0.6856\\n0.6978\\n0.6699\\n0.9476\\n0.9265\\n0.9356\\n8\\nSUPPORT\\nYes\\n336\\n336\\n336\\n336\\n2722\\n2722\\n3020\\n9\\nACCURACY\\nNone\\n0.9244\\n0.9385\\n0.9388\\n0.9339\\n0.9469\\n0.9248\\n0.9344\\n10\\nKAPPA\\nNone\\n0.4690\\n0.6521\\n0.6641\\n0.6336\\n0.8938\\n0.8497\\n0.8688\\n11\\nMCC\\nNone\\n0.5348\\n0.6583\\n0.6671\\n0.6374\\n0.8942\\n0.8506\\n0.8695\\nIn\\xa0[177]:\\n# SUPPORT Yes = SUPPORT No  ->  all statistics are reliable\\n# F1_SCORE Yes ~ same \\n# ACCURACY Yes ~ same\\nIn\\xa0[178]:\\ndf_pred_stat_last\\nOut[178]:\\nSTAT_NAME\\nCLASS_NAME\\nINIT\\nADD\\ncols_imp\\ncols_imp_mn\\nsm_max_cols_all\\nsm_max_cols_imp\\nsm_max_bl_cols_imp\\n0\\nAUC\\nNone\\n0.9733\\n0.9834\\n0.9857\\n0.9870\\n0.9784\\n0.9709\\n0.9422\\n1\\nRECALL\\nNo\\n0.9964\\n0.9800\\n0.9782\\n0.9811\\n0.9453\\n0.9195\\n0.9190\\n2\\nPRECISION\\nNo\\n0.9247\\n0.9488\\n0.9541\\n0.9548\\n0.9751\\n0.9726\\n0.8427\\n3\\nF1_SCORE\\nNo\\n0.9592\\n0.9641\\n0.9660\\n0.9678\\n0.9600\\n0.9453\\n0.8792\\n4\\nSUPPORT\\nNo\\n1702\\n1702\\n1702\\n1702\\n1702\\n1702\\n210\\n5\\nRECALL\\nYes\\n0.3428\\n0.5714\\n0.6190\\n0.6238\\n0.8047\\n0.7904\\n0.8285\\n6\\nPRECISION\\nYes\\n0.9230\\n0.7792\\n0.7784\\n0.8036\\n0.6450\\n0.5478\\n0.9109\\n7\\nF1_SCORE\\nYes\\n0.5000\\n0.6593\\n0.6896\\n0.7024\\n0.7161\\n0.6471\\n0.8678\\n8\\nSUPPORT\\nYes\\n210\\n210\\n210\\n210\\n210\\n210\\n210\\n9\\nACCURACY\\nNone\\n0.9246\\n0.9351\\n0.9388\\n0.9419\\n0.9299\\n0.9053\\n0.8738\\n10\\nKAPPA\\nNone\\n0.4683\\n0.6244\\n0.6562\\n0.6708\\n0.6766\\n0.5945\\n0.7476\\n11\\nMCC\\nNone\\n0.5363\\n0.6336\\n0.6615\\n0.6774\\n0.6820\\n0.6079\\n0.7506\\nIn\\xa0[179]:\\n# SUPPORT Yes = SUPPOT No  ->  all statistics are reliable\\n# RECALL Yes ~ same\\n# PRECISION Yes up jump. False alarm. Overestimation\\n# ACCURACY reliable\\nIn\\xa0[180]:\\n# list from range with inclusive right\\ndef f_list_range_inclusive(start,stop,step): \\n    ls = list(range(start,stop,step))\\n    if stop not in ls:\\n        ls.append(stop)\\n    return ls \\nIn\\xa0[181]:\\nls_smote = f_list_range_inclusive(0,f_smote_nmax(d_train_bl),100)\\nls_smote\\nOut[181]:\\n[0, 100, 200, 300, 400, 500, 600, 700, 799]\\nIn\\xa0[182]:\\ndef statistics_no_yes(df): # display only Yes and No\\n    return df[df[\\'CLASS_NAME\\'].isin([\\'Yes\\', \\'No\\'])]    \\nIn\\xa0[183]:\\ndef f_recall_yes_descending(df): # select  Yes\\n    return df.loc[(df[\\'STAT_NAME\\'] ==\\'RECALL\\') & (df[\\'CLASS_NAME\\'] == \\'Yes\\')].squeeze().sort_values(ascending=False)    \\ndef f_f1_score_yes_descending(df): # select F1_SCORE Yes\\n    return df.loc[(df[\\'STAT_NAME\\'] ==\\'F1_SCORE\\') & (df[\\'CLASS_NAME\\'] == \\'Yes\\')].squeeze().sort_values(ascending=False)    \\ndef f_f1_score_no_descending(df): # select only F1_SCORE No\\n    return df.loc[(df[\\'STAT_NAME\\'] ==\\'F1_SCORE\\') & (df[\\'CLASS_NAME\\'] == \\'No\\')].squeeze().sort_values(ascending=False)    \\ndef f_auc_descending(df): # select AUC\\n    return df.loc[(df[\\'STAT_NAME\\'] ==\\'AUC\\')].squeeze().sort_values(ascending=False)   \\ndef f_accuracy_descending(df): # select AUC\\n    return df.loc[(df[\\'STAT_NAME\\'] ==\\'ACCURACY\\')].squeeze().sort_values(ascending=False) # select ACCURACY\\nIn\\xa0[184]:\\n%%time\\n# SMOTE with steps\\n# fire on all cylinders\\ndef f_smote_models(p_d_train, p_d_test, p_cols, p_smote_nstep):\\n    stat_value = \\'STAT_VALUE\\' \\n    cols_order = [\\'STAT_NAME\\', \\'CLASS_NAME\\', \\'STAT_VALUE\\']\\n    ls_smote = f_list_range_inclusive(0,f_smote_nmax(p_d_train),p_smote_nstep)\\n    print(f\"ls_smote: {ls_smote}\")\\n    dict_models_smote = {}\\n    first_step = True\\n    for e_smote in ls_smote:\\n        smote_n = f\\'smote_{e_smote}\\'\\n        col_name = f\\'{stat_value}_{e_smote}\\'\\n        print(f\\'{smote_n} column {col_name}\\')\\n        d_train_bl_sm = f_smote(p_d_train, e_smote)\\n        d_train_bl_sm = f_employee_id_unique(d_train_bl_sm)\\n        HGBT_MODEL=model_fit(d_train_bl_sm[p_cols]) # model statitiscs\\n        score_res = f_score_res(HGBT_MODEL,p_d_test[p_cols]) # predicted statistics\\n        if first_step:\\n            df_model_statistics = HGBT_MODEL.statistics_.collect()\\n            df_model_statistics = df_model_statistics[cols_order]\\n            df_model_statistics.rename(columns = {stat_value:col_name}, inplace = True)\\n            df_predicted_statistics = score_res[1].collect()\\n            df_predicted_statistics = df_predicted_statistics[cols_order]\\n            df_predicted_statistics.rename(columns = {stat_value:col_name}, inplace = True)\\n            first_step = False\\n        else:\\n            df_model_statistics[col_name] = HGBT_MODEL.statistics_.collect()[stat_value]\\n            df_predicted_statistics[col_name] = score_res[1].collect()[stat_value]\\n        dict_models_smote[col_name] = [HGBT_MODEL,e_smote] # [model, smote_amount] for each STAT_VALUE\\n        \\n    return dict_models_smote, df_model_statistics, df_predicted_statistics\\nCPU times: total: 0 ns\\nWall time: 0 ns\\nIn\\xa0[185]:\\nd_train_bl[cols_imp].shape, d_test_bl[cols_imp].shape, d_train.shape, d_test.shape\\nOut[185]:\\n([16784, 31], [420, 31], [15292, 47], [1912, 47])\\nIn\\xa0[186]:\\nf_class_count(d_train_bl[cols_imp]).collect()\\nOut[186]:\\nFLIGHT_RISK\\nCount\\n0\\nNo\\n15104\\n1\\nYes\\n1680\\nIn\\xa0[187]:\\nf_class_count(d_test_bl[cols_imp]).collect()\\nOut[187]:\\nFLIGHT_RISK\\nCount\\n0\\nYes\\n210\\n1\\nNo\\n210\\nIn\\xa0[188]:\\n%%time\\n# SMOTE with steps\\n# fire on all cylinders\\ndict_models_smote, df_model_statistics_sm, df_predicted_statistics_sm = \\\\\\nf_smote_models(d_train_bl, d_test_bl, cols_imp, 100)\\nls_smote: [0, 100, 200, 300, 400, 500, 600, 700, 799]\\nsmote_0 column STAT_VALUE_0\\nsmote_100 column STAT_VALUE_100\\nsmote_200 column STAT_VALUE_200\\nsmote_300 column STAT_VALUE_300\\nsmote_400 column STAT_VALUE_400\\nsmote_500 column STAT_VALUE_500\\nsmote_600 column STAT_VALUE_600\\nsmote_700 column STAT_VALUE_700\\nsmote_799 column STAT_VALUE_799\\nCPU times: total: 62.5 ms\\nWall time: 8min 34s\\nIn\\xa0[189]:\\ndict_models_smote # [model, smote_amount] for each STAT_VALUE\\nOut[189]:\\n{\\'STAT_VALUE_0\\': [<hana_ml.algorithms.pal.unified_classification.UnifiedClassification at 0x25c038b3850>,\\n  0],\\n \\'STAT_VALUE_100\\': [<hana_ml.algorithms.pal.unified_classification.UnifiedClassification at 0x25c038b3a00>,\\n  100],\\n \\'STAT_VALUE_200\\': [<hana_ml.algorithms.pal.unified_classification.UnifiedClassification at 0x25c038d8a90>,\\n  200],\\n \\'STAT_VALUE_300\\': [<hana_ml.algorithms.pal.unified_classification.UnifiedClassification at 0x25c038d8bb0>,\\n  300],\\n \\'STAT_VALUE_400\\': [<hana_ml.algorithms.pal.unified_classification.UnifiedClassification at 0x25c038d8850>,\\n  400],\\n \\'STAT_VALUE_500\\': [<hana_ml.algorithms.pal.unified_classification.UnifiedClassification at 0x25c038ebc70>,\\n  500],\\n \\'STAT_VALUE_600\\': [<hana_ml.algorithms.pal.unified_classification.UnifiedClassification at 0x25c038d8190>,\\n  600],\\n \\'STAT_VALUE_700\\': [<hana_ml.algorithms.pal.unified_classification.UnifiedClassification at 0x25c038eb220>,\\n  700],\\n \\'STAT_VALUE_799\\': [<hana_ml.algorithms.pal.unified_classification.UnifiedClassification at 0x25c038eb370>,\\n  799]}\\nIn\\xa0[190]:\\n# statistics_no_yes(df_model_statistics)\\ndf_model_statistics_sm\\nOut[190]:\\nSTAT_NAME\\nCLASS_NAME\\nSTAT_VALUE_0\\nSTAT_VALUE_100\\nSTAT_VALUE_200\\nSTAT_VALUE_300\\nSTAT_VALUE_400\\nSTAT_VALUE_500\\nSTAT_VALUE_600\\nSTAT_VALUE_700\\nSTAT_VALUE_799\\n0\\nAUC\\nNone\\n0.9867829385447328\\n0.9812232860684682\\n0.9816351925088159\\n0.9787969753152026\\n0.9781200404481551\\n0.9829435494789421\\n0.9778921975789432\\n0.9805952293399859\\n0.9814217836781113\\n1\\nRECALL\\nNo\\n0.9837802052300563\\n0.9688844753392917\\n0.9536577292287322\\n0.9476994372724263\\n0.9377689506785832\\n0.9334657398212513\\n0.9285004965243296\\n0.9258523667659715\\n0.9205561072492552\\n2\\nPRECISION\\nNo\\n0.9547060713138452\\n0.9484769928710305\\n0.9536577292287322\\n0.9473858371939113\\n0.9459098497495826\\n0.9556082683835988\\n0.9489174560216509\\n0.9500679347826086\\n0.9501195763580458\\n3\\nF1_SCORE\\nNo\\n0.9690251059667427\\n0.9585721303422302\\n0.9536577292287323\\n0.9475426112857853\\n0.9418218085106382\\n0.9444072337575351\\n0.9385979588422284\\n0.9378038558256495\\n0.9351042367182246\\n4\\nSUPPORT\\nNo\\n3021\\n3021\\n3021\\n3021\\n3021\\n3021\\n3021\\n3021\\n3021\\n5\\nRECALL\\nYes\\n0.5803571428571429\\n0.7633928571428571\\n0.8611111111111112\\n0.8816964285714286\\n0.9035714285714286\\n0.9350198412698413\\n0.9357993197278912\\n0.9453125\\n0.9516556291390729\\n6\\nPRECISION\\nYes\\n0.7991803278688525\\n0.8451400329489291\\n0.8611111111111112\\n0.8823529411764706\\n0.8898007033997656\\n0.9036433365292426\\n0.9106330161357055\\n0.9189873417721519\\n0.9229287090558767\\n7\\nF1_SCORE\\nYes\\n0.6724137931034483\\n0.8021892103205629\\n0.8611111111111112\\n0.8820245627093413\\n0.8966331955109273\\n0.9190638712823014\\n0.9230446634514574\\n0.9319640564826701\\n0.9370720573850669\\n8\\nSUPPORT\\nYes\\n336\\n672\\n1008\\n1344\\n1680\\n2016\\n2352\\n2688\\n3020\\n9\\nACCURACY\\nNone\\n0.9434018468871016\\n0.9314920119144328\\n0.9305038471084637\\n0.92737686139748\\n0.9255477557966391\\n0.9340877506452253\\n0.9316955146100875\\n0.9350148887721142\\n0.9361032941565965\\n10\\nKAPPA\\nNone\\n0.6422899098868706\\n0.7608906782504282\\n0.8147688403398433\\n0.8295671844913473\\n0.8384603648389375\\n0.8634984085409364\\n0.8616631270253143\\n0.869791646494887\\n0.8722072431456156\\n11\\nMCC\\nNone\\n0.6521468193543035\\n0.7623304786507337\\n0.8147688403398433\\n0.8295673043886949\\n0.8385207413832467\\n0.8638562550541513\\n0.861921872993784\\n0.8701094323199194\\n0.8726299106561325\\nIn\\xa0[191]:\\n# statistics_no_yes(df_predicted_statistics)\\ndf_predicted_statistics_sm\\nOut[191]:\\nSTAT_NAME\\nCLASS_NAME\\nSTAT_VALUE_0\\nSTAT_VALUE_100\\nSTAT_VALUE_200\\nSTAT_VALUE_300\\nSTAT_VALUE_400\\nSTAT_VALUE_500\\nSTAT_VALUE_600\\nSTAT_VALUE_700\\nSTAT_VALUE_799\\n0\\nAUC\\nNone\\n0.8924319727891157\\n0.928812358276644\\n0.9437329931972789\\n0.9268197278911564\\n0.929016439909297\\n0.9413208616780046\\n0.9272250566893424\\n0.9474007936507937\\n0.938421201814059\\n1\\nRECALL\\nNo\\n0.9952380952380953\\n0.9809523809523809\\n0.9666666666666667\\n0.9571428571428572\\n0.9523809523809523\\n0.9380952380952381\\n0.919047619047619\\n0.9476190476190476\\n0.919047619047619\\n2\\nPRECISION\\nNo\\n0.7084745762711865\\n0.7803030303030303\\n0.8055555555555556\\n0.7944664031620553\\n0.8097165991902834\\n0.8208333333333333\\n0.8283261802575107\\n0.8257261410788381\\n0.8283261802575107\\n3\\nF1_SCORE\\nNo\\n0.8277227722772277\\n0.8691983122362869\\n0.8787878787878789\\n0.8682505399568035\\n0.8752735229759301\\n0.8755555555555555\\n0.8713318284424378\\n0.8824833702882483\\n0.8713318284424378\\n4\\nSUPPORT\\nNo\\n210\\n210\\n210\\n210\\n210\\n210\\n210\\n210\\n210\\n5\\nRECALL\\nYes\\n0.5904761904761905\\n0.7238095238095238\\n0.7666666666666667\\n0.7523809523809524\\n0.7761904761904762\\n0.7952380952380952\\n0.8095238095238095\\n0.8\\n0.8095238095238095\\n6\\nPRECISION\\nYes\\n0.992\\n0.9743589743589743\\n0.9583333333333334\\n0.9461077844311377\\n0.9421965317919075\\n0.9277777777777778\\n0.9090909090909091\\n0.9385474860335196\\n0.9090909090909091\\n7\\nF1_SCORE\\nYes\\n0.7402985074626864\\n0.8306010928961748\\n0.8518518518518519\\n0.8381962864721485\\n0.8511749347258486\\n0.8564102564102564\\n0.8564231738035265\\n0.8637532133676092\\n0.8564231738035265\\n8\\nSUPPORT\\nYes\\n210\\n210\\n210\\n210\\n210\\n210\\n210\\n210\\n210\\n9\\nACCURACY\\nNone\\n0.7928571428571428\\n0.8523809523809524\\n0.8666666666666667\\n0.8547619047619047\\n0.8642857142857143\\n0.8666666666666667\\n0.8642857142857143\\n0.8738095238095238\\n0.8642857142857143\\n10\\nKAPPA\\nNone\\n0.5857142857142857\\n0.7047619047619048\\n0.7333333333333333\\n0.7095238095238096\\n0.7285714285714285\\n0.7333333333333333\\n0.7285714285714285\\n0.7476190476190476\\n0.7285714285714285\\n11\\nMCC\\nNone\\n0.6405294420256534\\n0.7292852883865353\\n0.7484551991837488\\n0.724882762118208\\n0.7401502712296406\\n0.7409328454600197\\n0.732980915331063\\n0.7559004704470662\\n0.732980915331063\\nIn\\xa0[192]:\\nf_recall_yes_descending(df_predicted_statistics_sm)\\nOut[192]:\\nCLASS_NAME                       Yes\\nSTAT_NAME                     RECALL\\nSTAT_VALUE_600    0.8095238095238095\\nSTAT_VALUE_799    0.8095238095238095\\nSTAT_VALUE_700                   0.8\\nSTAT_VALUE_500    0.7952380952380952\\nSTAT_VALUE_400    0.7761904761904762\\nSTAT_VALUE_200    0.7666666666666667\\nSTAT_VALUE_300    0.7523809523809524\\nSTAT_VALUE_100    0.7238095238095238\\nSTAT_VALUE_0      0.5904761904761905\\nName: 5, dtype: object\\nIn\\xa0[193]:\\n# SUPPORT Yes = SUPPORT No - F1_SCORE Yes reliable\\nf_f1_score_yes_descending(df_predicted_statistics_sm)\\nOut[193]:\\nCLASS_NAME                       Yes\\nSTAT_NAME                   F1_SCORE\\nSTAT_VALUE_700    0.8637532133676092\\nSTAT_VALUE_600    0.8564231738035265\\nSTAT_VALUE_799    0.8564231738035265\\nSTAT_VALUE_500    0.8564102564102564\\nSTAT_VALUE_200    0.8518518518518519\\nSTAT_VALUE_400    0.8511749347258486\\nSTAT_VALUE_300    0.8381962864721485\\nSTAT_VALUE_100    0.8306010928961748\\nSTAT_VALUE_0      0.7402985074626864\\nName: 7, dtype: object\\nIn\\xa0[194]:\\n# SUPPORT Yes = SUPPORT No - ACCURACY reliable\\nf_accuracy_descending(df_predicted_statistics_sm)\\nOut[194]:\\nSTAT_NAME                   ACCURACY\\nSTAT_VALUE_700    0.8738095238095238\\nSTAT_VALUE_200    0.8666666666666667\\nSTAT_VALUE_500    0.8666666666666667\\nSTAT_VALUE_400    0.8642857142857143\\nSTAT_VALUE_600    0.8642857142857143\\nSTAT_VALUE_799    0.8642857142857143\\nSTAT_VALUE_300    0.8547619047619047\\nSTAT_VALUE_100    0.8523809523809524\\nSTAT_VALUE_0      0.7928571428571428\\nCLASS_NAME                      None\\nName: 9, dtype: object\\nIn\\xa0[195]:\\npred_stat_desc_sm = f_accuracy_descending(df_predicted_statistics_sm)\\npred_stat_desc_sm\\nOut[195]:\\nSTAT_NAME                   ACCURACY\\nSTAT_VALUE_700    0.8738095238095238\\nSTAT_VALUE_200    0.8666666666666667\\nSTAT_VALUE_500    0.8666666666666667\\nSTAT_VALUE_400    0.8642857142857143\\nSTAT_VALUE_600    0.8642857142857143\\nSTAT_VALUE_799    0.8642857142857143\\nSTAT_VALUE_300    0.8547619047619047\\nSTAT_VALUE_100    0.8523809523809524\\nSTAT_VALUE_0      0.7928571428571428\\nCLASS_NAME                      None\\nName: 9, dtype: object\\nIn\\xa0[196]:\\nsmote_best_model_name = pred_stat_desc_sm[:2].index.to_list()[1]\\nsmote_best_model_name\\nOut[196]:\\n\\'STAT_VALUE_700\\'\\nIn\\xa0[197]:\\nsmote_best_model = dict_models_smote[smote_best_model_name][0]\\nsmote_best_amount = dict_models_smote[smote_best_model_name][1]\\nsmote_best_model, smote_best_amount\\nOut[197]:\\n(<hana_ml.algorithms.pal.unified_classification.UnifiedClassification at 0x25c038eb220>,\\n 700)\\nIn\\xa0[198]:\\nsmote_best_model.statistics_.collect()\\nOut[198]:\\nSTAT_NAME\\nSTAT_VALUE\\nCLASS_NAME\\n0\\nAUC\\n0.9805952293399859\\nNone\\n1\\nRECALL\\n0.9258523667659715\\nNo\\n2\\nPRECISION\\n0.9500679347826086\\nNo\\n3\\nF1_SCORE\\n0.9378038558256495\\nNo\\n4\\nSUPPORT\\n3021\\nNo\\n5\\nRECALL\\n0.9453125\\nYes\\n6\\nPRECISION\\n0.9189873417721519\\nYes\\n7\\nF1_SCORE\\n0.9319640564826701\\nYes\\n8\\nSUPPORT\\n2688\\nYes\\n9\\nACCURACY\\n0.9350148887721142\\nNone\\n10\\nKAPPA\\n0.869791646494887\\nNone\\n11\\nMCC\\n0.8701094323199194\\nNone\\nIn\\xa0[199]:\\nsmote_best_model.confusion_matrix_.collect()\\nOut[199]:\\nACTUAL_CLASS\\nPREDICTED_CLASS\\nCOUNT\\n0\\nNo\\nNo\\n2797\\n1\\nNo\\nYes\\n224\\n2\\nYes\\nNo\\n147\\n3\\nYes\\nYes\\n2541\\nIn\\xa0[200]:\\nf_score_res(smote_best_model, d_test_bl[cols_imp])[1].collect()\\nOut[200]:\\nSTAT_NAME\\nSTAT_VALUE\\nCLASS_NAME\\n0\\nAUC\\n0.9474007936507937\\nNone\\n1\\nRECALL\\n0.9476190476190476\\nNo\\n2\\nPRECISION\\n0.8257261410788381\\nNo\\n3\\nF1_SCORE\\n0.8824833702882483\\nNo\\n4\\nSUPPORT\\n210\\nNo\\n5\\nRECALL\\n0.8\\nYes\\n6\\nPRECISION\\n0.9385474860335196\\nYes\\n7\\nF1_SCORE\\n0.8637532133676092\\nYes\\n8\\nSUPPORT\\n210\\nYes\\n9\\nACCURACY\\n0.8738095238095238\\nNone\\n10\\nKAPPA\\n0.7476190476190476\\nNone\\n11\\nMCC\\n0.7559004704470662\\nNone\\nIn\\xa0[201]:\\nf_score_res(smote_best_model, d_test_bl[cols_imp])[2].collect()\\nOut[201]:\\nACTUAL_CLASS\\nPREDICTED_CLASS\\nCOUNT\\n0\\nNo\\nNo\\n199\\n1\\nNo\\nYes\\n11\\n2\\nYes\\nNo\\n42\\n3\\nYes\\nYes\\n168\\nIn\\xa0[202]:\\nsmote_best_model.importance_.sort(\\'IMPORTANCE\\', desc=True).collect()\\nOut[202]:\\nVARIABLE_NAME\\nIMPORTANCE\\n0\\nHRTRAINING\\n0.353540\\n1\\nFUNCTIONALAREACHANGETYPE\\n0.115921\\n2\\nSICKDAYS\\n0.080760\\n3\\nPROMOTION_WITHIN_LAST_3_YEARS\\n0.067639\\n4\\nTIMEINPREVPOSITIONMONTH\\n0.058181\\n5\\nEMPLOYMENT_TYPE_2\\n0.052208\\n6\\nPREVIOUS_CAREER_PATH\\n0.038542\\n7\\nRISK_OF_LOSS\\n0.023417\\n8\\nCURRENT_COUNTRY\\n0.023271\\n9\\nSALARY\\n0.020924\\n10\\nJOBLEVELCHANGETYPE\\n0.020001\\n11\\nAGE\\n0.019775\\n12\\nCHANGE_IN_PERFORMANCE_RATING\\n0.018689\\n13\\nPREVIOUS_COUNTRY\\n0.017886\\n14\\nLINKEDIN\\n0.017514\\n15\\nTENURE_MONTHS\\n0.015400\\n16\\nFUTURE_LEADER\\n0.010235\\n17\\nPREVIOUS_PERFORMANCE_RATING\\n0.007124\\n18\\nIMPACT_OF_LOSS\\n0.006315\\n19\\nPREVIOUS_FUNCTIONAL_AREA\\n0.006156\\n20\\nCURRENT_JOB_LEVEL\\n0.005595\\n21\\nCRITICAL_JOB_ROLE\\n0.004405\\n22\\nGENDER\\n0.004174\\n23\\nTIMEINPREVPOS_INT\\n0.003757\\n24\\nCURRENT_FUNCTIONAL_AREA\\n0.002708\\n25\\nCURRENT_PERFORMANCE_RATING\\n0.002499\\n26\\nMGR_EMP\\n0.002074\\n27\\nHIGH_POTENTIAL\\n0.001288\\n28\\nCHANGED_POSITION_WITHIN_LAST_2_YEARS\\n0.000000\\nIn\\xa0[203]:\\n# cols_imp_full = smote_best_model.importance_.sort(\\'IMPORTANCE\\', desc=True).collect()[\\'VARIABLE_NAME\\'].to_list()\\n# cols_imp_full\\nsm_best_cols_imp = smote_best_model.importance_.sort(\\'IMPORTANCE\\', desc=True).collect()[\\'VARIABLE_NAME\\'].to_list()\\nsm_best_cols_imp\\nOut[203]:\\n[\\'HRTRAINING\\',\\n \\'FUNCTIONALAREACHANGETYPE\\',\\n \\'SICKDAYS\\',\\n \\'PROMOTION_WITHIN_LAST_3_YEARS\\',\\n \\'TIMEINPREVPOSITIONMONTH\\',\\n \\'EMPLOYMENT_TYPE_2\\',\\n \\'PREVIOUS_CAREER_PATH\\',\\n \\'RISK_OF_LOSS\\',\\n \\'CURRENT_COUNTRY\\',\\n \\'SALARY\\',\\n \\'JOBLEVELCHANGETYPE\\',\\n \\'AGE\\',\\n \\'CHANGE_IN_PERFORMANCE_RATING\\',\\n \\'PREVIOUS_COUNTRY\\',\\n \\'LINKEDIN\\',\\n \\'TENURE_MONTHS\\',\\n \\'FUTURE_LEADER\\',\\n \\'PREVIOUS_PERFORMANCE_RATING\\',\\n \\'IMPACT_OF_LOSS\\',\\n \\'PREVIOUS_FUNCTIONAL_AREA\\',\\n \\'CURRENT_JOB_LEVEL\\',\\n \\'CRITICAL_JOB_ROLE\\',\\n \\'GENDER\\',\\n \\'TIMEINPREVPOS_INT\\',\\n \\'CURRENT_FUNCTIONAL_AREA\\',\\n \\'CURRENT_PERFORMANCE_RATING\\',\\n \\'MGR_EMP\\',\\n \\'HIGH_POTENTIAL\\',\\n \\'CHANGED_POSITION_WITHIN_LAST_2_YEARS\\']\\nIn\\xa0[204]:\\nsmote_best_amount\\nOut[204]:\\n700\\nIn\\xa0[205]:\\ndf_model_stat_last, df_pred_stat_last = f_stat_all(smote_best_model, d_test_bl[cols_imp], \\'sm_best_bl_cols_imp\\')\\nIn\\xa0[206]:\\ndf_pred_stat_last\\nOut[206]:\\nSTAT_NAME\\nCLASS_NAME\\nINIT\\nADD\\ncols_imp\\ncols_imp_mn\\nsm_max_cols_all\\nsm_max_cols_imp\\nsm_max_bl_cols_imp\\nsm_best_bl_cols_imp\\n0\\nAUC\\nNone\\n0.9733\\n0.9834\\n0.9857\\n0.9870\\n0.9784\\n0.9709\\n0.9422\\n0.9474\\n1\\nRECALL\\nNo\\n0.9964\\n0.9800\\n0.9782\\n0.9811\\n0.9453\\n0.9195\\n0.9190\\n0.9476\\n2\\nPRECISION\\nNo\\n0.9247\\n0.9488\\n0.9541\\n0.9548\\n0.9751\\n0.9726\\n0.8427\\n0.8257\\n3\\nF1_SCORE\\nNo\\n0.9592\\n0.9641\\n0.9660\\n0.9678\\n0.9600\\n0.9453\\n0.8792\\n0.8824\\n4\\nSUPPORT\\nNo\\n1702\\n1702\\n1702\\n1702\\n1702\\n1702\\n210\\n210\\n5\\nRECALL\\nYes\\n0.3428\\n0.5714\\n0.6190\\n0.6238\\n0.8047\\n0.7904\\n0.8285\\n0.8\\n6\\nPRECISION\\nYes\\n0.9230\\n0.7792\\n0.7784\\n0.8036\\n0.6450\\n0.5478\\n0.9109\\n0.9385\\n7\\nF1_SCORE\\nYes\\n0.5000\\n0.6593\\n0.6896\\n0.7024\\n0.7161\\n0.6471\\n0.8678\\n0.8637\\n8\\nSUPPORT\\nYes\\n210\\n210\\n210\\n210\\n210\\n210\\n210\\n210\\n9\\nACCURACY\\nNone\\n0.9246\\n0.9351\\n0.9388\\n0.9419\\n0.9299\\n0.9053\\n0.8738\\n0.8738\\n10\\nKAPPA\\nNone\\n0.4683\\n0.6244\\n0.6562\\n0.6708\\n0.6766\\n0.5945\\n0.7476\\n0.7476\\n11\\nMCC\\nNone\\n0.5363\\n0.6336\\n0.6615\\n0.6774\\n0.6820\\n0.6079\\n0.7506\\n0.7559\\nIn\\xa0[207]:\\n%%time\\n# FEATURES IMPORTANCE TOP n\\n# fire on all cylinders\\ndef f_smote_model_top_f(p_d_train, p_cols_imp, p_d_test, p_smote_best_amount, p_cols_imp_start, p_cols_imp_step, p_verbosity):\\n    global tab_id\\n    global target\\n    lc_stat_value = \\'STAT_VALUE\\' \\n    lc_cols_order = [\\'STAT_NAME\\', \\'CLASS_NAME\\', \\'STAT_VALUE\\']\\n    lc_dict_models_top_f = {}\\n    lc_first_step = True\\n    lc_e_smote = p_smote_best_amount\\n    ls_top_imp = f_list_range_inclusive(p_cols_imp_start,len(p_cols_imp),p_cols_imp_step)\\n    for e_top_imp in ls_top_imp:\\n        lc_top_imp_n = f\\'top{e_top_imp}\\'\\n        lc_col_name = f\\'{lc_stat_value}_top{e_top_imp}\\'\\n        lc_cols_imp = p_cols_imp[:e_top_imp]\\n        lc_cols_imp = [tab_id] + lc_cols_imp + [target]\\n        lc_d_train_bl_sm = f_smote(p_d_train,lc_e_smote)\\n        lc_d_train_bl_sm = f_employee_id_unique(lc_d_train_bl_sm)\\n        print(f\"lc_col_name: {lc_col_name}, len(lc_cols_imp): {len(lc_cols_imp)}\")\\n        if p_verbosity:\\n            print(lc_cols_imp)  \\n            \\n        lc_HGBT_MODEL=model_fit(lc_d_train_bl_sm[lc_cols_imp]) # model statitiscs\\n        lc_score_res = f_score_res(lc_HGBT_MODEL,p_d_test[lc_cols_imp]) # predicted statistics\\n        if lc_first_step:\\n            lc_df_model_statistics = lc_HGBT_MODEL.statistics_.collect()\\n            lc_df_model_statistics = lc_df_model_statistics[lc_cols_order]\\n            lc_df_model_statistics.rename(columns = {lc_stat_value:lc_col_name}, inplace = True)\\n            lc_df_predicted_statistics = lc_score_res[1].collect()\\n            lc_df_predicted_statistics = lc_df_predicted_statistics[lc_cols_order]\\n            lc_df_predicted_statistics.rename(columns = {lc_stat_value:lc_col_name}, inplace = True)\\n            lc_first_step = False\\n        else:\\n            lc_df_model_statistics[lc_col_name] = lc_HGBT_MODEL.statistics_.collect()[lc_stat_value]\\n            lc_df_predicted_statistics[lc_col_name] = lc_score_res[1].collect()[lc_stat_value]\\n        lc_dict_models_top_f[lc_col_name] = [lc_HGBT_MODEL,lc_cols_imp, len(lc_cols_imp), lc_top_imp_n] # model, columns\\n        \\n    return lc_dict_models_top_f, lc_df_model_statistics, lc_df_predicted_statistics\\nCPU times: total: 0 ns\\nWall time: 0 ns\\nIn\\xa0[208]:\\n# for e in f_list_range_inclusive(3,len(sm_best_cols_imp[:3]),3): # Tests\\n#     print(e, sm_best_cols_imp[:e])\\nIn\\xa0[209]:\\nd_train_bl.shape, d_test_bl.shape, smote_best_amount, len(sm_best_cols_imp)\\nOut[209]:\\n([16784, 47], [420, 47], 700, 29)\\nIn\\xa0[210]:\\n# len(sm_best_cols_imp) # tests\\nIn\\xa0[211]:\\n# sm_best_cols_imp[::-1][:5] # tests\\nIn\\xa0[212]:\\n%%time\\n# FEATURES IMPORTANCE TOP n\\n# fire on all cylinders\\ndict_models_top_f, df_model_statistics_top_f, df_predicted_statistics_top_f = \\\\\\nf_smote_model_top_f(d_train_bl, sm_best_cols_imp, d_test_bl, smote_best_amount, 1, 1, True)\\n# f_smote_model_top_f(d_train_bl, sm_best_cols_imp[::-1][:5], d_test_bl, smote_best_amount, 28, 10, True) # revers cols test\\nlc_col_name: STAT_VALUE_top1, len(lc_cols_imp): 3\\n[\\'EMPLOYEE_ID\\', \\'HRTRAINING\\', \\'FLIGHT_RISK\\']\\nlc_col_name: STAT_VALUE_top2, len(lc_cols_imp): 4\\n[\\'EMPLOYEE_ID\\', \\'HRTRAINING\\', \\'FUNCTIONALAREACHANGETYPE\\', \\'FLIGHT_RISK\\']\\nlc_col_name: STAT_VALUE_top3, len(lc_cols_imp): 5\\n[\\'EMPLOYEE_ID\\', \\'HRTRAINING\\', \\'FUNCTIONALAREACHANGETYPE\\', \\'SICKDAYS\\', \\'FLIGHT_RISK\\']\\nlc_col_name: STAT_VALUE_top4, len(lc_cols_imp): 6\\n[\\'EMPLOYEE_ID\\', \\'HRTRAINING\\', \\'FUNCTIONALAREACHANGETYPE\\', \\'SICKDAYS\\', \\'PROMOTION_WITHIN_LAST_3_YEARS\\', \\'FLIGHT_RISK\\']\\nlc_col_name: STAT_VALUE_top5, len(lc_cols_imp): 7\\n[\\'EMPLOYEE_ID\\', \\'HRTRAINING\\', \\'FUNCTIONALAREACHANGETYPE\\', \\'SICKDAYS\\', \\'PROMOTION_WITHIN_LAST_3_YEARS\\', \\'TIMEINPREVPOSITIONMONTH\\', \\'FLIGHT_RISK\\']\\nlc_col_name: STAT_VALUE_top6, len(lc_cols_imp): 8\\n[\\'EMPLOYEE_ID\\', \\'HRTRAINING\\', \\'FUNCTIONALAREACHANGETYPE\\', \\'SICKDAYS\\', \\'PROMOTION_WITHIN_LAST_3_YEARS\\', \\'TIMEINPREVPOSITIONMONTH\\', \\'EMPLOYMENT_TYPE_2\\', \\'FLIGHT_RISK\\']\\nlc_col_name: STAT_VALUE_top7, len(lc_cols_imp): 9\\n[\\'EMPLOYEE_ID\\', \\'HRTRAINING\\', \\'FUNCTIONALAREACHANGETYPE\\', \\'SICKDAYS\\', \\'PROMOTION_WITHIN_LAST_3_YEARS\\', \\'TIMEINPREVPOSITIONMONTH\\', \\'EMPLOYMENT_TYPE_2\\', \\'PREVIOUS_CAREER_PATH\\', \\'FLIGHT_RISK\\']\\nlc_col_name: STAT_VALUE_top8, len(lc_cols_imp): 10\\n[\\'EMPLOYEE_ID\\', \\'HRTRAINING\\', \\'FUNCTIONALAREACHANGETYPE\\', \\'SICKDAYS\\', \\'PROMOTION_WITHIN_LAST_3_YEARS\\', \\'TIMEINPREVPOSITIONMONTH\\', \\'EMPLOYMENT_TYPE_2\\', \\'PREVIOUS_CAREER_PATH\\', \\'RISK_OF_LOSS\\', \\'FLIGHT_RISK\\']\\nlc_col_name: STAT_VALUE_top9, len(lc_cols_imp): 11\\n[\\'EMPLOYEE_ID\\', \\'HRTRAINING\\', \\'FUNCTIONALAREACHANGETYPE\\', \\'SICKDAYS\\', \\'PROMOTION_WITHIN_LAST_3_YEARS\\', \\'TIMEINPREVPOSITIONMONTH\\', \\'EMPLOYMENT_TYPE_2\\', \\'PREVIOUS_CAREER_PATH\\', \\'RISK_OF_LOSS\\', \\'CURRENT_COUNTRY\\', \\'FLIGHT_RISK\\']\\nlc_col_name: STAT_VALUE_top10, len(lc_cols_imp): 12\\n[\\'EMPLOYEE_ID\\', \\'HRTRAINING\\', \\'FUNCTIONALAREACHANGETYPE\\', \\'SICKDAYS\\', \\'PROMOTION_WITHIN_LAST_3_YEARS\\', \\'TIMEINPREVPOSITIONMONTH\\', \\'EMPLOYMENT_TYPE_2\\', \\'PREVIOUS_CAREER_PATH\\', \\'RISK_OF_LOSS\\', \\'CURRENT_COUNTRY\\', \\'SALARY\\', \\'FLIGHT_RISK\\']\\nlc_col_name: STAT_VALUE_top11, len(lc_cols_imp): 13\\n[\\'EMPLOYEE_ID\\', \\'HRTRAINING\\', \\'FUNCTIONALAREACHANGETYPE\\', \\'SICKDAYS\\', \\'PROMOTION_WITHIN_LAST_3_YEARS\\', \\'TIMEINPREVPOSITIONMONTH\\', \\'EMPLOYMENT_TYPE_2\\', \\'PREVIOUS_CAREER_PATH\\', \\'RISK_OF_LOSS\\', \\'CURRENT_COUNTRY\\', \\'SALARY\\', \\'JOBLEVELCHANGETYPE\\', \\'FLIGHT_RISK\\']\\nlc_col_name: STAT_VALUE_top12, len(lc_cols_imp): 14\\n[\\'EMPLOYEE_ID\\', \\'HRTRAINING\\', \\'FUNCTIONALAREACHANGETYPE\\', \\'SICKDAYS\\', \\'PROMOTION_WITHIN_LAST_3_YEARS\\', \\'TIMEINPREVPOSITIONMONTH\\', \\'EMPLOYMENT_TYPE_2\\', \\'PREVIOUS_CAREER_PATH\\', \\'RISK_OF_LOSS\\', \\'CURRENT_COUNTRY\\', \\'SALARY\\', \\'JOBLEVELCHANGETYPE\\', \\'AGE\\', \\'FLIGHT_RISK\\']\\nlc_col_name: STAT_VALUE_top13, len(lc_cols_imp): 15\\n[\\'EMPLOYEE_ID\\', \\'HRTRAINING\\', \\'FUNCTIONALAREACHANGETYPE\\', \\'SICKDAYS\\', \\'PROMOTION_WITHIN_LAST_3_YEARS\\', \\'TIMEINPREVPOSITIONMONTH\\', \\'EMPLOYMENT_TYPE_2\\', \\'PREVIOUS_CAREER_PATH\\', \\'RISK_OF_LOSS\\', \\'CURRENT_COUNTRY\\', \\'SALARY\\', \\'JOBLEVELCHANGETYPE\\', \\'AGE\\', \\'CHANGE_IN_PERFORMANCE_RATING\\', \\'FLIGHT_RISK\\']\\nlc_col_name: STAT_VALUE_top14, len(lc_cols_imp): 16\\n[\\'EMPLOYEE_ID\\', \\'HRTRAINING\\', \\'FUNCTIONALAREACHANGETYPE\\', \\'SICKDAYS\\', \\'PROMOTION_WITHIN_LAST_3_YEARS\\', \\'TIMEINPREVPOSITIONMONTH\\', \\'EMPLOYMENT_TYPE_2\\', \\'PREVIOUS_CAREER_PATH\\', \\'RISK_OF_LOSS\\', \\'CURRENT_COUNTRY\\', \\'SALARY\\', \\'JOBLEVELCHANGETYPE\\', \\'AGE\\', \\'CHANGE_IN_PERFORMANCE_RATING\\', \\'PREVIOUS_COUNTRY\\', \\'FLIGHT_RISK\\']\\nlc_col_name: STAT_VALUE_top15, len(lc_cols_imp): 17\\n[\\'EMPLOYEE_ID\\', \\'HRTRAINING\\', \\'FUNCTIONALAREACHANGETYPE\\', \\'SICKDAYS\\', \\'PROMOTION_WITHIN_LAST_3_YEARS\\', \\'TIMEINPREVPOSITIONMONTH\\', \\'EMPLOYMENT_TYPE_2\\', \\'PREVIOUS_CAREER_PATH\\', \\'RISK_OF_LOSS\\', \\'CURRENT_COUNTRY\\', \\'SALARY\\', \\'JOBLEVELCHANGETYPE\\', \\'AGE\\', \\'CHANGE_IN_PERFORMANCE_RATING\\', \\'PREVIOUS_COUNTRY\\', \\'LINKEDIN\\', \\'FLIGHT_RISK\\']\\nlc_col_name: STAT_VALUE_top16, len(lc_cols_imp): 18\\n[\\'EMPLOYEE_ID\\', \\'HRTRAINING\\', \\'FUNCTIONALAREACHANGETYPE\\', \\'SICKDAYS\\', \\'PROMOTION_WITHIN_LAST_3_YEARS\\', \\'TIMEINPREVPOSITIONMONTH\\', \\'EMPLOYMENT_TYPE_2\\', \\'PREVIOUS_CAREER_PATH\\', \\'RISK_OF_LOSS\\', \\'CURRENT_COUNTRY\\', \\'SALARY\\', \\'JOBLEVELCHANGETYPE\\', \\'AGE\\', \\'CHANGE_IN_PERFORMANCE_RATING\\', \\'PREVIOUS_COUNTRY\\', \\'LINKEDIN\\', \\'TENURE_MONTHS\\', \\'FLIGHT_RISK\\']\\nlc_col_name: STAT_VALUE_top17, len(lc_cols_imp): 19\\n[\\'EMPLOYEE_ID\\', \\'HRTRAINING\\', \\'FUNCTIONALAREACHANGETYPE\\', \\'SICKDAYS\\', \\'PROMOTION_WITHIN_LAST_3_YEARS\\', \\'TIMEINPREVPOSITIONMONTH\\', \\'EMPLOYMENT_TYPE_2\\', \\'PREVIOUS_CAREER_PATH\\', \\'RISK_OF_LOSS\\', \\'CURRENT_COUNTRY\\', \\'SALARY\\', \\'JOBLEVELCHANGETYPE\\', \\'AGE\\', \\'CHANGE_IN_PERFORMANCE_RATING\\', \\'PREVIOUS_COUNTRY\\', \\'LINKEDIN\\', \\'TENURE_MONTHS\\', \\'FUTURE_LEADER\\', \\'FLIGHT_RISK\\']\\nlc_col_name: STAT_VALUE_top18, len(lc_cols_imp): 20\\n[\\'EMPLOYEE_ID\\', \\'HRTRAINING\\', \\'FUNCTIONALAREACHANGETYPE\\', \\'SICKDAYS\\', \\'PROMOTION_WITHIN_LAST_3_YEARS\\', \\'TIMEINPREVPOSITIONMONTH\\', \\'EMPLOYMENT_TYPE_2\\', \\'PREVIOUS_CAREER_PATH\\', \\'RISK_OF_LOSS\\', \\'CURRENT_COUNTRY\\', \\'SALARY\\', \\'JOBLEVELCHANGETYPE\\', \\'AGE\\', \\'CHANGE_IN_PERFORMANCE_RATING\\', \\'PREVIOUS_COUNTRY\\', \\'LINKEDIN\\', \\'TENURE_MONTHS\\', \\'FUTURE_LEADER\\', \\'PREVIOUS_PERFORMANCE_RATING\\', \\'FLIGHT_RISK\\']\\nlc_col_name: STAT_VALUE_top19, len(lc_cols_imp): 21\\n[\\'EMPLOYEE_ID\\', \\'HRTRAINING\\', \\'FUNCTIONALAREACHANGETYPE\\', \\'SICKDAYS\\', \\'PROMOTION_WITHIN_LAST_3_YEARS\\', \\'TIMEINPREVPOSITIONMONTH\\', \\'EMPLOYMENT_TYPE_2\\', \\'PREVIOUS_CAREER_PATH\\', \\'RISK_OF_LOSS\\', \\'CURRENT_COUNTRY\\', \\'SALARY\\', \\'JOBLEVELCHANGETYPE\\', \\'AGE\\', \\'CHANGE_IN_PERFORMANCE_RATING\\', \\'PREVIOUS_COUNTRY\\', \\'LINKEDIN\\', \\'TENURE_MONTHS\\', \\'FUTURE_LEADER\\', \\'PREVIOUS_PERFORMANCE_RATING\\', \\'IMPACT_OF_LOSS\\', \\'FLIGHT_RISK\\']\\nlc_col_name: STAT_VALUE_top20, len(lc_cols_imp): 22\\n[\\'EMPLOYEE_ID\\', \\'HRTRAINING\\', \\'FUNCTIONALAREACHANGETYPE\\', \\'SICKDAYS\\', \\'PROMOTION_WITHIN_LAST_3_YEARS\\', \\'TIMEINPREVPOSITIONMONTH\\', \\'EMPLOYMENT_TYPE_2\\', \\'PREVIOUS_CAREER_PATH\\', \\'RISK_OF_LOSS\\', \\'CURRENT_COUNTRY\\', \\'SALARY\\', \\'JOBLEVELCHANGETYPE\\', \\'AGE\\', \\'CHANGE_IN_PERFORMANCE_RATING\\', \\'PREVIOUS_COUNTRY\\', \\'LINKEDIN\\', \\'TENURE_MONTHS\\', \\'FUTURE_LEADER\\', \\'PREVIOUS_PERFORMANCE_RATING\\', \\'IMPACT_OF_LOSS\\', \\'PREVIOUS_FUNCTIONAL_AREA\\', \\'FLIGHT_RISK\\']\\nlc_col_name: STAT_VALUE_top21, len(lc_cols_imp): 23\\n[\\'EMPLOYEE_ID\\', \\'HRTRAINING\\', \\'FUNCTIONALAREACHANGETYPE\\', \\'SICKDAYS\\', \\'PROMOTION_WITHIN_LAST_3_YEARS\\', \\'TIMEINPREVPOSITIONMONTH\\', \\'EMPLOYMENT_TYPE_2\\', \\'PREVIOUS_CAREER_PATH\\', \\'RISK_OF_LOSS\\', \\'CURRENT_COUNTRY\\', \\'SALARY\\', \\'JOBLEVELCHANGETYPE\\', \\'AGE\\', \\'CHANGE_IN_PERFORMANCE_RATING\\', \\'PREVIOUS_COUNTRY\\', \\'LINKEDIN\\', \\'TENURE_MONTHS\\', \\'FUTURE_LEADER\\', \\'PREVIOUS_PERFORMANCE_RATING\\', \\'IMPACT_OF_LOSS\\', \\'PREVIOUS_FUNCTIONAL_AREA\\', \\'CURRENT_JOB_LEVEL\\', \\'FLIGHT_RISK\\']\\nlc_col_name: STAT_VALUE_top22, len(lc_cols_imp): 24\\n[\\'EMPLOYEE_ID\\', \\'HRTRAINING\\', \\'FUNCTIONALAREACHANGETYPE\\', \\'SICKDAYS\\', \\'PROMOTION_WITHIN_LAST_3_YEARS\\', \\'TIMEINPREVPOSITIONMONTH\\', \\'EMPLOYMENT_TYPE_2\\', \\'PREVIOUS_CAREER_PATH\\', \\'RISK_OF_LOSS\\', \\'CURRENT_COUNTRY\\', \\'SALARY\\', \\'JOBLEVELCHANGETYPE\\', \\'AGE\\', \\'CHANGE_IN_PERFORMANCE_RATING\\', \\'PREVIOUS_COUNTRY\\', \\'LINKEDIN\\', \\'TENURE_MONTHS\\', \\'FUTURE_LEADER\\', \\'PREVIOUS_PERFORMANCE_RATING\\', \\'IMPACT_OF_LOSS\\', \\'PREVIOUS_FUNCTIONAL_AREA\\', \\'CURRENT_JOB_LEVEL\\', \\'CRITICAL_JOB_ROLE\\', \\'FLIGHT_RISK\\']\\nlc_col_name: STAT_VALUE_top23, len(lc_cols_imp): 25\\n[\\'EMPLOYEE_ID\\', \\'HRTRAINING\\', \\'FUNCTIONALAREACHANGETYPE\\', \\'SICKDAYS\\', \\'PROMOTION_WITHIN_LAST_3_YEARS\\', \\'TIMEINPREVPOSITIONMONTH\\', \\'EMPLOYMENT_TYPE_2\\', \\'PREVIOUS_CAREER_PATH\\', \\'RISK_OF_LOSS\\', \\'CURRENT_COUNTRY\\', \\'SALARY\\', \\'JOBLEVELCHANGETYPE\\', \\'AGE\\', \\'CHANGE_IN_PERFORMANCE_RATING\\', \\'PREVIOUS_COUNTRY\\', \\'LINKEDIN\\', \\'TENURE_MONTHS\\', \\'FUTURE_LEADER\\', \\'PREVIOUS_PERFORMANCE_RATING\\', \\'IMPACT_OF_LOSS\\', \\'PREVIOUS_FUNCTIONAL_AREA\\', \\'CURRENT_JOB_LEVEL\\', \\'CRITICAL_JOB_ROLE\\', \\'GENDER\\', \\'FLIGHT_RISK\\']\\nlc_col_name: STAT_VALUE_top24, len(lc_cols_imp): 26\\n[\\'EMPLOYEE_ID\\', \\'HRTRAINING\\', \\'FUNCTIONALAREACHANGETYPE\\', \\'SICKDAYS\\', \\'PROMOTION_WITHIN_LAST_3_YEARS\\', \\'TIMEINPREVPOSITIONMONTH\\', \\'EMPLOYMENT_TYPE_2\\', \\'PREVIOUS_CAREER_PATH\\', \\'RISK_OF_LOSS\\', \\'CURRENT_COUNTRY\\', \\'SALARY\\', \\'JOBLEVELCHANGETYPE\\', \\'AGE\\', \\'CHANGE_IN_PERFORMANCE_RATING\\', \\'PREVIOUS_COUNTRY\\', \\'LINKEDIN\\', \\'TENURE_MONTHS\\', \\'FUTURE_LEADER\\', \\'PREVIOUS_PERFORMANCE_RATING\\', \\'IMPACT_OF_LOSS\\', \\'PREVIOUS_FUNCTIONAL_AREA\\', \\'CURRENT_JOB_LEVEL\\', \\'CRITICAL_JOB_ROLE\\', \\'GENDER\\', \\'TIMEINPREVPOS_INT\\', \\'FLIGHT_RISK\\']\\nlc_col_name: STAT_VALUE_top25, len(lc_cols_imp): 27\\n[\\'EMPLOYEE_ID\\', \\'HRTRAINING\\', \\'FUNCTIONALAREACHANGETYPE\\', \\'SICKDAYS\\', \\'PROMOTION_WITHIN_LAST_3_YEARS\\', \\'TIMEINPREVPOSITIONMONTH\\', \\'EMPLOYMENT_TYPE_2\\', \\'PREVIOUS_CAREER_PATH\\', \\'RISK_OF_LOSS\\', \\'CURRENT_COUNTRY\\', \\'SALARY\\', \\'JOBLEVELCHANGETYPE\\', \\'AGE\\', \\'CHANGE_IN_PERFORMANCE_RATING\\', \\'PREVIOUS_COUNTRY\\', \\'LINKEDIN\\', \\'TENURE_MONTHS\\', \\'FUTURE_LEADER\\', \\'PREVIOUS_PERFORMANCE_RATING\\', \\'IMPACT_OF_LOSS\\', \\'PREVIOUS_FUNCTIONAL_AREA\\', \\'CURRENT_JOB_LEVEL\\', \\'CRITICAL_JOB_ROLE\\', \\'GENDER\\', \\'TIMEINPREVPOS_INT\\', \\'CURRENT_FUNCTIONAL_AREA\\', \\'FLIGHT_RISK\\']\\nlc_col_name: STAT_VALUE_top26, len(lc_cols_imp): 28\\n[\\'EMPLOYEE_ID\\', \\'HRTRAINING\\', \\'FUNCTIONALAREACHANGETYPE\\', \\'SICKDAYS\\', \\'PROMOTION_WITHIN_LAST_3_YEARS\\', \\'TIMEINPREVPOSITIONMONTH\\', \\'EMPLOYMENT_TYPE_2\\', \\'PREVIOUS_CAREER_PATH\\', \\'RISK_OF_LOSS\\', \\'CURRENT_COUNTRY\\', \\'SALARY\\', \\'JOBLEVELCHANGETYPE\\', \\'AGE\\', \\'CHANGE_IN_PERFORMANCE_RATING\\', \\'PREVIOUS_COUNTRY\\', \\'LINKEDIN\\', \\'TENURE_MONTHS\\', \\'FUTURE_LEADER\\', \\'PREVIOUS_PERFORMANCE_RATING\\', \\'IMPACT_OF_LOSS\\', \\'PREVIOUS_FUNCTIONAL_AREA\\', \\'CURRENT_JOB_LEVEL\\', \\'CRITICAL_JOB_ROLE\\', \\'GENDER\\', \\'TIMEINPREVPOS_INT\\', \\'CURRENT_FUNCTIONAL_AREA\\', \\'CURRENT_PERFORMANCE_RATING\\', \\'FLIGHT_RISK\\']\\nlc_col_name: STAT_VALUE_top27, len(lc_cols_imp): 29\\n[\\'EMPLOYEE_ID\\', \\'HRTRAINING\\', \\'FUNCTIONALAREACHANGETYPE\\', \\'SICKDAYS\\', \\'PROMOTION_WITHIN_LAST_3_YEARS\\', \\'TIMEINPREVPOSITIONMONTH\\', \\'EMPLOYMENT_TYPE_2\\', \\'PREVIOUS_CAREER_PATH\\', \\'RISK_OF_LOSS\\', \\'CURRENT_COUNTRY\\', \\'SALARY\\', \\'JOBLEVELCHANGETYPE\\', \\'AGE\\', \\'CHANGE_IN_PERFORMANCE_RATING\\', \\'PREVIOUS_COUNTRY\\', \\'LINKEDIN\\', \\'TENURE_MONTHS\\', \\'FUTURE_LEADER\\', \\'PREVIOUS_PERFORMANCE_RATING\\', \\'IMPACT_OF_LOSS\\', \\'PREVIOUS_FUNCTIONAL_AREA\\', \\'CURRENT_JOB_LEVEL\\', \\'CRITICAL_JOB_ROLE\\', \\'GENDER\\', \\'TIMEINPREVPOS_INT\\', \\'CURRENT_FUNCTIONAL_AREA\\', \\'CURRENT_PERFORMANCE_RATING\\', \\'MGR_EMP\\', \\'FLIGHT_RISK\\']\\nlc_col_name: STAT_VALUE_top28, len(lc_cols_imp): 30\\n[\\'EMPLOYEE_ID\\', \\'HRTRAINING\\', \\'FUNCTIONALAREACHANGETYPE\\', \\'SICKDAYS\\', \\'PROMOTION_WITHIN_LAST_3_YEARS\\', \\'TIMEINPREVPOSITIONMONTH\\', \\'EMPLOYMENT_TYPE_2\\', \\'PREVIOUS_CAREER_PATH\\', \\'RISK_OF_LOSS\\', \\'CURRENT_COUNTRY\\', \\'SALARY\\', \\'JOBLEVELCHANGETYPE\\', \\'AGE\\', \\'CHANGE_IN_PERFORMANCE_RATING\\', \\'PREVIOUS_COUNTRY\\', \\'LINKEDIN\\', \\'TENURE_MONTHS\\', \\'FUTURE_LEADER\\', \\'PREVIOUS_PERFORMANCE_RATING\\', \\'IMPACT_OF_LOSS\\', \\'PREVIOUS_FUNCTIONAL_AREA\\', \\'CURRENT_JOB_LEVEL\\', \\'CRITICAL_JOB_ROLE\\', \\'GENDER\\', \\'TIMEINPREVPOS_INT\\', \\'CURRENT_FUNCTIONAL_AREA\\', \\'CURRENT_PERFORMANCE_RATING\\', \\'MGR_EMP\\', \\'HIGH_POTENTIAL\\', \\'FLIGHT_RISK\\']\\nlc_col_name: STAT_VALUE_top29, len(lc_cols_imp): 31\\n[\\'EMPLOYEE_ID\\', \\'HRTRAINING\\', \\'FUNCTIONALAREACHANGETYPE\\', \\'SICKDAYS\\', \\'PROMOTION_WITHIN_LAST_3_YEARS\\', \\'TIMEINPREVPOSITIONMONTH\\', \\'EMPLOYMENT_TYPE_2\\', \\'PREVIOUS_CAREER_PATH\\', \\'RISK_OF_LOSS\\', \\'CURRENT_COUNTRY\\', \\'SALARY\\', \\'JOBLEVELCHANGETYPE\\', \\'AGE\\', \\'CHANGE_IN_PERFORMANCE_RATING\\', \\'PREVIOUS_COUNTRY\\', \\'LINKEDIN\\', \\'TENURE_MONTHS\\', \\'FUTURE_LEADER\\', \\'PREVIOUS_PERFORMANCE_RATING\\', \\'IMPACT_OF_LOSS\\', \\'PREVIOUS_FUNCTIONAL_AREA\\', \\'CURRENT_JOB_LEVEL\\', \\'CRITICAL_JOB_ROLE\\', \\'GENDER\\', \\'TIMEINPREVPOS_INT\\', \\'CURRENT_FUNCTIONAL_AREA\\', \\'CURRENT_PERFORMANCE_RATING\\', \\'MGR_EMP\\', \\'HIGH_POTENTIAL\\', \\'CHANGED_POSITION_WITHIN_LAST_2_YEARS\\', \\'FLIGHT_RISK\\']\\nCPU times: total: 109 ms\\nWall time: 21min 6s\\nIn\\xa0[213]:\\ndf_model_statistics_top_f\\nOut[213]:\\nSTAT_NAME\\nCLASS_NAME\\nSTAT_VALUE_top1\\nSTAT_VALUE_top2\\nSTAT_VALUE_top3\\nSTAT_VALUE_top4\\nSTAT_VALUE_top5\\nSTAT_VALUE_top6\\nSTAT_VALUE_top7\\nSTAT_VALUE_top8\\n...\\nSTAT_VALUE_top20\\nSTAT_VALUE_top21\\nSTAT_VALUE_top22\\nSTAT_VALUE_top23\\nSTAT_VALUE_top24\\nSTAT_VALUE_top25\\nSTAT_VALUE_top26\\nSTAT_VALUE_top27\\nSTAT_VALUE_top28\\nSTAT_VALUE_top29\\n0\\nAUC\\nNone\\n0.7485541309105562\\n0.7668495574205755\\n0.8898981031968496\\n0.909775403256946\\n0.9523481207329952\\n0.9582288581905858\\n0.9587397857819674\\n0.9625909111312445\\n...\\n0.9775155501936156\\n0.9801667589113028\\n0.9783512132677885\\n0.9794189376443134\\n0.9798465643252852\\n0.9803230976917793\\n0.9790858874113486\\n0.9804917245071063\\n0.9798712784627935\\n0.9811244739271372\\n1\\nRECALL\\nNo\\n0.7057265806024495\\n0.7057265806024495\\n0.7735849056603774\\n0.8189341277722608\\n0.8785170473353194\\n0.8904336312479312\\n0.8881165177093677\\n0.8907646474677259\\n...\\n0.9152598477325389\\n0.922211188348229\\n0.9212181396888448\\n0.9205561072492552\\n0.9169149288315127\\n0.9195630585898709\\n0.9205561072492552\\n0.9248593181065872\\n0.9248593181065872\\n0.9228732207878186\\n2\\nPRECISION\\nNo\\n0.8548516439454691\\n0.8548516439454691\\n0.8626799557032115\\n0.8742049469964664\\n0.8972278566599053\\n0.893687707641196\\n0.9027590847913862\\n0.9078947368421053\\n...\\n0.9372881355932203\\n0.943127962085308\\n0.9449915110356536\\n0.9417541483237386\\n0.9499314128943759\\n0.9487704918032787\\n0.9439918533604889\\n0.9509870660313138\\n0.9461564510667119\\n0.9534883720930233\\n3\\nF1_SCORE\\nNo\\n0.7731640979147779\\n0.7731640979147779\\n0.8157068062827224\\n0.8456674072808068\\n0.8877738752299715\\n0.8920577018736527\\n0.8953779409310862\\n0.8992481203007517\\n...\\n0.9261430246189918\\n0.93255230125523\\n0.9329534026148174\\n0.9310344827586207\\n0.9331312110493515\\n0.9339384770549671\\n0.9321266968325792\\n0.9377412317502937\\n0.9353866755942416\\n0.9379310344827586\\n4\\nSUPPORT\\nNo\\n3021\\n3021\\n3021\\n3021\\n3021\\n3021\\n3021\\n3021\\n...\\n3021\\n3021\\n3021\\n3021\\n3021\\n3021\\n3021\\n3021\\n3021\\n3021\\n5\\nRECALL\\nYes\\n0.8653273809523809\\n0.8653273809523809\\n0.8616071428571429\\n0.8675595238095238\\n0.8869047619047619\\n0.8809523809523809\\n0.8924851190476191\\n0.8984375\\n...\\n0.9311755952380952\\n0.9375\\n0.9397321428571429\\n0.9360119047619048\\n0.9456845238095238\\n0.9441964285714286\\n0.9386160714285714\\n0.9464285714285714\\n0.9408482142857143\\n0.9494047619047619\\n6\\nPRECISION\\nYes\\n0.7234836702954899\\n0.7234836702954899\\n0.772\\n0.8100034734282737\\n0.8665939658306071\\n0.8773619859207114\\n0.8765071245889661\\n0.8797814207650273\\n...\\n0.9072127582457412\\n0.9147005444646098\\n0.9138929088277858\\n0.9129172714078374\\n0.9101324740422485\\n0.912621359223301\\n0.9131378935939196\\n0.9180801154817755\\n0.9176342525399129\\n0.9163375224416517\\n7\\nF1_SCORE\\nYes\\n0.7880738607487717\\n0.7880738607487717\\n0.8143459915611814\\n0.8377941440632296\\n0.876631733774591\\n0.8791535177278633\\n0.8844239631336407\\n0.8890115958034235\\n...\\n0.9190380025702222\\n0.9259599485577806\\n0.9266324284666178\\n0.9243203526818515\\n0.9275679620507207\\n0.9281404278661547\\n0.9257017061089707\\n0.9320388349514563\\n0.929096252755327\\n0.9325781107253791\\n8\\nSUPPORT\\nYes\\n2688\\n2688\\n2688\\n2688\\n2688\\n2688\\n2688\\n2688\\n...\\n2688\\n2688\\n2688\\n2688\\n2688\\n2688\\n2688\\n2688\\n2688\\n2688\\n9\\nACCURACY\\nNone\\n0.7808723068838676\\n0.7808723068838676\\n0.815028901734104\\n0.8418286915396742\\n0.882466281310212\\n0.8859695218076721\\n0.8901734104046243\\n0.8943772990015765\\n...\\n0.9227535470310037\\n0.929409703976178\\n0.929935190050797\\n0.9278332457523208\\n0.930460676125416\\n0.931161324224908\\n0.929059379926432\\n0.9350148887721142\\n0.932387458399019\\n0.9353652128218602\\n10\\nKAPPA\\nNone\\n0.5649494008639051\\n0.5649494008639051\\n0.6311544374501676\\n0.6838156756610553\\n0.7644343595065145\\n0.7712120716850485\\n0.7798181644049321\\n0.7882808707593545\\n...\\n0.845205019336853\\n0.8585317766661889\\n0.8596107610601711\\n0.8553753979760864\\n0.8607463531422392\\n0.8621155595870541\\n0.8578529855548467\\n0.8698076378727402\\n0.8645021932494401\\n0.8705465803677053\\n11\\nMCC\\nNone\\n0.5746831059848179\\n0.5746831059848179\\n0.6349359504833003\\n0.6853500835196453\\n0.7646213973645677\\n0.7712178345480558\\n0.7799336372485263\\n0.7884387833514255\\n...\\n0.8454676150898258\\n0.8587693315233239\\n0.8599167308267556\\n0.8556191903658027\\n0.8613307367718015\\n0.8625748567444109\\n0.858150355182816\\n0.8701768271132071\\n0.8647485868867905\\n0.8710510757576019\\n12 rows √ó 31 columns\\nIn\\xa0[214]:\\ndf_predicted_statistics_top_f\\nOut[214]:\\nSTAT_NAME\\nCLASS_NAME\\nSTAT_VALUE_top1\\nSTAT_VALUE_top2\\nSTAT_VALUE_top3\\nSTAT_VALUE_top4\\nSTAT_VALUE_top5\\nSTAT_VALUE_top6\\nSTAT_VALUE_top7\\nSTAT_VALUE_top8\\n...\\nSTAT_VALUE_top20\\nSTAT_VALUE_top21\\nSTAT_VALUE_top22\\nSTAT_VALUE_top23\\nSTAT_VALUE_top24\\nSTAT_VALUE_top25\\nSTAT_VALUE_top26\\nSTAT_VALUE_top27\\nSTAT_VALUE_top28\\nSTAT_VALUE_top29\\n0\\nAUC\\nNone\\n0.7562074829931973\\n0.7714795918367346\\n0.8700680272108844\\n0.8938548752834468\\n0.9353514739229025\\n0.9526814058956916\\n0.9532369614512471\\n0.9484467120181406\\n...\\n0.9373242630385488\\n0.9392573696145124\\n0.9322363945578231\\n0.9412556689342404\\n0.9351417233560091\\n0.9408956916099773\\n0.9378826530612245\\n0.9403004535147392\\n0.9398554421768708\\n0.9472278911564626\\n1\\nRECALL\\nNo\\n0.7095238095238096\\n0.7095238095238096\\n0.8428571428571429\\n0.8476190476190476\\n0.8809523809523809\\n0.9285714285714286\\n0.919047619047619\\n0.9095238095238095\\n...\\n0.9285714285714286\\n0.919047619047619\\n0.9428571428571428\\n0.9095238095238095\\n0.9380952380952381\\n0.9476190476190476\\n0.9333333333333333\\n0.9476190476190476\\n0.9428571428571428\\n0.9428571428571428\\n2\\nPRECISION\\nNo\\n0.8142076502732241\\n0.8142076502732241\\n0.8082191780821918\\n0.8165137614678899\\n0.8295964125560538\\n0.8590308370044053\\n0.8577777777777778\\n0.8642533936651584\\n...\\n0.8333333333333334\\n0.8247863247863247\\n0.8319327731092437\\n0.8526785714285714\\n0.8312236286919831\\n0.8361344537815126\\n0.8235294117647058\\n0.8189300411522634\\n0.825\\n0.8461538461538461\\n3\\nF1_SCORE\\nNo\\n0.7582697201017813\\n0.7582697201017813\\n0.8251748251748252\\n0.8317757009345795\\n0.8545034642032333\\n0.8924485125858124\\n0.8873563218390804\\n0.8863109048723898\\n...\\n0.8783783783783784\\n0.8693693693693693\\n0.8839285714285715\\n0.880184331797235\\n0.8814317673378076\\n0.8883928571428572\\n0.8749999999999999\\n0.8785871964679911\\n0.88\\n0.8918918918918919\\n4\\nSUPPORT\\nNo\\n210\\n210\\n210\\n210\\n210\\n210\\n210\\n210\\n...\\n210\\n210\\n210\\n210\\n210\\n210\\n210\\n210\\n210\\n210\\n5\\nRECALL\\nYes\\n0.8380952380952381\\n0.8380952380952381\\n0.8\\n0.8095238095238095\\n0.819047619047619\\n0.8476190476190476\\n0.8476190476190476\\n0.8571428571428571\\n...\\n0.8142857142857143\\n0.8047619047619048\\n0.8095238095238095\\n0.8428571428571429\\n0.8095238095238095\\n0.8142857142857143\\n0.8\\n0.7904761904761904\\n0.8\\n0.8285714285714286\\n6\\nPRECISION\\nYes\\n0.7426160337552743\\n0.7426160337552743\\n0.835820895522388\\n0.8415841584158416\\n0.8730964467005076\\n0.9222797927461139\\n0.9128205128205128\\n0.9045226130653267\\n...\\n0.9193548387096774\\n0.9086021505376344\\n0.9340659340659341\\n0.9030612244897959\\n0.9289617486338798\\n0.9395604395604396\\n0.9230769230769231\\n0.9378531073446328\\n0.9333333333333333\\n0.9354838709677419\\n7\\nF1_SCORE\\nYes\\n0.7874720357941835\\n0.7874720357941835\\n0.8175182481751825\\n0.8252427184466018\\n0.8452088452088452\\n0.8833746898263027\\n0.8790123456790123\\n0.8801955990220048\\n...\\n0.8636363636363636\\n0.8535353535353536\\n0.8673469387755102\\n0.8719211822660099\\n0.8651399491094147\\n0.8724489795918368\\n0.8571428571428571\\n0.8578811369509044\\n0.8615384615384616\\n0.8787878787878788\\n8\\nSUPPORT\\nYes\\n210\\n210\\n210\\n210\\n210\\n210\\n210\\n210\\n...\\n210\\n210\\n210\\n210\\n210\\n210\\n210\\n210\\n210\\n210\\n9\\nACCURACY\\nNone\\n0.7738095238095238\\n0.7738095238095238\\n0.8214285714285714\\n0.8285714285714286\\n0.85\\n0.888095238095238\\n0.8833333333333333\\n0.8833333333333333\\n...\\n0.8714285714285714\\n0.861904761904762\\n0.8761904761904762\\n0.8761904761904762\\n0.8738095238095238\\n0.8809523809523809\\n0.8666666666666667\\n0.8690476190476191\\n0.8714285714285714\\n0.8857142857142857\\n10\\nKAPPA\\nNone\\n0.5476190476190477\\n0.5476190476190477\\n0.6428571428571429\\n0.6571428571428571\\n0.7\\n0.7761904761904762\\n0.7666666666666667\\n0.7666666666666667\\n...\\n0.7428571428571429\\n0.7238095238095238\\n0.7523809523809524\\n0.7523809523809524\\n0.7476190476190476\\n0.7619047619047619\\n0.7333333333333333\\n0.7380952380952381\\n0.7428571428571429\\n0.7714285714285715\\n11\\nMCC\\nNone\\n0.5522021871918072\\n0.5522021871918072\\n0.6434483363899111\\n0.657620215133451\\n0.7013451372039254\\n0.778746344940852\\n0.7686299648023246\\n0.7677206122629325\\n...\\n0.747756501105966\\n0.7285832574878643\\n0.7591592960815526\\n0.7540585039671973\\n0.7538760294705542\\n0.768768907424357\\n0.7399400733959437\\n0.7473807852602852\\n0.7505553499465134\\n0.7765163665331186\\n12 rows √ó 31 columns\\nIn\\xa0[215]:\\nf_f1_score_no_descending(df_predicted_statistics_top_f)[:10]\\nOut[215]:\\nCLASS_NAME                          No\\nSTAT_NAME                     F1_SCORE\\nSTAT_VALUE_top15    0.8974943052391801\\nSTAT_VALUE_top6     0.8924485125858124\\nSTAT_VALUE_top18    0.8923076923076922\\nSTAT_VALUE_top29    0.8918918918918919\\nSTAT_VALUE_top19     0.888888888888889\\nSTAT_VALUE_top25    0.8883928571428572\\nSTAT_VALUE_top7     0.8873563218390804\\nSTAT_VALUE_top8     0.8863109048723898\\nName: 3, dtype: object\\nIn\\xa0[216]:\\nf_f1_score_yes_descending(df_predicted_statistics_top_f)[:10]\\nOut[216]:\\nCLASS_NAME                         Yes\\nSTAT_NAME                     F1_SCORE\\nSTAT_VALUE_top15    0.8877805486284288\\nSTAT_VALUE_top6     0.8833746898263027\\nSTAT_VALUE_top8     0.8801955990220048\\nSTAT_VALUE_top7     0.8790123456790123\\nSTAT_VALUE_top29    0.8787878787878788\\nSTAT_VALUE_top19    0.8771929824561403\\nSTAT_VALUE_top18    0.8727272727272728\\nSTAT_VALUE_top25    0.8724489795918368\\nName: 7, dtype: object\\nIn\\xa0[217]:\\n# SUPPORT Yes = SUPPORT No ACCURACY is reliable\\nf_accuracy_descending(df_predicted_statistics_top_f)[:10]\\nOut[217]:\\nSTAT_NAME                     ACCURACY\\nSTAT_VALUE_top15    0.8928571428571429\\nSTAT_VALUE_top6      0.888095238095238\\nSTAT_VALUE_top29    0.8857142857142857\\nSTAT_VALUE_top7     0.8833333333333333\\nSTAT_VALUE_top18    0.8833333333333333\\nSTAT_VALUE_top8     0.8833333333333333\\nSTAT_VALUE_top19    0.8833333333333333\\nSTAT_VALUE_top25    0.8809523809523809\\nSTAT_VALUE_top23    0.8761904761904762\\nName: 9, dtype: object\\nIn\\xa0[218]:\\ndef f_sel_top_pred_acc(p_df,p_n):\\n    s =  f_accuracy_descending(p_df) # sorted series\\n    ls_s = s[:p_n+1].index.to_list() # sorted list with last n+1\\n    key = ls_s[-1] # last element\\n    model = dict_models_top_f[key][0]\\n    cols = dict_models_top_f[key][1]\\n    cols_len = dict_models_top_f[key][2]\\n    cols_name = dict_models_top_f[key][3]\\n    return [key, model, cols, cols_len, cols_name]\\nIn\\xa0[219]:\\n# key, model, cols, cols_len, cols_name Top 1\\ntop_1_imp_ls = f_sel_top_pred_acc(df_predicted_statistics_top_f,1)\\ntop_1_imp_ls[0], top_1_imp_ls[1], top_1_imp_ls[2], top_1_imp_ls[3], top_1_imp_ls[4]\\nOut[219]:\\n(\\'STAT_VALUE_top15\\',\\n <hana_ml.algorithms.pal.unified_classification.UnifiedClassification at 0x25c02d19430>,\\n [\\'EMPLOYEE_ID\\',\\n  \\'HRTRAINING\\',\\n  \\'FUNCTIONALAREACHANGETYPE\\',\\n  \\'SICKDAYS\\',\\n  \\'PROMOTION_WITHIN_LAST_3_YEARS\\',\\n  \\'TIMEINPREVPOSITIONMONTH\\',\\n  \\'EMPLOYMENT_TYPE_2\\',\\n  \\'PREVIOUS_CAREER_PATH\\',\\n  \\'RISK_OF_LOSS\\',\\n  \\'CURRENT_COUNTRY\\',\\n  \\'SALARY\\',\\n  \\'JOBLEVELCHANGETYPE\\',\\n  \\'AGE\\',\\n  \\'CHANGE_IN_PERFORMANCE_RATING\\',\\n  \\'PREVIOUS_COUNTRY\\',\\n  \\'LINKEDIN\\',\\n  \\'FLIGHT_RISK\\'],\\n 17,\\n \\'top15\\')\\nIn\\xa0[220]:\\n# dict_models_top_f\\nIn\\xa0[221]:\\n# CHEAT SHEET\\n# f_recall_yes_descending(df)\\n# f_f1_score_yes_descending(df)\\n# f_auc_descending(df)\\n# f_f1_score_no_descending(df)\\nIn\\xa0[222]:\\ntop_1_imp_ls[1].statistics_.collect()\\nOut[222]:\\nSTAT_NAME\\nSTAT_VALUE\\nCLASS_NAME\\n0\\nAUC\\n0.9758036474507881\\nNone\\n1\\nRECALL\\n0.9129427341939755\\nNo\\n2\\nPRECISION\\n0.9419398907103825\\nNo\\n3\\nF1_SCORE\\n0.9272146579257019\\nNo\\n4\\nSUPPORT\\n3021\\nNo\\n5\\nRECALL\\n0.9367559523809523\\nYes\\n6\\nPRECISION\\n0.9054297015462064\\nYes\\n7\\nF1_SCORE\\n0.9208264765039311\\nYes\\n8\\nSUPPORT\\n2688\\nYes\\n9\\nACCURACY\\n0.9241548432299878\\nNone\\n10\\nKAPPA\\n0.8480815198503675\\nNone\\n11\\nMCC\\n0.8485333402901478\\nNone\\nIn\\xa0[223]:\\n# top_imp_best_model.confusion_matrix_.collect()\\ntop_1_imp_ls[1].confusion_matrix_.collect()\\nOut[223]:\\nACTUAL_CLASS\\nPREDICTED_CLASS\\nCOUNT\\n0\\nNo\\nNo\\n2758\\n1\\nNo\\nYes\\n263\\n2\\nYes\\nNo\\n170\\n3\\nYes\\nYes\\n2518\\nIn\\xa0[224]:\\nf_score_res(top_1_imp_ls[1], d_test_bl)[1].collect()\\nOut[224]:\\nSTAT_NAME\\nSTAT_VALUE\\nCLASS_NAME\\n0\\nAUC\\n0.9498951247165532\\nNone\\n1\\nRECALL\\n0.9380952380952381\\nNo\\n2\\nPRECISION\\n0.8602620087336245\\nNo\\n3\\nF1_SCORE\\n0.8974943052391801\\nNo\\n4\\nSUPPORT\\n210\\nNo\\n5\\nRECALL\\n0.8476190476190476\\nYes\\n6\\nPRECISION\\n0.9319371727748691\\nYes\\n7\\nF1_SCORE\\n0.8877805486284288\\nYes\\n8\\nSUPPORT\\n210\\nYes\\n9\\nACCURACY\\n0.8928571428571429\\nNone\\n10\\nKAPPA\\n0.7857142857142857\\nNone\\n11\\nMCC\\n0.7889500706904004\\nNone\\nIn\\xa0[225]:\\nf_score_res(top_1_imp_ls[1], d_test_bl)[2].collect()\\nOut[225]:\\nACTUAL_CLASS\\nPREDICTED_CLASS\\nCOUNT\\n0\\nNo\\nNo\\n197\\n1\\nNo\\nYes\\n13\\n2\\nYes\\nNo\\n32\\n3\\nYes\\nYes\\n178\\nIn\\xa0[226]:\\n# RECALL    Yes: 1 - Yes_No/(Yes_No + Yes_Yes)\\n# PRECISION Yes: 1 - No_Yes/(No_Yes + Yes_Yes)\\nIn\\xa0[227]:\\ntop_1_imp_ls[4]\\nOut[227]:\\n\\'top15\\'\\nIn\\xa0[228]:\\ndf_model_stat_last, df_pred_stat_last = f_stat_all(top_1_imp_ls[1], d_test_bl, top_1_imp_ls[4])\\nIn\\xa0[229]:\\ndf_model_stat_last\\nOut[229]:\\nSTAT_NAME\\nCLASS_NAME\\nINIT\\nADD\\ncols_imp\\ncols_imp_mn\\nsm_max_cols_all\\nsm_max_cols_imp\\nsm_max_bl_cols_imp\\nsm_best_bl_cols_imp\\ntop15\\n0\\nAUC\\nNone\\n0.9743\\n0.9830\\n0.9856\\n0.9843\\n0.9868\\n0.9776\\n0.9818\\n0.9805\\n0.9758\\n1\\nRECALL\\nNo\\n0.9959\\n0.9790\\n0.9753\\n0.9739\\n0.9320\\n0.9015\\n0.9152\\n0.9258\\n0.9129\\n2\\nPRECISION\\nNo\\n0.9249\\n0.9531\\n0.9567\\n0.9529\\n0.9606\\n0.9456\\n0.9518\\n0.9500\\n0.9419\\n3\\nF1_SCORE\\nNo\\n0.9591\\n0.9659\\n0.9659\\n0.9632\\n0.9461\\n0.9230\\n0.9331\\n0.9378\\n0.9272\\n4\\nSUPPORT\\nNo\\n2722\\n2722\\n2722\\n2722\\n2722\\n2722\\n3021\\n3021\\n3021\\n5\\nRECALL\\nYes\\n0.3452\\n0.6101\\n0.6428\\n0.6101\\n0.9617\\n0.9481\\n0.9536\\n0.9453\\n0.9367\\n6\\nPRECISION\\nYes\\n0.9133\\n0.7824\\n0.7632\\n0.7427\\n0.9339\\n0.9059\\n0.9183\\n0.9189\\n0.9054\\n7\\nF1_SCORE\\nYes\\n0.5010\\n0.6856\\n0.6978\\n0.6699\\n0.9476\\n0.9265\\n0.9356\\n0.9319\\n0.9208\\n8\\nSUPPORT\\nYes\\n336\\n336\\n336\\n336\\n2722\\n2722\\n3020\\n2688\\n2688\\n9\\nACCURACY\\nNone\\n0.9244\\n0.9385\\n0.9388\\n0.9339\\n0.9469\\n0.9248\\n0.9344\\n0.9350\\n0.9241\\n10\\nKAPPA\\nNone\\n0.4690\\n0.6521\\n0.6641\\n0.6336\\n0.8938\\n0.8497\\n0.8688\\n0.8697\\n0.8480\\n11\\nMCC\\nNone\\n0.5348\\n0.6583\\n0.6671\\n0.6374\\n0.8942\\n0.8506\\n0.8695\\n0.8701\\n0.8485\\nIn\\xa0[230]:\\ndf_pred_stat_last\\nOut[230]:\\nSTAT_NAME\\nCLASS_NAME\\nINIT\\nADD\\ncols_imp\\ncols_imp_mn\\nsm_max_cols_all\\nsm_max_cols_imp\\nsm_max_bl_cols_imp\\nsm_best_bl_cols_imp\\ntop15\\n0\\nAUC\\nNone\\n0.9733\\n0.9834\\n0.9857\\n0.9870\\n0.9784\\n0.9709\\n0.9422\\n0.9474\\n0.9498\\n1\\nRECALL\\nNo\\n0.9964\\n0.9800\\n0.9782\\n0.9811\\n0.9453\\n0.9195\\n0.9190\\n0.9476\\n0.9380\\n2\\nPRECISION\\nNo\\n0.9247\\n0.9488\\n0.9541\\n0.9548\\n0.9751\\n0.9726\\n0.8427\\n0.8257\\n0.8602\\n3\\nF1_SCORE\\nNo\\n0.9592\\n0.9641\\n0.9660\\n0.9678\\n0.9600\\n0.9453\\n0.8792\\n0.8824\\n0.8974\\n4\\nSUPPORT\\nNo\\n1702\\n1702\\n1702\\n1702\\n1702\\n1702\\n210\\n210\\n210\\n5\\nRECALL\\nYes\\n0.3428\\n0.5714\\n0.6190\\n0.6238\\n0.8047\\n0.7904\\n0.8285\\n0.8\\n0.8476\\n6\\nPRECISION\\nYes\\n0.9230\\n0.7792\\n0.7784\\n0.8036\\n0.6450\\n0.5478\\n0.9109\\n0.9385\\n0.9319\\n7\\nF1_SCORE\\nYes\\n0.5000\\n0.6593\\n0.6896\\n0.7024\\n0.7161\\n0.6471\\n0.8678\\n0.8637\\n0.8877\\n8\\nSUPPORT\\nYes\\n210\\n210\\n210\\n210\\n210\\n210\\n210\\n210\\n210\\n9\\nACCURACY\\nNone\\n0.9246\\n0.9351\\n0.9388\\n0.9419\\n0.9299\\n0.9053\\n0.8738\\n0.8738\\n0.8928\\n10\\nKAPPA\\nNone\\n0.4683\\n0.6244\\n0.6562\\n0.6708\\n0.6766\\n0.5945\\n0.7476\\n0.7476\\n0.7857\\n11\\nMCC\\nNone\\n0.5363\\n0.6336\\n0.6615\\n0.6774\\n0.6820\\n0.6079\\n0.7506\\n0.7559\\n0.7889\\nIn\\xa0[231]:\\nf_score_res(top_1_imp_ls[1], d_test_bl)[0].sort_values([tab_id]).head(5).collect() # prediction\\nOut[231]:\\nEMPLOYEE_ID\\nSCORE\\nCONFIDENCE\\nREASON_CODE\\n0\\n10033\\nNo\\n0.998697\\n[{\"attr\":\"SICKDAYS\",\"val\":-2.3005012586457966,...\\n1\\n10034\\nNo\\n0.997572\\n[{\"attr\":\"SICKDAYS\",\"val\":-2.1461261874013997,...\\n2\\n10036\\nYes\\n0.884731\\n[{\"attr\":\"HRTRAINING\",\"val\":1.2600622007181038...\\n3\\n10056\\nYes\\n0.992384\\n[{\"attr\":\"SICKDAYS\",\"val\":3.2852997983870959,\"...\\n4\\n10064\\nNo\\n0.985765\\n[{\"attr\":\"HRTRAINING\",\"val\":-1.738132753015509...\\nIn\\xa0[232]:\\nd_test_bl[[tab_id,target]].sort_values([tab_id]).head(5).collect() # actual\\nOut[232]:\\nEMPLOYEE_ID\\nFLIGHT_RISK\\n0\\n10033\\nNo\\n1\\n10034\\nNo\\n2\\n10036\\nYes\\n3\\n10056\\nYes\\n4\\n10064\\nNo\\nIn\\xa0[233]:\\n# Top 1 contribution feauture to prediction\\nf_score_res(top_1_imp_ls[1], d_test_bl)[0].\\\\\\n    select(\\'EMPLOYEE_ID\\', \\'SCORE\\', \\'CONFIDENCE\\', \\'REASON_CODE\\', \\n                      (\\'json_query(\"REASON_CODE\", \\\\\\'$[0].attr\\\\\\')\\', \\'Top1\\'), \\n                      (\\'json_query(\"REASON_CODE\", \\\\\\'$[0].pct\\\\\\')\\', \\'PCT_1\\') ).head(5).collect()\\nOut[233]:\\nEMPLOYEE_ID\\nSCORE\\nCONFIDENCE\\nREASON_CODE\\nTop1\\nPCT_1\\n0\\n10033\\nNo\\n0.998697\\n[{\"attr\":\"SICKDAYS\",\"val\":-2.3005012586457966,...\\n\"SICKDAYS\"\\n28.49847521037374\\n1\\n10034\\nNo\\n0.997572\\n[{\"attr\":\"SICKDAYS\",\"val\":-2.1461261874013997,...\\n\"SICKDAYS\"\\n28.604720941656255\\n2\\n10036\\nYes\\n0.884731\\n[{\"attr\":\"HRTRAINING\",\"val\":1.2600622007181038...\\n\"HRTRAINING\"\\n17.883981590687534\\n3\\n10056\\nYes\\n0.992384\\n[{\"attr\":\"SICKDAYS\",\"val\":3.2852997983870959,\"...\\n\"SICKDAYS\"\\n30.97615277828276\\n4\\n10064\\nNo\\n0.985765\\n[{\"attr\":\"HRTRAINING\",\"val\":-1.738132753015509...\\n\"HRTRAINING\"\\n27.416934166112573\\nIn\\xa0[234]:\\ndef f_actual_vs_score(p_model, p_d_test): # actual and prediction\\n    pred_res = f_score_res(p_model, p_d_test)[0]\\n    d_actual_vs_score = p_d_test.set_index(tab_id).join(pred_res.set_index(tab_id))\\n    d_actual_vs_score = d_actual_vs_score[[\\'EMPLOYEE_ID\\',\\'FLIGHT_RISK\\',\\'SCORE\\',\\'CONFIDENCE\\',\\'REASON_CODE\\']]\\n    return d_actual_vs_score\\nIn\\xa0[235]:\\n# VOTING of Top1, Top2, Top3 models\\nIn\\xa0[236]:\\nprint(f\"Top 1 voting with features {top_1_imp_ls[4]}\") # Top 2 voting\\nf_actual_vs_score(top_1_imp_ls[1], d_test_bl).head(5).collect()\\nTop 1 voting with features top15\\nOut[236]:\\nEMPLOYEE_ID\\nFLIGHT_RISK\\nSCORE\\nCONFIDENCE\\nREASON_CODE\\n0\\n10033\\nNo\\nNo\\n0.998697\\n[{\"attr\":\"SICKDAYS\",\"val\":-2.3005012586457966,...\\n1\\n10034\\nNo\\nNo\\n0.997572\\n[{\"attr\":\"SICKDAYS\",\"val\":-2.1461261874013997,...\\n2\\n10036\\nYes\\nYes\\n0.884731\\n[{\"attr\":\"HRTRAINING\",\"val\":1.2600622007181038...\\n3\\n10056\\nYes\\nYes\\n0.992384\\n[{\"attr\":\"SICKDAYS\",\"val\":3.2852997983870959,\"...\\n4\\n10064\\nNo\\nNo\\n0.985765\\n[{\"attr\":\"HRTRAINING\",\"val\":-1.738132753015509...\\nIn\\xa0[237]:\\ntop_2_imp_ls = f_sel_top_pred_acc(df_predicted_statistics_top_f,2) # Top 2 selection\\ntop_3_imp_ls = f_sel_top_pred_acc(df_predicted_statistics_top_f,3) # Top 3 selection\\nIn\\xa0[238]:\\nprint(f\"Top 2 voting with features {top_2_imp_ls[4]}\") # Top 2 voting\\nf_actual_vs_score(top_2_imp_ls[1], d_test_bl).head(5).collect() \\nTop 2 voting with features top6\\nOut[238]:\\nEMPLOYEE_ID\\nFLIGHT_RISK\\nSCORE\\nCONFIDENCE\\nREASON_CODE\\n0\\n10033\\nNo\\nNo\\n0.994945\\n[{\"attr\":\"SICKDAYS\",\"val\":-2.2233301283208748,...\\n1\\n10034\\nNo\\nNo\\n0.994841\\n[{\"attr\":\"SICKDAYS\",\"val\":-1.9678355279055906,...\\n2\\n10036\\nYes\\nYes\\n0.819114\\n[{\"attr\":\"HRTRAINING\",\"val\":1.4050784746928247...\\n3\\n10056\\nYes\\nYes\\n0.978584\\n[{\"attr\":\"SICKDAYS\",\"val\":2.6587680774372766,\"...\\n4\\n10064\\nNo\\nNo\\n0.983785\\n[{\"attr\":\"HRTRAINING\",\"val\":-1.885638333861579...\\nIn\\xa0[239]:\\nprint(f\"Top 3 voting with features {top_3_imp_ls[4]}\") # Top 3 voting\\nf_actual_vs_score(top_3_imp_ls[1], d_test_bl).head(5).collect()\\nTop 3 voting with features top29\\nOut[239]:\\nEMPLOYEE_ID\\nFLIGHT_RISK\\nSCORE\\nCONFIDENCE\\nREASON_CODE\\n0\\n10033\\nNo\\nNo\\n0.998340\\n[{\"attr\":\"SICKDAYS\",\"val\":-2.191292869324807,\"...\\n1\\n10034\\nNo\\nNo\\n0.997552\\n[{\"attr\":\"SICKDAYS\",\"val\":-2.5425011214054464,...\\n2\\n10036\\nYes\\nNo\\n0.658788\\n[{\"attr\":\"HRTRAINING\",\"val\":1.080771396301631,...\\n3\\n10056\\nYes\\nYes\\n0.940803\\n[{\"attr\":\"SICKDAYS\",\"val\":2.6118364815537786,\"...\\n4\\n10064\\nNo\\nNo\\n0.996290\\n[{\"attr\":\"HRTRAINING\",\"val\":-1.379976997337837...\\nIn\\xa0[240]:\\n# Top 5 contribution feauture to prediction for rows\\ndef f_contrib_top5f_head(p_model, p_d_test, p_rows):\\n    df_contribution_top5 = pd.DataFrame()\\n    \\n    df_contribution_top = f_score_res(p_model, p_d_test)[0].\\\\\\n    select(\\'EMPLOYEE_ID\\', \\'SCORE\\', \\'CONFIDENCE\\', \\'REASON_CODE\\', \\n                      (\\'json_query(\"REASON_CODE\", \\\\\\'$[0].attr\\\\\\')\\', \\'Top\\'), \\n                      (\\'json_query(\"REASON_CODE\", \\\\\\'$[0].pct\\\\\\')\\', \\'PCT\\') ).head(p_rows).collect()\\n \\n    df_contribution_top5 = pd.concat([df_contribution_top5, df_contribution_top], axis=0, ignore_index=True)\\n    df_contribution_top = f_score_res(p_model, p_d_test)[0].\\\\\\n    select(\\'EMPLOYEE_ID\\', \\'SCORE\\', \\'CONFIDENCE\\', \\'REASON_CODE\\', \\n                      (\\'json_query(\"REASON_CODE\", \\\\\\'$[1].attr\\\\\\')\\', \\'Top\\'), \\n                      (\\'json_query(\"REASON_CODE\", \\\\\\'$[1].pct\\\\\\')\\', \\'PCT\\') ).head(p_rows).collect()\\n \\n    df_contribution_top5 = pd.concat([df_contribution_top5, df_contribution_top], axis=0, ignore_index=True)\\n    df_contribution_top = f_score_res(p_model, p_d_test)[0].\\\\\\n    select(\\'EMPLOYEE_ID\\', \\'SCORE\\', \\'CONFIDENCE\\', \\'REASON_CODE\\', \\n                      (\\'json_query(\"REASON_CODE\", \\\\\\'$[2].attr\\\\\\')\\', \\'Top\\'), \\n                      (\\'json_query(\"REASON_CODE\", \\\\\\'$[2].pct\\\\\\')\\', \\'PCT\\') ).head(p_rows).collect()\\n \\n    df_contribution_top5 = pd.concat([df_contribution_top5, df_contribution_top], axis=0, ignore_index=True)\\n   \\n    df_contribution_top = f_score_res(p_model, p_d_test)[0].\\\\\\n    select(\\'EMPLOYEE_ID\\', \\'SCORE\\', \\'CONFIDENCE\\', \\'REASON_CODE\\', \\n                      (\\'json_query(\"REASON_CODE\", \\\\\\'$[3].attr\\\\\\')\\', \\'Top\\'), \\n                      (\\'json_query(\"REASON_CODE\", \\\\\\'$[3].pct\\\\\\')\\', \\'PCT\\') ).head(p_rows).collect()\\n \\n    df_contribution_top5 = pd.concat([df_contribution_top5, df_contribution_top], axis=0, ignore_index=True)\\n    df_contribution_top = f_score_res(p_model, p_d_test)[0].\\\\\\n    select(\\'EMPLOYEE_ID\\', \\'SCORE\\', \\'CONFIDENCE\\', \\'REASON_CODE\\', \\n                      (\\'json_query(\"REASON_CODE\", \\\\\\'$[4].attr\\\\\\')\\', \\'Top\\'), \\n                      (\\'json_query(\"REASON_CODE\", \\\\\\'$[4].pct\\\\\\')\\', \\'PCT\\') ).head(p_rows).collect()\\n \\n    df_contribution_top5 = pd.concat([df_contribution_top5, df_contribution_top], axis=0, ignore_index=True)\\n    df_contribution_top5 = df_contribution_top5.sort_values(by=[tab_id])\\n    return df_contribution_top5\\nIn\\xa0[241]:\\n f_contrib_top5f_head(top_1_imp_ls[1], d_test_bl, 3)\\nOut[241]:\\nEMPLOYEE_ID\\nSCORE\\nCONFIDENCE\\nREASON_CODE\\nTop\\nPCT\\n0\\n10033\\nNo\\n0.998697\\n[{\"attr\":\"SICKDAYS\",\"val\":-2.3005012586457966,...\\n\"SICKDAYS\"\\n28.49847521037374\\n3\\n10033\\nNo\\n0.998697\\n[{\"attr\":\"SICKDAYS\",\"val\":-2.3005012586457966,...\\n\"HRTRAINING\"\\n20.004226264298525\\n6\\n10033\\nNo\\n0.998697\\n[{\"attr\":\"SICKDAYS\",\"val\":-2.3005012586457966,...\\n\"PREVIOUS_CAREER_PATH\"\\n13.468698519836725\\n9\\n10033\\nNo\\n0.998697\\n[{\"attr\":\"SICKDAYS\",\"val\":-2.3005012586457966,...\\n\"LINKEDIN\"\\n6.355941761599763\\n12\\n10033\\nNo\\n0.998697\\n[{\"attr\":\"SICKDAYS\",\"val\":-2.3005012586457966,...\\n\"FUNCTIONALAREACHANGETYPE\"\\n6.23681257187179\\n1\\n10034\\nNo\\n0.997572\\n[{\"attr\":\"SICKDAYS\",\"val\":-2.1461261874013997,...\\n\"SICKDAYS\"\\n28.604720941656255\\n4\\n10034\\nNo\\n0.997572\\n[{\"attr\":\"SICKDAYS\",\"val\":-2.1461261874013997,...\\n\"HRTRAINING\"\\n20.4976248325675\\n7\\n10034\\nNo\\n0.997572\\n[{\"attr\":\"SICKDAYS\",\"val\":-2.1461261874013997,...\\n\"PREVIOUS_CAREER_PATH\"\\n14.181244854940238\\n10\\n10034\\nNo\\n0.997572\\n[{\"attr\":\"SICKDAYS\",\"val\":-2.1461261874013997,...\\n\"EMPLOYMENT_TYPE_2\"\\n6.901302373136609\\n13\\n10034\\nNo\\n0.997572\\n[{\"attr\":\"SICKDAYS\",\"val\":-2.1461261874013997,...\\n\"CURRENT_COUNTRY\"\\n6.14777312433024\\n2\\n10036\\nYes\\n0.884731\\n[{\"attr\":\"HRTRAINING\",\"val\":1.2600622007181038...\\n\"HRTRAINING\"\\n17.883981590687534\\n5\\n10036\\nYes\\n0.884731\\n[{\"attr\":\"HRTRAINING\",\"val\":1.2600622007181038...\\n\"TIMEINPREVPOSITIONMONTH\"\\n16.32437512678746\\n8\\n10036\\nYes\\n0.884731\\n[{\"attr\":\"HRTRAINING\",\"val\":1.2600622007181038...\\n\"FUNCTIONALAREACHANGETYPE\"\\n11.815982986776549\\n11\\n10036\\nYes\\n0.884731\\n[{\"attr\":\"HRTRAINING\",\"val\":1.2600622007181038...\\n\"EMPLOYMENT_TYPE_2\"\\n10.839100430486204\\n14\\n10036\\nYes\\n0.884731\\n[{\"attr\":\"HRTRAINING\",\"val\":1.2600622007181038...\\n\"PREVIOUS_CAREER_PATH\"\\n9.579417179568948\\nIn\\xa0[242]:\\ntop_1_imp_ls[4]\\nOut[242]:\\n\\'top15\\'\\nIn\\xa0[243]:\\n# https://blogs.sap.com/2022/08/03/azure-machine-learning-triggering-calculations-ml-in-sap-data-warehouse-cloud/\\nfrom hana_ml.visualizers.unified_report import UnifiedReport\\n# UnifiedReport(df_remote).build().display()\\nUnifiedReport(top_1_imp_ls[1]).build().display()\\nIn order to review the unified classification model report better, you need to adjust the size of the left area or hide the left area temporarily!\\nIn\\xa0[244]:\\n# https://github.com/SAP-samples/hana-ml-samples/blob/main/Python-API/usecase-examples/sapcommunity-hanaml-challenge/PAL%20Tutorial%20-%20Unified%20Classification%20Hybrid%20Gradient%20Boosting%20-%20PredictiveQuality%20Example.ipynb\\n# Visualize Confusion Matrix\\nimport matplotlib.pyplot as plt\\nfrom hana_ml.visualizers.metrics import MetricsVisualizer\\nf, ax1 = plt.subplots(1,1)\\nmv1 = MetricsVisualizer(ax1, title = \\'Modedl confusion matrix\\')\\nax1 = mv1.plot_confusion_matrix(top_1_imp_ls[1].confusion_matrix_, normalize=False)\\nIn\\xa0[245]:\\nf, ax1 = plt.subplots(1,1)\\nmv1 = MetricsVisualizer(ax1, title = \\'Model confusion matrix %\\')\\nax1 = mv1.plot_confusion_matrix(top_1_imp_ls[1].confusion_matrix_, normalize=True)\\nIn\\xa0[246]:\\nd_test_bl.shape\\nOut[246]:\\n[420, 47]\\nIn\\xa0[257]:\\n# https://pypi.org/project/hana-ml/\\nfrom hana_ml.visualizers.model_debriefing import TreeModelDebriefing\\nfrom hana_ml.visualizers.model_debriefing import TreeModelDebriefing\\nfeatures_shapely = top_1_imp_ls[2] # Top 3 imp model\\nfeatures_shapely = features_shapely[1:]\\nfeatures_shapely = features_shapely[:-1]\\npred_res_shapely = f_score_res(top_1_imp_ls[1], d_test_bl)[0]\\nshapley_explainer = TreeModelDebriefing.shapley_explainer(feature_data=d_test_bl.head(500).select(features_shapely),\\n                                                         reason_code_data=pred_res_shapely.head(500).select(\\'REASON_CODE\\'))\\nshapley_explainer.summary_plot()\\nC:\\\\Users\\\\si\\\\anaconda3\\\\envs\\\\ml_hana\\\\lib\\\\site-packages\\\\hana_ml\\\\visualizers\\\\shap.py:1697: UserWarning: No data for colormapping provided via \\'c\\'. Parameters \\'vmin\\', \\'vmax\\' will be ignored\\n  plt.scatter(x, y,\\n1.Using Shapley values to show the distribution of the impacts each feature has on the model output.\\n2.The color represents the feature value (red high, blue low).\\n3.The plot below shows the relationship between feature value and Shapley value.\\n-- If the dots in the left area are blue and the dots in the right area are red, then it means that the feature value and the Shapley value are typically positive correlation.\\n-- If the dots in the left area are red and the dots in the right area are blue, then it means that the feature value and the Shapley value are typically negative correlation.\\n-- If all the dots are concentrated near 0, it means that the Shapley value has nothing to do with this feature.\\nIn\\xa0[258]:\\nf_score_res(top_1_imp_ls[1],d_test_bl)[2].collect()\\nOut[258]:\\nACTUAL_CLASS\\nPREDICTED_CLASS\\nCOUNT\\n0\\nNo\\nNo\\n197\\n1\\nNo\\nYes\\n13\\n2\\nYes\\nNo\\n32\\n3\\nYes\\nYes\\n178\\nIn\\xa0[259]:\\ndef f_cf_mx_values(p_model, p_d_test): \\n    cf_mx = f_score_res(p_model,p_d_test)[2].collect()\\n    TN = cf_mx.loc[(cf_mx[\\'ACTUAL_CLASS\\'] == \\'No\\') & (cf_mx[\\'PREDICTED_CLASS\\'] == \\'No\\'),\\'COUNT\\']\\n    FP = cf_mx.loc[(cf_mx[\\'ACTUAL_CLASS\\'] == \\'No\\') & (cf_mx[\\'PREDICTED_CLASS\\'] == \\'Yes\\'),\\'COUNT\\']\\n    TP = cf_mx.loc[(cf_mx[\\'ACTUAL_CLASS\\'] == \\'Yes\\') & (cf_mx[\\'PREDICTED_CLASS\\'] == \\'Yes\\'),\\'COUNT\\']\\n    FN = cf_mx.loc[(cf_mx[\\'ACTUAL_CLASS\\'] == \\'Yes\\') & (cf_mx[\\'PREDICTED_CLASS\\'] == \\'No\\'),\\'COUNT\\']\\n    TP = TP.values[0]\\n    TN = TN.values[0]\\n    FP = FP.values[0]\\n    FN = FN.values[0]\\n    P = TP + FN\\n    N = TN + FP\\n    return TN, FP, FN, TP, P, N\\nIn\\xa0[260]:\\nTN, FP, FN, TP, P, N = f_cf_mx_values(top_1_imp_ls[1], d_test_bl)\\nTN, FP, FN, TP, P, N\\nOut[260]:\\n(197, 13, 32, 178, 210, 210)\\nIn\\xa0[265]:\\ncf_matrix_ls = [TN, FP, FN, TP]\\nIn\\xa0[266]:\\nimport numpy as np\\ncf_matrix = np.array(cf_matrix_ls).reshape(2,2)\\nIn\\xa0[267]:\\ngroup_names = [\\'True No\\',\\'False Yes\\',\\'False No\\',\\'True Yes\\']\\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\\n                cf_matrix.flatten()]\\ngroup_percentages = [\"{0:.2%}\".format(2*value) for value in\\n                     cf_matrix.flatten()/np.sum(cf_matrix)]\\nlabels = [f\"{v1}\\\\n{v2}\\\\n{v3}\" for v1, v2, v3 in\\n          zip(group_names,group_counts, group_percentages)]\\nlabels = np.asarray(labels).reshape(2,2)\\nsns.heatmap(cf_matrix, annot=labels, fmt=\\'\\', cmap=\\'Blues\\')\\nOut[267]:\\n<AxesSubplot: >\\nIn\\xa0[268]:\\n# SUPPORT Yes = SUPOORT No - all statisitcs are reliable\\nf_score_res(top_1_imp_ls[1],d_test_bl)[1].collect()\\n# RECALL    Yes - True Yes. Hit. Underestimation\\n# PRECISION Yes - False alarm. Overestimation\\nOut[268]:\\nSTAT_NAME\\nSTAT_VALUE\\nCLASS_NAME\\n0\\nAUC\\n0.9498951247165532\\nNone\\n1\\nRECALL\\n0.9380952380952381\\nNo\\n2\\nPRECISION\\n0.8602620087336245\\nNo\\n3\\nF1_SCORE\\n0.8974943052391801\\nNo\\n4\\nSUPPORT\\n210\\nNo\\n5\\nRECALL\\n0.8476190476190476\\nYes\\n6\\nPRECISION\\n0.9319371727748691\\nYes\\n7\\nF1_SCORE\\n0.8877805486284288\\nYes\\n8\\nSUPPORT\\n210\\nYes\\n9\\nACCURACY\\n0.8928571428571429\\nNone\\n10\\nKAPPA\\n0.7857142857142857\\nNone\\n11\\nMCC\\n0.7889500706904004\\nNone\\nIn\\xa0[269]:\\n# Further steps test with an increased number of percentage for test in split d_test to check accuracy\\n# Check HGBT trending parameters\\n# Save list of models\\nIn\\xa0[270]:\\n# END sound\\n', doc_id='ee2e2b31-92d4-4a61-a8a1-c9bae1b2326e', embedding=None, doc_hash='144d7e2c8c513db5e10ff1ad6f560767f796db9b097a0521362c8a7da438ee9c', extra_info=None),\n",
       "  Document(text='', doc_id='837849c7-0005-4afe-8748-558a2f97d863', embedding=None, doc_hash='dc937b59892604f5a86ac96936cd7ff09e25f18ae6b758e8014a24c7fa039e91', extra_info=None),\n",
       "  Document(text='\\nNotebook\\nPython & HANA ML Check¬∂This Notebooks installs the required library to connect to SAP HANA Cloud under SAP Data Warehouse Cloud and tests the connectivity.\\nDocumentation¬∂\\nSAP HANA Python Client API for Machine Learning Algorithms:\\nhttps://pypi.org/project/hana-ml/\\nSAP HANA Automated Predictive Library (APL):\\nhttps://help.sap.com/doc/1d0ebfe5e8dd44d09606814d83308d4b/2.0.06/en-US/hana_ml.algorithms.apl.html\\nSAP HANA Predictive Analysis Library (PAL):\\nhttps://help.sap.com/doc/1d0ebfe5e8dd44d09606814d83308d4b/2.0.06/en-US/hana_ml.algorithms.pal.html\\nPackage Dependencies: \\nhttps://help.sap.com/doc/1d0ebfe5e8dd44d09606814d83308d4b/2.0.06/en-US/Installation.html\\nExamples: \\nhttps://github.com/SAP-samples/hana-ml-samples/tree/main/Python-API/pal/notebooks\\nSAP HANA ML Library¬∂You will be using the \\'SAP HANA Python Client API for Machine Learning Algorithm\\'. Begin by ensuring that you have the correct version available.\\nIn\\xa0[\\xa0]:\\nimport hana_ml\\nprint(hana_ml.__version__)\\nIn case you have an oldder version than 2.14.22102800, run the following cell to upgrade the library.Running this cell, even if you have already that version installed, does not do any harm.\\nIn\\xa0[\\xa0]:\\n!pip install hana-ml==2.14.22102800\\n!pip install shapely\\nAfter upgrading the library you have to restart the Kernel to ensure the new version is loaded in your current session. In the Jupyter Lab menu go into \"Kernel\" -> \"Restart Kernel...\". Once the Kernel has been restarted, continue with the next cell.\\nIn\\xa0[\\xa0]:\\nimport hana_ml\\nprint(hana_ml.__version__)\\nTest connection to SAP HANA¬∂Establish a connection to the SAP HANA system and send a statement to verify the connection is working as expected. For simplicity you will use the user name and password in clear text. Fore a more realistic and secure approach it is possible to leverage credentials that are safely kept in the Secure User Store from the SAP HANA client, as described in the following blog: https://blogs.sap.com/2020/07/27/hands-on-tutorial-automated-predictive-apl-in-sap-hana-cloud/\\nYou must update these logon credentials as provided to you by your workshop host.\\nIn\\xa0[\\xa0]:\\nhana_address = \\'>YOURHORSTNAME>\\' \\nhana_port = 443 # Adjust if needed / as advised\\nhana_user = \\'YOURUSERNAME\\' \\nhana_password = \\'YOURPASSWORD\\' \\nhana_encrypt = \\'true\\' # Adjust if needed / as advised\\nimport hana_ml.dataframe as dataframe\\n# Instantiate connection object\\nconn = dataframe.ConnectionContext(address = hana_address,\\n                                   port = hana_port,\\n                                   user = hana_user, \\n                                   password = hana_password, \\n                                   encrypt = hana_encrypt,\\n                                   sslValidateCertificate = \\'false\\' \\n                                  )\\n# Control connection\\nconn.connection.isconnected()\\nThe above cell should execute without error and display the value True.\\n', doc_id='8380a7cd-27d2-4da2-9a48-a6c825389d5f', embedding=None, doc_hash='290348e9ec15c130043823da5503a917b9444041d6255a23eec9f6141fed577b', extra_info=None),\n",
       "  Document(text='\\nNotebook\\nNotebook - Data Upload¬∂This Notebooks transforms and uploads the  data to SAP HANA.\\nDocumentation¬∂\\nSAP HANA Python Client API for Machine Learning Algorithms:\\nhttps://pypi.org/project/hana-ml/\\nSAP HANA Automated Predictive Library (APL):\\nhttps://help.sap.com/doc/1d0ebfe5e8dd44d09606814d83308d4b/2.0.06/en-US/hana_ml.algorithms.apl.html\\nSAP HANA Predictive Analysis Library (PAL):\\nhttps://help.sap.com/doc/1d0ebfe5e8dd44d09606814d83308d4b/2.0.06/en-US/hana_ml.algorithms.pal.html\\nPackage Dependencies: \\nhttps://help.sap.com/doc/1d0ebfe5e8dd44d09606814d83308d4b/2.0.06/en-US/Installation.html\\nExamples: \\nhttps://github.com/SAP-samples/hana-ml-samples/tree/main/Python-API/pal/notebooks\\nSAP HANA ML Library¬∂You will be using the \\'SAP HANA Python Client API for Machine Learning Algorithm\\'. See the notebook \"10 Connectivity Check\" that you are using the approviate version of that package.\\nIn\\xa0[\\xa0]:\\nimport hana_ml\\nprint(hana_ml.__version__)\\nLoad the CSV file into a Python object (Pandas DataFrame)¬∂\\nYou must update the path to the file on your environment\\nIn\\xa0[\\xa0]:\\nimport pandas as pd\\ndf_data = pd.read_csv(r\\'<YourPath>\\\\<YourCSVFile>.csv\\', sep = \\',\\')\\ndf_data.head(5)\\nTransform the data¬∂Before uploading the data to SAP HANA Cloud, carry out a few transformations. Turn the column headers into upper case.\\nIn\\xa0[\\xa0]:\\ndf_data.columns = map(str.upper, df_data.columns)\\ndf_data.head(5)\\nUpload the data to SAP HANA¬∂We are happy with the data, so upload it to SAP HANA. Establish a connection with the hana_ml wrapper‚Ä¶\\nYou must update these logon credentials as provided to you by your workshop host.\\nIn\\xa0[\\xa0]:\\nhana_address = \\'>YOURHORSTNAME>\\' \\nhana_port = 443 # Adjust if needed / as advised\\nhana_user = \\'YOURUSERNAME\\' \\nhana_password = \\'YOURPASSWORD\\' \\nhana_encrypt = \\'true\\' # Adjust if needed / as advised\\nimport hana_ml.dataframe as dataframe\\n# Instantiate connection object\\nconn = dataframe.ConnectionContext(address = hana_address,\\n                                   port = 443, \\n                                   user = hana_user, \\n                                   password = hana_password, \\n                                   encrypt = hana_encrypt,\\n                                   sslValidateCertificate = \\'false\\' \\n                                  )\\n# Control connection\\nconn.connection.isconnected()\\n...and upload the Pandas DataFrame into a table called after your Username.\\nIn\\xa0[\\xa0]:\\ndf_remote = dataframe.create_dataframe_from_pandas(connection_context = conn, \\n                                                   pandas_df = df_data, \\n                                                   table_name = \\'<YOURTABLENAME>\\',\\n                                                   force = True,\\n                                                   replace = False)\\nClose the connection.\\nIn\\xa0[\\xa0]:\\n', doc_id='aa07076a-11d1-4cd3-a82b-c2ffb65803f1', embedding=None, doc_hash='6ff2830e413e92ecdbb49e544cd0b9ab844278007aa3cedab780684b56e1109a', extra_info=None),\n",
       "  Document(text='\\nNotebook\\nHANA Cloud - Predicitive Analysis Library Hands On¬∂\\nDocumentation¬∂\\nSAP HANA Python Client API for Machine Learning Algorithms: https://pypi.org/project/hana-ml/\\nSAP HANA Predictive Analysis Library (PAL): https://help.sap.com/viewer/2cfbc5cf2bc14f028cfbe2a2bba60a50/1.0.12/en-US\\nSAP HANA ML Library\\nYou will be using the \\'SAP HANA Python Client API for Machine Learning Algorithm\\'.\\n1.  How can we directly access the data in our HANA?¬∂\\nIn\\xa0[\\xa0]:\\n!pip install hana_ml\\nIn\\xa0[1]:\\nimport hana_ml\\nIn\\xa0[\\xa0]:\\nhana_address = #your hostname as string\\nhana_port = #your port as integer\\nhana_user = #your user as string\\nhana_password = #your password as string\\nhana_encrypt = \\'true\\' #for HANA Cloud\\nIn\\xa0[\\xa0]:\\nimport hana_ml.dataframe as dataframe\\n# Establish connection\\nconn = dataframe.ConnectionContext(address = hana_address,\\n                                   port = hana_port, \\n                                   user = hana_user, \\n                                   password = hana_password, \\n                                   encrypt = hana_encrypt,\\n                                   sslValidateCertificate = \\'false\\')\\nThrough a HANA Key we are able to hide our login credentials.\\nIn\\xa0[\\xa0]:\\nimport hana_ml.dataframe as dataframe\\n# Establish connection\\nconn = dataframe.ConnectionContext(userkey = \\'MYHANACLOUD\\',\\n                                   encrypt = \\'true\\',\\n                                   sslValidateCertificate = \\'false\\')\\nIn\\xa0[4]:\\nimport pandas as pd\\n#load data Predictive_Quality_Use_Case.csv, change path to your directory\\ndf = pd.read_csv(r\"./pred_quality.csv\")\\ndf.head()\\nOut[4]:\\nsupplier\\nmachine\\nquality\\nsensor1\\nsensor2\\nsensor3\\nsensor4\\nsensor5\\nsensor6\\nsensor7\\nsensor8\\nsensor9\\nsensor10\\n0\\nSupE\\nMachA\\n0\\n0.341645\\n0.502370\\n0.431381\\n0.201660\\n0.521939\\n0.436171\\n0.281260\\n0.478072\\n0.378124\\n0.458864\\n1\\nSupE\\nMachA\\n0\\n0.298768\\n0.189764\\n0.359475\\n0.246197\\n0.427249\\n0.222523\\n0.522413\\n0.210466\\n0.519614\\n0.277754\\n2\\nSupF\\nMachC\\n0\\n0.441123\\n0.373059\\n0.434842\\n0.359969\\n0.453749\\n0.540650\\n0.495720\\n0.478441\\n0.299070\\n0.208134\\n3\\nSupF\\nMachB\\n0\\n0.307060\\n0.388752\\n0.490346\\n0.286483\\n0.289103\\n0.580277\\n0.348547\\n0.475260\\n0.233768\\n0.152235\\n4\\nSupC\\nMachA\\n0\\n0.378825\\n0.132827\\n0.163909\\n0.341255\\n0.360088\\n0.377897\\n0.370030\\n0.389969\\n0.399875\\n0.348875\\nIn\\xa0[5]:\\n#change columns to upper string\\ndf.columns = map(str.upper, df.columns)\\nIn\\xa0[6]:\\n#insert a product ID, which will later be used as key\\ndf.insert(0, \\'PRODUCT_ID\\', df.reset_index().index)\\nIn\\xa0[7]:\\n#control a sample of the data\\ndf.head()\\nOut[7]:\\nPRODUCT_ID\\nSUPPLIER\\nMACHINE\\nQUALITY\\nSENSOR1\\nSENSOR2\\nSENSOR3\\nSENSOR4\\nSENSOR5\\nSENSOR6\\nSENSOR7\\nSENSOR8\\nSENSOR9\\nSENSOR10\\n0\\n0\\nSupE\\nMachA\\n0\\n0.341645\\n0.502370\\n0.431381\\n0.201660\\n0.521939\\n0.436171\\n0.281260\\n0.478072\\n0.378124\\n0.458864\\n1\\n1\\nSupE\\nMachA\\n0\\n0.298768\\n0.189764\\n0.359475\\n0.246197\\n0.427249\\n0.222523\\n0.522413\\n0.210466\\n0.519614\\n0.277754\\n2\\n2\\nSupF\\nMachC\\n0\\n0.441123\\n0.373059\\n0.434842\\n0.359969\\n0.453749\\n0.540650\\n0.495720\\n0.478441\\n0.299070\\n0.208134\\n3\\n3\\nSupF\\nMachB\\n0\\n0.307060\\n0.388752\\n0.490346\\n0.286483\\n0.289103\\n0.580277\\n0.348547\\n0.475260\\n0.233768\\n0.152235\\n4\\n4\\nSupC\\nMachA\\n0\\n0.378825\\n0.132827\\n0.163909\\n0.341255\\n0.360088\\n0.377897\\n0.370030\\n0.389969\\n0.399875\\n0.348875\\nIn\\xa0[8]:\\n#Create a SAP HANA dataframe and point it to the table with the uploaded data.\\ndf_remote = dataframe.create_dataframe_from_pandas(connection_context = conn, \\n                                                   pandas_df = df, \\n                                                   table_name = \\'PREDICTIVEQUALITY\\',\\n                                                   force = True,\\n                                                   replace = False)\\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.87s/it]\\n2.  How can we explore our data and react to data quality issues early?¬∂\\nIn\\xa0[9]:\\n#control the size of the data\\ndf_remote.count()\\nOut[9]:\\n125000\\nIn\\xa0[10]:\\n#control the variable types in SAP HANA\\ndf_remote.dtypes()\\nOut[10]:\\n[(\\'PRODUCT_ID\\', \\'INT\\', 10, 10, 10, 0),\\n (\\'SUPPLIER\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'MACHINE\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'QUALITY\\', \\'INT\\', 10, 10, 10, 0),\\n (\\'SENSOR1\\', \\'DOUBLE\\', 15, 15, 15, 0),\\n (\\'SENSOR2\\', \\'DOUBLE\\', 15, 15, 15, 0),\\n (\\'SENSOR3\\', \\'DOUBLE\\', 15, 15, 15, 0),\\n (\\'SENSOR4\\', \\'DOUBLE\\', 15, 15, 15, 0),\\n (\\'SENSOR5\\', \\'DOUBLE\\', 15, 15, 15, 0),\\n (\\'SENSOR6\\', \\'DOUBLE\\', 15, 15, 15, 0),\\n (\\'SENSOR7\\', \\'DOUBLE\\', 15, 15, 15, 0),\\n (\\'SENSOR8\\', \\'DOUBLE\\', 15, 15, 15, 0),\\n (\\'SENSOR9\\', \\'DOUBLE\\', 15, 15, 15, 0),\\n (\\'SENSOR10\\', \\'DOUBLE\\', 15, 15, 15, 0)]\\nThe variable QUALITY is binary and labels all products of bad quality with a 1. Since this is a categorical variable we transform it to type NVARCHAR with the following command.\\nIn\\xa0[11]:\\n#transform the variable QUALITY\\ndf_remote = df_remote.cast(\\'QUALITY\\', \\'NVARCHAR(20)\\')\\nIn\\xa0[20]:\\ndf_remote.agg([(\\'count\\', \\'PRODUCT_ID\\', \\'N\\')], group_by=\\'QUALITY\\').collect()\\nOut[20]:\\nQUALITY\\nN\\n0\\n0\\n100000\\n1\\n1\\n25000\\nIn\\xa0[12]:\\n#control the variable types\\ndf_remote.dtypes()\\nOut[12]:\\n[(\\'PRODUCT_ID\\', \\'INT\\', 10, 10, 10, 0),\\n (\\'SUPPLIER\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'MACHINE\\', \\'NVARCHAR\\', 5000, 5000, 5000, 0),\\n (\\'QUALITY\\', \\'NVARCHAR\\', 20, 20, 20, 0),\\n (\\'SENSOR1\\', \\'DOUBLE\\', 15, 15, 15, 0),\\n (\\'SENSOR2\\', \\'DOUBLE\\', 15, 15, 15, 0),\\n (\\'SENSOR3\\', \\'DOUBLE\\', 15, 15, 15, 0),\\n (\\'SENSOR4\\', \\'DOUBLE\\', 15, 15, 15, 0),\\n (\\'SENSOR5\\', \\'DOUBLE\\', 15, 15, 15, 0),\\n (\\'SENSOR6\\', \\'DOUBLE\\', 15, 15, 15, 0),\\n (\\'SENSOR7\\', \\'DOUBLE\\', 15, 15, 15, 0),\\n (\\'SENSOR8\\', \\'DOUBLE\\', 15, 15, 15, 0),\\n (\\'SENSOR9\\', \\'DOUBLE\\', 15, 15, 15, 0),\\n (\\'SENSOR10\\', \\'DOUBLE\\', 15, 15, 15, 0)]\\nIn\\xa0[25]:\\n#describe the data in SAP HANA\\ndf_remote.describe().collect()\\nOut[25]:\\ncolumn\\ncount\\nunique\\nnulls\\nmean\\nstd\\nmin\\nmax\\nmedian\\n25_percent_cont\\n25_percent_disc\\n50_percent_cont\\n50_percent_disc\\n75_percent_cont\\n75_percent_disc\\n0\\nPRODUCT_ID\\n125000\\n125000\\n0\\n62499.500000\\n36084.536161\\n0.0\\n124999.0\\n62500.000000\\n31249.750000\\n31249.000000\\n62499.500000\\n62499.000000\\n93749.250000\\n93749.000000\\n1\\nSENSOR1\\n125000\\n124982\\n0\\n0.479726\\n0.153780\\n0.0\\n1.0\\n0.478589\\n0.354348\\n0.354348\\n0.478589\\n0.478587\\n0.606131\\n0.606128\\n2\\nSENSOR2\\n125000\\n124991\\n0\\n0.485109\\n0.205528\\n0.0\\n1.0\\n0.485640\\n0.309291\\n0.309281\\n0.485640\\n0.485637\\n0.660287\\n0.660286\\n3\\nSENSOR3\\n125000\\n124988\\n0\\n0.497013\\n0.208116\\n0.0\\n1.0\\n0.496390\\n0.318205\\n0.318204\\n0.496390\\n0.496385\\n0.675093\\n0.675093\\n4\\nSENSOR4\\n125000\\n124983\\n0\\n0.459679\\n0.149585\\n0.0\\n1.0\\n0.466757\\n0.337220\\n0.337219\\n0.466757\\n0.466756\\n0.579259\\n0.579259\\n5\\nSENSOR5\\n125000\\n124977\\n0\\n0.509524\\n0.156953\\n0.0\\n1.0\\n0.510624\\n0.379915\\n0.379915\\n0.510624\\n0.510622\\n0.638532\\n0.638532\\n6\\nSENSOR6\\n125000\\n124982\\n0\\n0.509260\\n0.158821\\n0.0\\n1.0\\n0.509265\\n0.377351\\n0.377350\\n0.509265\\n0.509264\\n0.641560\\n0.641557\\n7\\nSENSOR7\\n125000\\n124986\\n0\\n0.483033\\n0.159582\\n0.0\\n1.0\\n0.482779\\n0.349749\\n0.349749\\n0.482779\\n0.482776\\n0.616109\\n0.616108\\n8\\nSENSOR8\\n125000\\n124985\\n0\\n0.485639\\n0.158661\\n0.0\\n1.0\\n0.485044\\n0.352291\\n0.352289\\n0.485044\\n0.485043\\n0.618902\\n0.618901\\n9\\nSENSOR9\\n125000\\n124987\\n0\\n0.469235\\n0.152392\\n0.0\\n1.0\\n0.468354\\n0.342879\\n0.342879\\n0.468354\\n0.468352\\n0.592790\\n0.592790\\n10\\nSENSOR10\\n125000\\n124983\\n0\\n0.488177\\n0.161700\\n0.0\\n1.0\\n0.486822\\n0.356040\\n0.356037\\n0.486822\\n0.486822\\n0.620648\\n0.620648\\n11\\nSUPPLIER\\n125000\\n7\\n0\\nNaN\\nNaN\\nNaN\\nNaN\\nNaN\\nNaN\\nNaN\\nNaN\\nNaN\\nNaN\\nNaN\\n12\\nMACHINE\\n125000\\n3\\n0\\nNaN\\nNaN\\nNaN\\nNaN\\nNaN\\nNaN\\nNaN\\nNaN\\nNaN\\nNaN\\nNaN\\n13\\nQUALITY\\n125000\\n2\\n0\\nNaN\\nNaN\\nNaN\\nNaN\\nNaN\\nNaN\\nNaN\\nNaN\\nNaN\\nNaN\\nNaN\\nIn\\xa0[13]:\\n#create a Data Report for further exploration\\nfrom hana_ml.visualizers.dataset_report import DatasetReportBuilder\\ndatasetReportBuilder = DatasetReportBuilder()\\ndatasetReportBuilder.build(df_remote, key=\"PRODUCT_ID\")\\nGenerating dataset report...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:35<00:00,  4.49s/it]\\nIn\\xa0[14]:\\n#generate Data Report directly in Jupyter Notebook\\ndatasetReportBuilder.generate_notebook_iframe_report()\\nIn order to review the dataset report better, you need to adjust the size of the left area or hide the left area temporarily!\\n3.  How can we leverage the computing power of our HANA in our machine learning use case?¬∂\\nIn\\xa0[15]:\\n#create training and testing set\\nfrom hana_ml.algorithms.pal import partition\\nhdf_train, hdf_test, hdf_val = partition.train_test_val_split( random_seed = 1017,\\n                                                               data = df_remote, \\n                                                               training_percentage = 0.8, \\n                                                                testing_percentage = 0.1,\\n                                                                validation_percentage = 0.1)\\nIn\\xa0[16]:\\n#control the size of the training and testing set\\nprint(\\'Size of training subset: \\' + str(hdf_train.count()))\\nprint(\\'Size of test subset: \\' + str(hdf_test.count()))\\nSize of training subset: 100000\\nSize of test subset: 12500\\nIn\\xa0[17]:\\n# Union train and validation data into one set\\ndf_trainval=hdf_train.select(\\'*\\', (\\'1\\', \\'TRAIN_VAL_INDICATOR\\' )).union(hdf_val.select(\\'*\\', (\\'2\\', \\'TRAIN_VAL_INDICATOR\\' )))\\ndisplay(df_trainval.head(5).collect())\\nPRODUCT_ID\\nSUPPLIER\\nMACHINE\\nQUALITY\\nSENSOR1\\nSENSOR2\\nSENSOR3\\nSENSOR4\\nSENSOR5\\nSENSOR6\\nSENSOR7\\nSENSOR8\\nSENSOR9\\nSENSOR10\\nTRAIN_VAL_INDICATOR\\n0\\n22\\nSupC\\nMachC\\n0\\n0.598684\\n0.544622\\n0.653084\\n0.467637\\n0.833161\\n0.600017\\n0.580115\\n0.558271\\n0.494238\\n0.806028\\n1\\n1\\n57\\nSupC\\nMachC\\n0\\n0.336701\\n0.442155\\n0.373946\\n0.341537\\n0.380335\\n0.471353\\n0.475528\\n0.380412\\n0.160266\\n0.293423\\n1\\n2\\n71\\nSupC\\nMachC\\n0\\n0.586596\\n0.580597\\n0.643189\\n0.532847\\n0.576641\\n0.742735\\n0.625323\\n0.677316\\n0.642405\\n0.565801\\n1\\n3\\n78\\nSupC\\nMachC\\n0\\n0.302299\\n0.518902\\n0.351178\\n0.405238\\n0.296945\\n0.325774\\n0.370321\\n0.456620\\n0.384333\\n0.358422\\n1\\n4\\n87\\nSupC\\nMachC\\n0\\n0.589654\\n0.676642\\n0.772341\\n0.623043\\n0.642281\\n0.645835\\n0.667018\\n0.578739\\n0.597443\\n0.486433\\n1\\nLet us now train or random forest on the training set. First, we set the numbers of trees very high, to see where the Out of Bag error converges. After optimizing the numbers of trees we will take a closer look at the variables considered at each split.\\nIn\\xa0[39]:\\n# Train the Station classifer model using PAL HybridGradientBoostingTree\\nfrom hana_ml.algorithms.pal.unified_classification import UnifiedClassification\\n# Initialize the model object \\nhgbc = UnifiedClassification(func=\\'HybridGradientBoostingTree\\',\\n                            n_estimators = 101, max_depth=5,\\n                            split_method=\\'histogram\\', max_bin_num=1000, \\n                            #feature_grouping=True, tol_rate=0.001,\\n                            learning_rate=0.1, split_threshold=0.1,\\n                            resampling_method=\\'stratified_cv\\', fold_num=5, evaluation_metric = \\'error_rate\\', ref_metric=[\\'auc\\']\\n                            )\\n# Execute the training of the model\\nhgbc.fit(data=df_trainval, \\n         key=\\'PRODUCT_ID\\', \\n         features=[\\'SUPPLIER\\',\\'MACHINE\\',\\'SENSOR1\\',\\'SENSOR2\\',\\'SENSOR3\\',\\'SENSOR4\\',\\'SENSOR5\\',\\'SENSOR6\\',\\'SENSOR7\\',\\'SENSOR8\\',\\'SENSOR9\\',\\'SENSOR10\\'],\\n         label=\\'QUALITY\\', categorical_variable= [\\'SUPPLIER\\', \\'MACHINE\\', \\'QUALITY\\'],\\n         ntiles=20,  build_report=True,\\n         partition_method=\\'user_defined\\', purpose=\\'TRAIN_VAL_INDICATOR\\')\\ndisplay(hgbc.runtime)\\n14.351041316986084\\nIn\\xa0[36]:\\n# Build Model Report\\nfrom hana_ml.visualizers.unified_report import UnifiedReport\\nUnifiedReport(hgbc).build().display()\\nIn order to review the unified classification model report better, you need to adjust the size of the left area or hide the left area temporarily!\\nIn\\xa0[40]:\\n# Test model generalization using the test data-subset, not used during training\\nscorepredictions, scorestats, scorecm, scoremetrics = hgbc.score(data=hdf_test , key= \\'PRODUCT_ID\\', label=\\'QUALITY\\', \\n                                                                 ntiles=20, \\n                                                                  thread_ratio=1.0)\\ndisplay(hgbc.runtime)\\ndisplay(scorestats.sort(\\'CLASS_NAME\\').collect())\\ndisplay(scorecm.filter(\\'COUNT != 0\\').collect())\\n#display(scoremetrics.collect())\\n38.16511154174805\\nSTAT_NAME\\nSTAT_VALUE\\nCLASS_NAME\\n0\\nAUC\\n0.9684766144\\nNone\\n1\\nACCURACY\\n0.90616\\nNone\\n2\\nKAPPA\\n0.6877432035884072\\nNone\\n3\\nMCC\\n0.6938425680131939\\nNone\\n4\\nRECALL\\n0.966054032339058\\n0\\n5\\nPRECISION\\n0.9201262674574325\\n0\\n6\\nF1_SCORE\\n0.9425309881926411\\n0\\n7\\nSUPPORT\\n9957\\n0\\n8\\nRECALL\\n0.6716476602438065\\n1\\n9\\nPRECISION\\n0.8347996089931574\\n1\\n10\\nF1_SCORE\\n0.7443887557202005\\n1\\n11\\nSUPPORT\\n2543\\n1\\nACTUAL_CLASS\\nPREDICTED_CLASS\\nCOUNT\\n0\\n0\\n0\\n9619\\n1\\n0\\n1\\n338\\n2\\n1\\n0\\n835\\n3\\n1\\n1\\n1708\\n4.  How can we save and create different versions of our results¬∂\\nIn\\xa0[\\xa0]:\\n#create Model storage\\nfrom hana_ml.model_storage import ModelStorage \\nMODEL_SCHEMA = \\'YANNICK\\' # HANA schema in which models are to be saved \\nmodel_storage = ModelStorage(connection_context=conn, schema = MODEL_SCHEMA) \\nrf.name = \\'Unified Classification HGBT Model\\' \\nmodel_storage.save_model(model=rf, if_exists = \\'replace\\')\\nIn\\xa0[\\xa0]:\\n', doc_id='18c5160f-e5bc-437a-b5d4-9eaa025d3c50', embedding=None, doc_hash='0bc97a4871da72177f142077dc1356d1f81e72f8594bff1f34eb65cdcc23b2ea', extra_info=None),\n",
       "  Document(text=\"\\nNotebook\\nNotebook - Data Upload¬∂This Notebooks transforms and uploads the  data to SAP HANA.\\nDocumentation¬∂\\nSAP HANA Python Client API for Machine Learning Algorithms:\\nhttps://pypi.org/project/hana-ml/\\nSAP HANA Automated Predictive Library (APL):\\nhttps://help.sap.com/doc/1d0ebfe5e8dd44d09606814d83308d4b/2.0.06/en-US/hana_ml.algorithms.apl.html\\nSAP HANA Predictive Analysis Library (PAL):\\nhttps://help.sap.com/doc/1d0ebfe5e8dd44d09606814d83308d4b/2.0.06/en-US/hana_ml.algorithms.pal.html\\nPackage Dependencies: \\nhttps://help.sap.com/doc/1d0ebfe5e8dd44d09606814d83308d4b/2.0.06/en-US/Installation.html\\nExamples: \\nhttps://github.com/SAP-samples/hana-ml-samples/tree/main/Python-API/pal/notebooks\\nSAP HANA ML Library¬∂You will be using the 'SAP HANA Python Client API for Machine Learning Algorithm'. Begin by ensuring that you have the correct version available.\\n__Note__, in case you have a version prior to 2.13.22072200, run the following cell to upgrade the library.Running this cell, even if you have already that version installed, does not do any harm.  \\nThe current hana-ml version on pypi.org  is 2.14.22101400 and can also be used certainly.\\nIn\\xa0[1]:\\n#!pip install hana-ml==2.14.22101400\\n#!pip install shapely\\nIn\\xa0[\\xa0]:\\nimport hana_ml\\nprint(hana_ml.__version__)\\nConnect¬∂\\nIn\\xa0[\\xa0]:\\nhana_address = '<HOSTNAME>' \\nhana_port = 443 # Adjust if needed / as advised\\nhana_user = 'YOURUSERNAME' \\nhana_password = 'YOURPASSWORD' \\nhana_encrypt = 'true' # Adjust if needed / as advised\\nimport hana_ml.dataframe as dataframe\\n# Instantiate connection object\\nconn = dataframe.ConnectionContext(address = hana_address,\\n                                   port = 443, \\n                                   user = hana_user, \\n                                   password = hana_password, \\n                                   encrypt = hana_encrypt,\\n                                   sslValidateCertificate = 'false' \\n                                  )\\n# Control connection\\nprint(conn.connection.isconnected())\\nprint(conn.hana_version())\\nLoad the CSV file into a Python object (Pandas DataFrame)¬∂\\nIn\\xa0[\\xa0]:\\nimport pandas as pd\\ndf_data = pd.read_csv(r'.\\\\Emp_Churn_Original.csv', sep = ',')\\ndf_data.head(5)\\nBefore uploading the data to SAP HANA Cloud, carry out a few transformations. Turn the column headers into upper case.\\nIn\\xa0[\\xa0]:\\ndf_data.columns = map(str.upper, df_data.columns)\\ndf_data.head(5)\\nUpload the data to SAP HANA¬∂...and upload the Pandas DataFrame into a table called after your Username.\\nIn\\xa0[\\xa0]:\\nhdf_employeechurn = dataframe.create_dataframe_from_pandas(connection_context = conn, \\n                                                   pandas_df = df_data, \\n                                                   table_name = 'EMPLOYEE_CHURN_ORG',\\n                                                   force = True,\\n                                                   replace = False)\\nIn\\xa0[\\xa0]:\\n# if you have uploaded the data already, there are multiple ways to create the HANA dataframe\\n# hdf_employeechurn = conn.table('EMPLOYEE_CHURN_ORG') \\n# hdf_employeechurn = conn.sql('Select * from EMPLOYEE_CHURN_ORG') \\nExploring the uploaded data set¬∂\\nIn\\xa0[\\xa0]:\\n#control the variable types in SAP HANA\\nhdf_employeechurn.dtypes()\\nIn\\xa0[\\xa0]:\\nhdf_employeechurn.columns\\nIn\\xa0[\\xa0]:\\n# Display Top 10 rows\\ndisplay(hdf_employeechurn.head(10).collect())\\ndisplay(hdf_employeechurn.count())\\nIn\\xa0[\\xa0]:\\n# Display row count\\ndisplay(hdf_employeechurn.count())\\n#display(hdf_employeechurn.distinct('EMPLOYEE_ID').count())\\nIn\\xa0[\\xa0]:\\n# Explore column descriptive statistics using the describe method\\nhdf_employeechurn.describe().collect()\\nIn\\xa0[\\xa0]:\\n#dropping some columns PREVCOUNTRYLAT PREVCOUNTRYLON\\nhdf_employeechurn=hdf_employeechurn.drop('PREVCOUNTRYLAT').drop('PREVCOUNTRYLON')\\nIn\\xa0[\\xa0]:\\nhdf_employeechurn.agg([('count', 'EMPLOYEE_ID', 'N')], group_by='FLIGHT_RISK').collect()\\nIn\\xa0[\\xa0]:\\n \\nWhat could be next?¬∂\\nLooking at missing values\\nSplitting Train / Test data samples ...\\nExploring data yoga approaches \\n\", doc_id='75eed80f-9d1b-458d-ba55-2fa1355317ae', embedding=None, doc_hash='9fcd105060816def24a6bc1ccb3ab547a3306d02c7b5322354ada695871ed897', extra_info=None)],\n",
       " 'total_tokens': 43819,\n",
       " 'total_cost_ada': 0.0175276,\n",
       " 'total_cost_davinci': 1.31457,\n",
       " 'index': <llama_index.indices.vector_store.base.GPTVectorStoreIndex at 0x1de28f20be0>,\n",
       " 'query_engine': <llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x1de28f20a90>}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspecting the object lct\n",
    "lct.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "605a8ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_data',\n",
       " '_is_protocol',\n",
       " '_is_runtime_protocol',\n",
       " 'add',\n",
       " 'client',\n",
       " 'delete',\n",
       " 'from_persist_dir',\n",
       " 'from_persist_path',\n",
       " 'get',\n",
       " 'is_embedding_query',\n",
       " 'persist',\n",
       " 'query',\n",
       " 'stores_text']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What embedding stores - vector_store.json\n",
    "dir(lct.index.vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "951aa344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_add_nodes_to_index',\n",
       " '_aget_node_embedding_results',\n",
       " '_async_add_nodes_to_index',\n",
       " '_build_index_from_nodes',\n",
       " '_delete',\n",
       " '_docstore',\n",
       " '_get_node_embedding_results',\n",
       " '_index_struct',\n",
       " '_insert',\n",
       " '_is_protocol',\n",
       " '_service_context',\n",
       " '_storage_context',\n",
       " '_use_async',\n",
       " '_vector_store',\n",
       " 'as_query_engine',\n",
       " 'as_retriever',\n",
       " 'build_index_from_nodes',\n",
       " 'delete',\n",
       " 'docstore',\n",
       " 'from_documents',\n",
       " 'index_id',\n",
       " 'index_struct',\n",
       " 'index_struct_cls',\n",
       " 'insert',\n",
       " 'insert_nodes',\n",
       " 'refresh',\n",
       " 'service_context',\n",
       " 'set_index_id',\n",
       " 'storage_context',\n",
       " 'update',\n",
       " 'vector_store']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(lct.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee02e6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [retrieve] Total LLM token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 5 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [retrieve] Total embedding token usage: 5 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 958 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [get_response] Total LLM token usage: 958 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\n",
      "The content is about SAP HANA and its related technologies, such as SAP HANA Cloud's Auto ML capabilities, SAP HANA Python Client API for Machine Learning Algorithms, and SAP HANA Predictive Analysis Library (PAL). It also includes information about a book related to SAP HANA and a blog post about SAP HANA Machine Learning with ABAP Managed Database Procedures in SAP BW/4HANA.\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# time_now()\n",
    "question = \"What is content about?\"\n",
    "lct.post_question(question)\n",
    "print(lct.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39348fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [retrieve] Total LLM token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 6 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [retrieve] Total embedding token usage: 6 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1252 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [get_response] Total LLM token usage: 1252 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\n",
      "The SAP HANA Cloud Machine Learning Challenge team organized the Community Call.\n"
     ]
    }
   ],
   "source": [
    "question = \"Who organized the Community Call?\"\n",
    "lct.post_question(question)\n",
    "print(lct.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ccef2f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [retrieve] Total LLM token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 6 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [retrieve] Total embedding token usage: 6 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1840 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [get_response] Total LLM token usage: 1840 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\n",
      "Participants must solve the problem of predicting employee churn.\n"
     ]
    }
   ],
   "source": [
    "question = \"What problem participants must solve?\"\n",
    "lct.post_question(question)\n",
    "print(lct.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2914750",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [retrieve] Total LLM token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 7 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [retrieve] Total embedding token usage: 7 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1936 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [get_response] Total LLM token usage: 1936 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\n",
      "Data for predicting employee churn can include information about the employee such as their job title, years of experience, salary, performance reviews, and other factors that may influence their decision to stay or leave the company. Additionally, data can be collected from the company itself, such as the onboarding process, company culture, learning opportunities, and other factors that may influence employee churn. By analyzing this data, patterns can be identified that can help predict employee churn and inform decisions about how to improve the company's retention rate.\n"
     ]
    }
   ],
   "source": [
    "question = \"Explain data for predicting employee churn\"\n",
    "lct.post_question(question)\n",
    "print(lct.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "443189c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [retrieve] Total LLM token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 14 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [retrieve] Total embedding token usage: 14 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1787 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [get_response] Total LLM token usage: 1787 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\n",
      "The participants utilized the HybridGradientBoostingTree model for their machine learning.\n"
     ]
    }
   ],
   "source": [
    "question = \"Can you tell me which machine learning models were utilized by the participants?\"\n",
    "lct.post_question(question)\n",
    "print(lct.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29f335b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [retrieve] Total LLM token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 13 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [retrieve] Total embedding token usage: 13 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 2085 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [get_response] Total LLM token usage: 2085 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\n",
      "The top 5 important features discovered by the model are: SICKDAYS, HRTRAINING, PREVIOUS_CAREER_PATH, LINKEDIN, and FUNCTIONALAREACHANGETYPE.\n"
     ]
    }
   ],
   "source": [
    "question = \"Which are the top 5 important features discoverd by the model?\"\n",
    "lct.post_question(question)\n",
    "print(lct.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05302378",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [retrieve] Total LLM token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 11 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [retrieve] Total embedding token usage: 11 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1925 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [get_response] Total LLM token usage: 1925 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\n",
      "The following code is an example of using the SAP HANA Python Client API for Machine Learning Algorithms to implement a HGBT (Hierarchical Gradient Boosting Tree) model. \n",
      "\n",
      "# Import the necessary libraries\n",
      "import hana_ml\n",
      "from hana_ml.algorithms.apl.hgbt import HGBT\n",
      "\n",
      "# Create a connection to the SAP HANA system\n",
      "connection_context = hana_ml.dataframe.ConnectionContext(address='<hostname>:<port>',\n",
      "                                                        user='<username>',\n",
      "                                                        password='<password>')\n",
      "\n",
      "# Load the data into a dataframe\n",
      "df = connection_context.table('<schema>.<table>')\n",
      "\n",
      "# Create the HGBT model\n",
      "hgbt = HGBT(conn_context=connection_context)\n",
      "\n",
      "# Fit the model\n",
      "hgbt.fit(data=df, key='<key_column>', label='<label_column>')\n",
      "\n",
      "# Make predictions\n",
      "predictions = hgbt.predict(data=df)\n",
      "\n",
      "# Evaluate the model\n",
      "hgbt.evaluate(data=df, label='\n"
     ]
    }
   ],
   "source": [
    "question = \"Python full code SAP HANA Machine learning HGBT example\"\n",
    "lct.post_question(question)\n",
    "print(lct.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f4dee16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [retrieve] Total LLM token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 10 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [retrieve] Total embedding token usage: 10 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1913 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [get_response] Total LLM token usage: 1913 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\n",
      "In [1]:\n",
      "# Import the necessary libraries\n",
      "import hana_ml\n",
      "import pandas as pd\n",
      "\n",
      "# Load the CSV file into a Python object (Pandas DataFrame)\n",
      "df_data = pd.read_csv(r'Emp_Churn_Train.csv', sep = ',')\n",
      "\n",
      "# Create a connection to the HANA system\n",
      "connection_context = hana_ml.dataframe.ConnectionContext(address='<HANA_SYSTEM_ADDRESS>', port=<HANA_SYSTEM_PORT>, user='<HANA_SYSTEM_USER>', password='<HANA_SYSTEM_PASSWORD>')\n",
      "\n",
      "# Create a dataframe object from the Pandas DataFrame\n",
      "df_remote = connection_context.table('EMP_CHURN_TRAIN', schema='<HANA_SYSTEM_SCHEMA>', data=df_data)\n",
      "\n",
      "# Create training and testing set\n",
      "from hana_ml.algorithms.pal import partition\n",
      "hdf_train, hdf_test, hdf_val = partition.train_test_val_split( random_seed = 1017\n"
     ]
    }
   ],
   "source": [
    "question = \"Python full code hana_ml dataframe example\"\n",
    "lct.post_question(question)\n",
    "print(lct.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092a3e52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
